{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":97555,"databundleVersionId":11670858,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model, optimizers\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport pandas as pd\nimport time\nfrom tqdm import tqdm\nimport cv2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:12.662112Z","iopub.execute_input":"2025-04-28T03:36:12.662337Z","iopub.status.idle":"2025-04-28T03:36:25.219465Z","shell.execute_reply.started":"2025-04-28T03:36:12.662316Z","shell.execute_reply":"2025-04-28T03:36:25.218924Z"}},"outputs":[{"name":"stderr","text":"2025-04-28 03:36:13.950487: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745811374.123733      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745811374.182472      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Configuration\nBATCH_SIZE = 16\nIMG_HEIGHT = 256\nIMG_WIDTH = 256\nOUTPUT_CHANNELS = 1  # Depth map is single-channel\nLAMBDA = 100  # Weight for L1 loss in GAN objective\nEPOCHS = 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.220687Z","iopub.execute_input":"2025-04-28T03:36:25.221321Z","iopub.status.idle":"2025-04-28T03:36:25.224813Z","shell.execute_reply.started":"2025-04-28T03:36:25.221302Z","shell.execute_reply":"2025-04-28T03:36:25.224167Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Define the path for the new directory\ncheckpoint_dir = '/kaggle/working/checkpoints'\n\n# Create the directory if it doesn't exist\nos.makedirs(checkpoint_dir, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.225606Z","iopub.execute_input":"2025-04-28T03:36:25.225904Z","iopub.status.idle":"2025-04-28T03:36:25.346278Z","shell.execute_reply.started":"2025-04-28T03:36:25.225880Z","shell.execute_reply":"2025-04-28T03:36:25.345743Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Generator network (U-Net architecture)\ndef build_generator():\n    # Encoder\n    inputs = layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, 1])  # SPAD images are binary (1 channel)\n    \n    # Downsampling layers\n    down_stack = [\n        downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n        downsample(128, 4),  # (batch_size, 64, 64, 128)\n        downsample(256, 4),  # (batch_size, 32, 32, 256)\n        downsample(512, 4),  # (batch_size, 16, 16, 512)\n        downsample(512, 4),  # (batch_size, 8, 8, 512)\n        downsample(512, 4),  # (batch_size, 4, 4, 512)\n        downsample(512, 4),  # (batch_size, 2, 2, 512)\n        downsample(512, 4),  # (batch_size, 1, 1, 512)\n    ]\n    \n    # Upsampling layers\n    up_stack = [\n        upsample(512, 4, dropout=True),  # (batch_size, 2, 2, 1024)\n        upsample(512, 4, dropout=True),  # (batch_size, 4, 4, 1024)\n        upsample(512, 4, dropout=True),  # (batch_size, 8, 8, 1024)\n        upsample(512, 4),  # (batch_size, 16, 16, 1024)\n        upsample(256, 4),  # (batch_size, 32, 32, 512)\n        upsample(128, 4),  # (batch_size, 64, 64, 256)\n        upsample(64, 4),  # (batch_size, 128, 128, 128)\n    ]\n    \n    # Final layer (output depth map)\n    last = layers.Conv2DTranspose(\n        OUTPUT_CHANNELS, 4, strides=2, padding='same',\n        activation='tanh'  # Using tanh to get output in [-1, 1] range\n    )  # (batch_size, 256, 256, 1)\n    \n    x = inputs\n    \n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n    \n    skips = reversed(skips[:-1])\n    \n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = layers.Concatenate()([x, skip])\n    \n    x = last(x)\n    \n    return Model(inputs=inputs, outputs=x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.346895Z","iopub.execute_input":"2025-04-28T03:36:25.347113Z","iopub.status.idle":"2025-04-28T03:36:25.354981Z","shell.execute_reply.started":"2025-04-28T03:36:25.347096Z","shell.execute_reply":"2025-04-28T03:36:25.354281Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Discriminator network (PatchGAN)\ndef build_discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    inp = layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, 1], name='input_image')\n    tar = layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, 1], name='target_image')\n    \n    x = layers.Concatenate()([inp, tar])  # (batch_size, 256, 256, 2)\n    \n    down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)\n    down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)\n    down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n    \n    zero_pad1 = layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n    conv = layers.Conv2D(512, 4, strides=1, kernel_initializer=initializer, \n                         use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n    \n    batchnorm1 = layers.BatchNormalization()(conv)\n    leaky_relu = layers.LeakyReLU()(batchnorm1)\n    \n    zero_pad2 = layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n    last = layers.Conv2D(1, 4, strides=1, \n                         kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n    \n    return Model(inputs=[inp, tar], outputs=last)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.355606Z","iopub.execute_input":"2025-04-28T03:36:25.355841Z","iopub.status.idle":"2025-04-28T03:36:25.367076Z","shell.execute_reply.started":"2025-04-28T03:36:25.355826Z","shell.execute_reply":"2025-04-28T03:36:25.366540Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Helper functions for generator and discriminator\ndef downsample(filters, size, apply_batchnorm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    result = tf.keras.Sequential()\n    result.add(\n        layers.Conv2D(filters, size, strides=2, padding='same',\n                       kernel_initializer=initializer, use_bias=False))\n    \n    if apply_batchnorm:\n        result.add(layers.BatchNormalization())\n    \n    result.add(layers.LeakyReLU())\n    \n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.367768Z","iopub.execute_input":"2025-04-28T03:36:25.368336Z","iopub.status.idle":"2025-04-28T03:36:25.384201Z","shell.execute_reply.started":"2025-04-28T03:36:25.368312Z","shell.execute_reply":"2025-04-28T03:36:25.383652Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def upsample(filters, size, dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    result = tf.keras.Sequential()\n    result.add(\n        layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n                               kernel_initializer=initializer, use_bias=False))\n    \n    result.add(layers.BatchNormalization())\n    \n    if dropout:\n        result.add(layers.Dropout(0.5))\n    \n    result.add(layers.ReLU())\n    \n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.386690Z","iopub.execute_input":"2025-04-28T03:36:25.386961Z","iopub.status.idle":"2025-04-28T03:36:25.396871Z","shell.execute_reply.started":"2025-04-28T03:36:25.386945Z","shell.execute_reply":"2025-04-28T03:36:25.396252Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Loss functions\ndef generator_loss(disc_generated_output, gen_output, target):\n    gan_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(\n        tf.ones_like(disc_generated_output), disc_generated_output)\n    \n    # Mean absolute error (L1 loss) between generated and target depth maps\n    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n    \n    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n    \n    return total_gen_loss, gan_loss, l1_loss\n\ndef discriminator_loss(disc_real_output, disc_generated_output):\n    real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(\n        tf.ones_like(disc_real_output), disc_real_output)\n    \n    generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(\n        tf.zeros_like(disc_generated_output), disc_generated_output)\n    \n    total_disc_loss = real_loss + generated_loss\n    \n    return total_disc_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.397571Z","iopub.execute_input":"2025-04-28T03:36:25.397846Z","iopub.status.idle":"2025-04-28T03:36:25.411536Z","shell.execute_reply.started":"2025-04-28T03:36:25.397826Z","shell.execute_reply":"2025-04-28T03:36:25.410901Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def load_and_preprocess_data(spad_path, depth_path):\n    # Load SPAD binary image (0 or 1 values)\n    spad_img = tf.io.read_file(spad_path)\n    spad_img = tf.image.decode_png(spad_img, channels=1)\n    spad_img = tf.cast(spad_img, tf.float32)\n    spad_img = (spad_img / 255.0) * 2 - 1  # Normalize to [-1, 1]\n    \n    # Load depth map\n    depth_img = tf.io.read_file(depth_path)\n    depth_img = tf.image.decode_png(depth_img, channels=1)\n    depth_img = tf.cast(depth_img, tf.float32)\n    depth_img = (depth_img / 255.0) * 2 - 1  # Normalize to [-1, 1]\n    \n    return spad_img, depth_img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.412262Z","iopub.execute_input":"2025-04-28T03:36:25.412451Z","iopub.status.idle":"2025-04-28T03:36:25.423817Z","shell.execute_reply.started":"2025-04-28T03:36:25.412428Z","shell.execute_reply":"2025-04-28T03:36:25.423304Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def random_crop(spad_img, depth_img):\n    stacked_image = tf.stack([spad_img, depth_img], axis=0)\n    cropped_image = tf.image.random_crop(\n        stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 1])\n    \n    return cropped_image[0], cropped_image[1]\n\ndef normalize(spad_img, depth_img):\n    return spad_img, depth_img\n\n# def random_jitter(spad_img, depth_img):\n#     # Resize to bigger height and width\n#     spad_img, depth_img = resize(spad_img, depth_img, 286, 286)\n    \n#     # Random crop back to the target size\n#     spad_img, depth_img = random_crop(spad_img, depth_img)\n    \n#     if tf.random.uniform(()) > 0.5:\n#         # Random mirroring\n#         spad_img = tf.image.flip_left_right(spad_img)\n#         depth_img = tf.image.flip_left_right(depth_img)\n    \n#     return spad_img, depth_img\n\ndef resize(spad_img, depth_img, height, width):\n    spad_img = tf.image.resize(spad_img, [height, width],\n                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    depth_img = tf.image.resize(depth_img, [height, width],\n                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    \n    return spad_img, depth_img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.424413Z","iopub.execute_input":"2025-04-28T03:36:25.424776Z","iopub.status.idle":"2025-04-28T03:36:25.435886Z","shell.execute_reply.started":"2025-04-28T03:36:25.424752Z","shell.execute_reply":"2025-04-28T03:36:25.435237Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Create training dataset\ndef create_dataset(spad_paths, depth_paths, batch_size=BATCH_SIZE, \n                   buffer_size=400, train=True):\n    dataset = tf.data.Dataset.from_tensor_slices((spad_paths, depth_paths))\n    \n    dataset = dataset.map(load_and_preprocess_data, \n                          num_parallel_calls=tf.data.AUTOTUNE)\n    \n    # if train:\n    #     dataset = dataset.map(random_jitter, \n    #                          num_parallel_calls=tf.data.AUTOTUNE)\n    \n    dataset = dataset.map(normalize, \n                         num_parallel_calls=tf.data.AUTOTUNE)\n    \n    dataset = dataset.shuffle(buffer_size)\n    dataset = dataset.batch(batch_size)\n    \n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.436603Z","iopub.execute_input":"2025-04-28T03:36:25.436850Z","iopub.status.idle":"2025-04-28T03:36:25.449517Z","shell.execute_reply.started":"2025-04-28T03:36:25.436828Z","shell.execute_reply":"2025-04-28T03:36:25.448954Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Training the model\n@tf.function\ndef train_step(generator, discriminator, generator_optimizer, discriminator_optimizer, \n               spad_images, depth_maps):\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        # Generate depth maps from SPAD images\n        generated_depths = generator(spad_images, training=True)\n        \n        # Discriminator predictions\n        disc_real_output = discriminator([spad_images, depth_maps], training=True)\n        disc_generated_output = discriminator([spad_images, generated_depths], training=True)\n        \n        # Calculate losses\n        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(\n            disc_generated_output, generated_depths, depth_maps)\n        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n    \n    # Calculate gradients\n    generator_gradients = gen_tape.gradient(\n        gen_total_loss, generator.trainable_variables)\n    discriminator_gradients = disc_tape.gradient(\n        disc_loss, discriminator.trainable_variables)\n    \n    # Apply gradients\n    generator_optimizer.apply_gradients(\n        zip(generator_gradients, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(\n        zip(discriminator_gradients, discriminator.trainable_variables))\n    \n    return gen_total_loss, gen_gan_loss, gen_l1_loss, disc_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.450178Z","iopub.execute_input":"2025-04-28T03:36:25.450375Z","iopub.status.idle":"2025-04-28T03:36:25.464738Z","shell.execute_reply.started":"2025-04-28T03:36:25.450360Z","shell.execute_reply":"2025-04-28T03:36:25.464238Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def train(generator, discriminator, generator_optimizer, discriminator_optimizer, \n#           train_dataset, checkpoint_filepath='/kaggle/working/outputs/checkpoints'):\ndef train(generator, discriminator, generator_optimizer, discriminator_optimizer,\n          train_dataset, val_dataset=None, checkpoint_filepath='/kaggle/working/outputs/checkpoints'):\n    for epoch in range(EPOCHS):\n        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n        start_time = time.time()\n\n        # Train step\n        for n, (spad_batch, depth_batch) in enumerate(train_dataset):\n            gen_total_loss, gen_gan_loss, gen_l1_loss, disc_loss = train_step(\n                generator, discriminator, generator_optimizer, discriminator_optimizer,\n                spad_batch, depth_batch)\n\n            if n % 10 == 0:\n                print(f\"  [Train] Batch {n:03d} - Gen Loss: {gen_total_loss:.4f}, \"\n                      f\"GAN: {gen_gan_loss:.4f}, L1: {gen_l1_loss:.4f}, Disc: {disc_loss:.4f}\")\n\n        # --- Validation step (optional) ---\n        if val_dataset is not None:\n            mae_metric = tf.keras.metrics.MeanAbsoluteError()\n            mse_metric = tf.keras.metrics.MeanSquaredError()\n            ssim_scores = []\n\n            for spad_val_batch, depth_val_batch in val_dataset:\n                pred_val_batch = generator(spad_val_batch, training=False)\n                mae_metric.update_state(depth_val_batch, pred_val_batch)\n                mse_metric.update_state(depth_val_batch, pred_val_batch)\n                ssim_score = tf.reduce_mean(tf.image.ssim(depth_val_batch, pred_val_batch, max_val=2.0))\n                ssim_scores.append(ssim_score.numpy())\n\n            val_mae = mae_metric.result().numpy()\n            val_rmse = np.sqrt(mse_metric.result().numpy())\n            val_ssim = np.mean(ssim_scores)\n\n            print(f\"  [Val] MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}, SSIM: {val_ssim:.4f}\")\n\n        # Save checkpoint every 10 epochs or at the last one\n        if (epoch + 1) % 10 == 0 or epoch == EPOCHS - 1:\n            checkpoint_prefix = os.path.join(checkpoint_filepath, f\"ckpt_epoch_{epoch+1}\")\n            checkpoint = tf.train.Checkpoint(\n                generator_optimizer=generator_optimizer,\n                discriminator_optimizer=discriminator_optimizer,\n                generator=generator,\n                discriminator=discriminator)\n            checkpoint.save(file_prefix=checkpoint_prefix)\n            print(f\"Checkpoint saved: {checkpoint_prefix}\")\n\n            # Generate and save visualization\n            for idx, (spad, depth) in enumerate(train_dataset.take(1)):\n                generate_and_save_images(generator, epoch + 1, spad, depth)\n                break\n\n        print(f\"Epoch {epoch+1} completed in {time.time() - start_time:.2f} seconds.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.465335Z","iopub.execute_input":"2025-04-28T03:36:25.465499Z","iopub.status.idle":"2025-04-28T03:36:25.481537Z","shell.execute_reply.started":"2025-04-28T03:36:25.465480Z","shell.execute_reply":"2025-04-28T03:36:25.480956Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def generate_and_save_images(model, epoch, spad_batch, depth_batch):\n    # Generate images\n    predictions = model(spad_batch, training=False)\n    \n    fig = plt.figure(figsize=(15, 10))\n    \n    display_list = [spad_batch[0], depth_batch[0], predictions[0]]\n    title = ['SPAD Input', 'Ground Truth Depth', 'Predicted Depth']\n    \n    for i in range(3):\n        plt.subplot(1, 3, i+1)\n        plt.title(title[i])\n        # Getting the pixel values in the [0, 1] range to plot.\n        plt.imshow(display_list[i] * 0.5 + 0.5, cmap='viridis')\n        plt.axis('off')\n    \n    plt.savefig(f'predictions_epoch_{epoch}.png')\n    plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.482178Z","iopub.execute_input":"2025-04-28T03:36:25.482457Z","iopub.status.idle":"2025-04-28T03:36:25.497523Z","shell.execute_reply.started":"2025-04-28T03:36:25.482442Z","shell.execute_reply":"2025-04-28T03:36:25.496932Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Inference function for testing on new SPAD images\ndef predict_depth_map(generator, spad_image_path):\n    # Load and preprocess SPAD image\n    spad_img = tf.io.read_file(spad_image_path)\n    spad_img = tf.image.decode_png(spad_img, channels=1)\n    spad_img = tf.image.resize(spad_img, [IMG_HEIGHT, IMG_WIDTH])\n    spad_img = tf.cast(spad_img, tf.float32)\n    spad_img = (spad_img / 255.0) * 2 - 1  # Normalize to [-1, 1]\n    spad_img = tf.expand_dims(spad_img, 0)  # Add batch dimension\n    \n    # Generate depth map\n    predicted_depth = generator(spad_img, training=False)\n    \n    # Denormalize\n    predicted_depth = (predicted_depth[0] + 1) / 2\n    \n    return predicted_depth.numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.498152Z","iopub.execute_input":"2025-04-28T03:36:25.498644Z","iopub.status.idle":"2025-04-28T03:36:25.510085Z","shell.execute_reply.started":"2025-04-28T03:36:25.498606Z","shell.execute_reply":"2025-04-28T03:36:25.509511Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# # Class for img2csv functionality if the module is not available\n# class Img2CSV:\n#     @staticmethod\n#     def convert_to_csv(image_dir, output_csv):\n#         \"\"\"\n#         Convert depth map images to a submission CSV file.\n        \n#         Args:\n#             image_dir (str): Directory containing depth map PNG images\n#             output_csv (str): Output CSV file path\n#         \"\"\"\n#         print(f\"Converting depth maps from {image_dir} to CSV...\")\n#         all_data = []\n        \n#         # Get all PNG files in the directory\n#         image_files = [f for f in os.listdir(image_dir) if f.endswith(\"_depth.png\")]\n        \n#         for image_file in tqdm(image_files, desc=\"Processing images\"):\n#             # Extract the image ID (remove _depth.png)\n#             image_id = image_file.replace(\"_depth.png\", \"\")\n            \n#             # Load the depth map\n#             depth_map_path = os.path.join(image_dir, image_file)\n#             depth_map = plt.imread(depth_map_path)\n            \n#             # Handle RGB images by converting to grayscale if necessary\n#             if len(depth_map.shape) > 2:\n#                 depth_map = np.mean(depth_map, axis=2)\n            \n#             # Flatten the depth map and create rows\n#             flat_depth = depth_map.flatten()\n            \n#             for pixel_id, depth_value in enumerate(flat_depth):\n#                 all_data.append({\n#                     'image_id': image_id,\n#                     'pixel_id': pixel_id,\n#                     'depth': depth_value  # Already in range [0, 1] if read with plt.imread\n#                 })\n        \n#         # Create and save dataframe\n#         df = pd.DataFrame(all_data)\n#         df.to_csv(output_csv, index=False)\n#         print(f\"CSV file created: {output_csv}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.510714Z","iopub.execute_input":"2025-04-28T03:36:25.510907Z","iopub.status.idle":"2025-04-28T03:36:25.527788Z","shell.execute_reply.started":"2025-04-28T03:36:25.510893Z","shell.execute_reply":"2025-04-28T03:36:25.527198Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# # If img2csv.py is not available, use the built-in implementation\n# if 'img2csv' not in globals():\n#     img2csv = Img2CSV","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.528371Z","iopub.execute_input":"2025-04-28T03:36:25.528565Z","iopub.status.idle":"2025-04-28T03:36:25.545216Z","shell.execute_reply.started":"2025-04-28T03:36:25.528550Z","shell.execute_reply":"2025-04-28T03:36:25.544598Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Function to evaluate the model using various metrics\ndef evaluate_model(generator, val_dataset):\n    \"\"\"\n    Evaluate the model using various metrics: MAE, RMSE, SSIM\n    \n    Args:\n        generator: Trained generator model\n        val_dataset: Validation dataset\n    \n    Returns:\n        Dictionary of metrics\n    \"\"\"\n    mae_metric = tf.keras.metrics.MeanAbsoluteError()\n    mse_metric = tf.keras.metrics.MeanSquaredError()\n    \n    # SSIM implementation\n    def ssim(img1, img2):\n        return tf.reduce_mean(tf.image.ssim(img1, img2, max_val=2.0))\n    \n    ssim_scores = []\n    \n    print(\"Evaluating model...\")\n    for i, (spad_batch, depth_batch) in enumerate(tqdm(val_dataset)):\n        # Generate predictions\n        pred_batch = generator(spad_batch, training=False)\n        \n        # Update metrics\n        mae_metric.update_state(depth_batch, pred_batch)\n        mse_metric.update_state(depth_batch, pred_batch)\n        \n        # Calculate and collect SSIM scores\n        ssim_value = ssim(depth_batch, pred_batch)\n        ssim_scores.append(ssim_value.numpy())\n    \n    # Calculate final metrics\n    mae = mae_metric.result().numpy()\n    rmse = np.sqrt(mse_metric.result().numpy())\n    mean_ssim = np.mean(ssim_scores)\n    \n    metrics = {\n        'MAE': mae,\n        'RMSE': rmse,\n        'SSIM': mean_ssim\n    }\n    \n    print(\"Evaluation metrics:\")\n    for metric_name, metric_value in metrics.items():\n        print(f\"  {metric_name}: {metric_value:.4f}\")\n    \n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.545803Z","iopub.execute_input":"2025-04-28T03:36:25.546004Z","iopub.status.idle":"2025-04-28T03:36:25.559182Z","shell.execute_reply.started":"2025-04-28T03:36:25.545990Z","shell.execute_reply":"2025-04-28T03:36:25.558652Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Function to load a trained model from checkpoint\ndef load_model_from_checkpoint(checkpoint_path):\n    # Initialize models\n    generator = build_generator()\n    discriminator = build_discriminator()\n    \n    # Initialize optimizers\n    generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    \n    # Create checkpoint\n    checkpoint = tf.train.Checkpoint(\n        generator_optimizer=generator_optimizer,   \n        discriminator_optimizer=discriminator_optimizer,\n        generator=generator,\n        discriminator=discriminator)\n    \n    # Restore checkpoint\n    checkpoint.restore(checkpoint_path).expect_partial()\n    print(f\"Model loaded from checkpoint: {checkpoint_path}\")\n    \n    return generator, discriminator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.559786Z","iopub.execute_input":"2025-04-28T03:36:25.559992Z","iopub.status.idle":"2025-04-28T03:36:25.574149Z","shell.execute_reply.started":"2025-04-28T03:36:25.559977Z","shell.execute_reply":"2025-04-28T03:36:25.573540Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# # Main function to run the entire pipeline\n# def main():\n#     # Initialize models\n#     generator = build_generator()\n#     discriminator = build_discriminator()\n    \n#     # Optimizers\n#     generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n#     discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    \n#     # Dataset paths (replace with actual paths)\n#     SPAD_TRAIN_PATH = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/training-images'\n#     DEPTH_TRAIN_PATH = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/training-depths'\n#     # Add validation dataset paths\n#     SPAD_VAL_PATH = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/validation-images'\n#     DEPTH_VAL_PATH = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/validation-depths'\n#     SPAD_TEST_PATH = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/testing-images'\n#     OUTPUT_DIR = '/kaggle/working/outputs'\n#     CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, 'checkpoints')\n#     TEST_OUTPUT_DIR = os.path.join(OUTPUT_DIR, 'test_predictions')\n    \n#     # Create output directories\n#     os.makedirs(OUTPUT_DIR, exist_ok=True)\n#     os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n#     os.makedirs(TEST_OUTPUT_DIR, exist_ok=True)\n    \n#     # Get lists of image paths\n#     spad_train_images = sorted([os.path.join(SPAD_TRAIN_PATH, filename) \n#                          for filename in os.listdir(SPAD_TRAIN_PATH) \n#                          if filename.endswith('.png')])\n#     depth_train_maps = sorted([os.path.join(DEPTH_TRAIN_PATH, filename) \n#                         for filename in os.listdir(DEPTH_TRAIN_PATH) \n#                         if filename.endswith('.png')])\n    \n#     # Add validation image paths\n#     print(\"Loading validation data...\")\n#     try:\n#         spad_val_images = sorted([os.path.join(SPAD_VAL_PATH, filename) \n#                            for filename in os.listdir(SPAD_VAL_PATH) \n#                            if filename.endswith('.png')])\n#         depth_val_maps = sorted([os.path.join(DEPTH_VAL_PATH, filename) \n#                           for filename in os.listdir(DEPTH_VAL_PATH) \n#                           if filename.endswith('.png')])\n        \n#         # Debug: Check if validation directories exist and contain images\n#         print(f\"Found {len(spad_val_images)} validation SPAD images and {len(depth_val_maps)} validation depth maps\")\n        \n#         # Debug: Check if validation pairs match\n#         if len(spad_val_images) != len(depth_val_maps):\n#             print(f\"WARNING: Validation dataset has mismatched counts - {len(spad_val_images)} SPAD images vs {len(depth_val_maps)} depth maps\")\n            \n#         # Debug: Print first few validation image paths\n#         if spad_val_images:\n#             print(f\"First validation SPAD image: {os.path.basename(spad_val_images[0])}\")\n#         if depth_val_maps:\n#             print(f\"First validation depth map: {os.path.basename(depth_val_maps[0])}\")\n            \n#     except FileNotFoundError as e:\n#         print(f\"ERROR: Validation directory not found: {e}\")\n#         print(\"Falling back to splitting training data...\")\n#         # Fallback to splitting if validation directories don't exist\n#         train_spad_paths, train_depth_paths, spad_val_images, depth_val_maps = split_dataset(\n#             spad_train_images, depth_train_maps, val_split=0.2)\n#         print(f\"Fallback: {len(spad_val_images)} validation images created from training split\")\n    \n#     print(f\"Training on {len(spad_train_images)} images, validating on {len(spad_val_images)} images\")\n    \n#     # Create datasets\n#     train_dataset = create_dataset(spad_train_images, depth_train_maps, batch_size=BATCH_SIZE)\n#     val_dataset = create_dataset(spad_val_images, depth_val_maps, batch_size=BATCH_SIZE, train=False)\n    \n#     # Debug: Check first batch from each dataset\n#     for x_batch, y_batch in train_dataset.take(1):\n#         print(f\"Training batch shape: SPAD={x_batch.shape}, depth={y_batch.shape}\")\n        \n#     for x_batch, y_batch in val_dataset.take(1):\n#         print(f\"Validation batch shape: SPAD={x_batch.shape}, depth={y_batch.shape}\")\n    \n#     # Check if we should load from checkpoint or train from scratch\n#     load_checkpoint = input(\"Load from checkpoint? (y/n): \").lower() == 'y'\n    \n#     if load_checkpoint:\n#         checkpoint_path = input(\"Enter checkpoint path: \")\n#         generator, discriminator = load_model_from_checkpoint(checkpoint_path)\n#     else:\n#         # Train the model\n#         print(\"Starting model training...\")\n#         history = train(generator, discriminator, generator_optimizer, discriminator_optimizer, \n#                         train_dataset, val_dataset, checkpoint_filepath=CHECKPOINT_DIR)\n#         print(\"Training completed!\")\n        \n#         # Evaluate the model\n#         print(\"Evaluating model on validation set...\")\n#         metrics = evaluate_model(generator, val_dataset)\n        \n#         # Save evaluation metrics\n#         with open(os.path.join(OUTPUT_DIR, 'evaluation_metrics.txt'), 'w') as f:\n#             for metric_name, metric_value in metrics.items():\n#                 f.write(f\"{metric_name}: {metric_value:.4f}\\n\")\n    \n#     # Get test images\n#     test_spad_paths = sorted([os.path.join(SPAD_TEST_PATH, filename) \n#                              for filename in os.listdir(SPAD_TEST_PATH) \n#                              if filename.endswith('.png')])\n    \n#     print(f\"Running inference on {len(test_spad_paths)} test images...\")\n    \n#     # Create test dataset\n#     test_dataset = create_test_dataset(test_spad_paths, batch_size=BATCH_SIZE)\n    \n#     # Generate depth maps for test data\n#     predictions_dict = test(generator, test_dataset, test_spad_paths, output_dir=TEST_OUTPUT_DIR)\n    \n#     # Convert predictions to submission CSV\n#     submission_file = convert_to_submission(predictions_dict, \n#                                            output_dir=TEST_OUTPUT_DIR, \n#                                            output_file=os.path.join(OUTPUT_DIR, \"submission.csv\"))\n    \n#     print(f\"Submission file created: {submission_file}\")\n    \n#     # Visualize a few test predictions\n#     print(\"Creating visualization of test predictions...\")\n#     visualize_test_predictions(generator, test_dataset, test_spad_paths, \n#                               num_examples=5, output_dir=TEST_OUTPUT_DIR)\n    \n#     print(\"Pipeline completed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.574801Z","iopub.execute_input":"2025-04-28T03:36:25.575037Z","iopub.status.idle":"2025-04-28T03:36:25.589978Z","shell.execute_reply.started":"2025-04-28T03:36:25.575016Z","shell.execute_reply":"2025-04-28T03:36:25.589350Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def create_test_dataset(spad_paths, batch_size=BATCH_SIZE):\n    dataset = tf.data.Dataset.from_tensor_slices(spad_paths)\n    dataset = dataset.map(load_and_preprocess_test_data, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(batch_size)\n    return dataset\n\ndef load_and_preprocess_test_data(spad_path):\n    # Load SPAD binary image (0 or 1 values)\n    spad_img = tf.io.read_file(spad_path)\n    spad_img = tf.image.decode_png(spad_img, channels=1)\n    # spad_img = tf.image.resize(spad_img, [IMG_HEIGHT, IMG_WIDTH])  # Resize if necessary\n    spad_img = tf.cast(spad_img, tf.float32)\n    spad_img = (spad_img / 255.0) * 2 - 1  # Normalize to [-1, 1]\n    return spad_img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.590698Z","iopub.execute_input":"2025-04-28T03:36:25.591318Z","iopub.status.idle":"2025-04-28T03:36:25.604694Z","shell.execute_reply.started":"2025-04-28T03:36:25.591296Z","shell.execute_reply":"2025-04-28T03:36:25.604099Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Function to visualize test predictions\ndef visualize_test_predictions(generator, test_dataset, test_filenames, num_examples=5, output_dir=\".\"):\n    \"\"\"\n    Visualize test predictions for a few examples\n    \n    Args:\n        generator: Trained generator model\n        test_dataset: Test dataset\n        test_filenames: List of test filenames\n        num_examples: Number of examples to visualize\n        output_dir: Output directory for visualizations\n    \"\"\"\n    plt.figure(figsize=(12, 8))\n    \n    for i, spad_batch in enumerate(test_dataset):\n        if i >= num_examples:\n            break\n            \n        # Generate predictions\n        predicted_depths = generator(spad_batch, training=False)\n        \n        # Denormalize\n        spad_images = (spad_batch + 1) / 2\n        predicted_depths = (predicted_depths + 1) / 2\n        \n        # Display a few examples\n        for j in range(min(3, spad_batch.shape[0])):\n            # Get the filename\n            idx = i * BATCH_SIZE + j\n            if idx >= len(test_filenames):\n                break\n                \n            filename = os.path.basename(test_filenames[idx])\n            \n            plt.subplot(min(3, spad_batch.shape[0]), 2, j*2+1)\n            plt.title(f\"SPAD Input: {filename}\")\n            plt.imshow(spad_images[j], cmap='gray')\n            plt.axis('off')\n            \n            plt.subplot(min(3, spad_batch.shape[0]), 2, j*2+2)\n            plt.title(f\"Predicted Depth\")\n            plt.imshow(predicted_depths[j], cmap='viridis')\n            plt.axis('off')\n        \n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, f\"test_predictions_batch_{i}.png\"))\n        plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.607120Z","iopub.execute_input":"2025-04-28T03:36:25.607344Z","iopub.status.idle":"2025-04-28T03:36:25.617529Z","shell.execute_reply.started":"2025-04-28T03:36:25.607331Z","shell.execute_reply":"2025-04-28T03:36:25.616886Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.618282Z","iopub.execute_input":"2025-04-28T03:36:25.618741Z","iopub.status.idle":"2025-04-28T03:36:25.633550Z","shell.execute_reply.started":"2025-04-28T03:36:25.618718Z","shell.execute_reply":"2025-04-28T03:36:25.632968Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# /kaggle/working/checkpoints","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.634223Z","iopub.execute_input":"2025-04-28T03:36:25.634483Z","iopub.status.idle":"2025-04-28T03:36:25.645811Z","shell.execute_reply.started":"2025-04-28T03:36:25.634463Z","shell.execute_reply":"2025-04-28T03:36:25.645158Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def test(generator, test_dataset, test_filenames, output_dir=\".\"):\n    predictions_dict = {}  # Dictionary to store predictions\n    \n    for i, spad_batch in enumerate(test_dataset):\n        # Generate predictions\n        predicted_depths = generator(spad_batch, training=False)\n        \n        \n        # Denormalize predictions\n        predicted_depths = (predicted_depths + 1) / 2\n        \n        # Save predictions for each image in the batch\n        for j in range(spad_batch.shape[0]):\n            # Get the filename\n            # print(predicted_depths[j].numpy().shape)\n            idx = i * BATCH_SIZE + j\n            if idx >= len(test_filenames):\n                break\n            \n            filename = os.path.basename(test_filenames[idx])\n            image_id = filename.replace(\".png\", \"\")\n            \n            predictions_dict[image_id] = predicted_depths[j].numpy()  # Store prediction\n            \n            # Save the predicted depth map\n            output_path = os.path.join(output_dir, f\"{image_id}.png\")\n            # Clip values and ensure correct type\n            depth_map_to_save = predicted_depths[j].numpy()\n            depth_map_to_save = np.clip(depth_map_to_save, 0, 1)\n\n            # Convert to grayscale and save\n            depth_map_to_save = tf.image.convert_image_dtype(depth_map_to_save, dtype=tf.uint8)\n            # plt.imsave(output_path, predicted_depths[j].numpy(), cmap='viridis')\n            plt.imsave(output_path, np.squeeze(predicted_depths[j].numpy()), cmap='viridis')\n\n    \n    return predictions_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.646675Z","iopub.execute_input":"2025-04-28T03:36:25.646881Z","iopub.status.idle":"2025-04-28T03:36:25.657730Z","shell.execute_reply.started":"2025-04-28T03:36:25.646867Z","shell.execute_reply":"2025-04-28T03:36:25.657018Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def convert_to_submission(image_folder, output_csv):\n    \"\"\"\n    Converts prediction images into a submission CSV exactly like images_to_csv_with_metadata.\n\n    Args:\n        image_folder (str): Path to folder containing prediction images.\n        output_csv (str): Path to save the final submission CSV.\n\n    Returns:\n        str: Path to the saved submission CSV.\n    \"\"\"\n    data = []\n\n    # Loop through all images in the folder\n    for idx, filename in enumerate(sorted(os.listdir(image_folder))):\n        if filename.endswith(\".png\"):\n            filepath = os.path.join(image_folder, filename)\n            # Read the image\n            image = cv2.imread(filepath, cv2.IMREAD_UNCHANGED)\n            # Resize to (128, 128)\n            image = cv2.resize(image, (128, 128))\n            # Normalize the image\n            image = image / 255.0\n            image = (image - np.min(image)) / (np.max(image) - np.min(image) + 1e-6)\n            image = np.uint8(image * 255.0)\n            # Flatten the image\n            image_flat = image.flatten()\n            # Create a row: [id, ImageID, pixel0, pixel1, pixel2, ...]\n            row = [idx, filename] + image_flat.tolist()\n            data.append(row)\n\n    # Column names\n    num_pixels = len(data[0]) - 2 if data else 0\n    column_names = [\"id\", \"ImageID\"] + [str(i) for i in range(num_pixels)]\n\n    # Create DataFrame\n    submission_df = pd.DataFrame(data, columns=column_names)\n\n    # Ensure output directory exists\n    os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n\n    # Save to CSV\n    submission_df.to_csv(output_csv, index=False)\n\n    print(f\" Submission file saved to: {output_csv}\")\n    return output_csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.658398Z","iopub.execute_input":"2025-04-28T03:36:25.658589Z","iopub.status.idle":"2025-04-28T03:36:25.674811Z","shell.execute_reply.started":"2025-04-28T03:36:25.658569Z","shell.execute_reply":"2025-04-28T03:36:25.674174Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"generator = build_generator()\ndiscriminator = build_discriminator()\n\n# Optimizers\ngenerator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n# Dataset paths (replace with actual paths)\nSPAD_TRAIN_PATH = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/training-images'\nDEPTH_TRAIN_PATH = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/training-depths'\n# Add validation dataset paths\nSPAD_VAL_PATH = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/validation-images'\nDEPTH_VAL_PATH = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/validation-depths'\nSPAD_TEST_PATH = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/testing-images'\nOUTPUT_DIR = '/kaggle/working/outputs'\nCHECKPOINT_DIR = os.path.join(OUTPUT_DIR, 'checkpoints')\nTEST_OUTPUT_DIR = os.path.join(OUTPUT_DIR, 'test_predictions')\nVISUAL_OUTPUT_DIR = os.path.join(OUTPUT_DIR, 'test_visualizations')\n# Create output directories\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nos.makedirs(TEST_OUTPUT_DIR, exist_ok=True)\nos.makedirs(VISUAL_OUTPUT_DIR, exist_ok=True)\n\n\n# Get lists of image paths\nspad_train_images = sorted([os.path.join(SPAD_TRAIN_PATH, filename) \n                     for filename in os.listdir(SPAD_TRAIN_PATH) \n                     if filename.endswith('.png')])\ndepth_train_maps = sorted([os.path.join(DEPTH_TRAIN_PATH, filename) \n                    for filename in os.listdir(DEPTH_TRAIN_PATH) \n                    if filename.endswith('.png')])\n\n# Add validation image paths\nprint(\"Loading validation data...\")\ntry:\n    spad_val_images = sorted([os.path.join(SPAD_VAL_PATH, filename) \n                       for filename in os.listdir(SPAD_VAL_PATH) \n                       if filename.endswith('.png')])\n    depth_val_maps = sorted([os.path.join(DEPTH_VAL_PATH, filename) \n                      for filename in os.listdir(DEPTH_VAL_PATH) \n                      if filename.endswith('.png')])\n    \n    # Debug: Check if validation directories exist and contain images\n    print(f\"Found {len(spad_val_images)} validation SPAD images and {len(depth_val_maps)} validation depth maps\")\n    \n    # Debug: Check if validation pairs match\n    if len(spad_val_images) != len(depth_val_maps):\n        print(f\"WARNING: Validation dataset has mismatched counts - {len(spad_val_images)} SPAD images vs {len(depth_val_maps)} depth maps\")\n        \n    # Debug: Print first few validation image paths\n    if spad_val_images:\n        print(f\"First validation SPAD image: {os.path.basename(spad_val_images[0])}\")\n    if depth_val_maps:\n        print(f\"First validation depth map: {os.path.basename(depth_val_maps[0])}\")\n        \nexcept FileNotFoundError as e:\n    print(f\"ERROR: Validation directory not found: {e}\")\n    print(\"Falling back to splitting training data...\")\n    # Fallback to splitting if validation directories don't exist\n    train_spad_paths, train_depth_paths, spad_val_images, depth_val_maps = split_dataset(\n        spad_train_images, depth_train_maps, val_split=0.2)\n    print(f\"Fallback: {len(spad_val_images)} validation images created from training split\")\n\nprint(f\"Training on {len(spad_train_images)} images, validating on {len(spad_val_images)} images\")\n\n# Create datasets\ntrain_dataset = create_dataset(spad_train_images, depth_train_maps, batch_size=BATCH_SIZE)\nval_dataset = create_dataset(spad_val_images, depth_val_maps, batch_size=BATCH_SIZE, train=False)\n\n# Debug: Check first batch from each dataset\nfor x_batch, y_batch in train_dataset.take(1):\n    print(f\"Training batch shape: SPAD={x_batch.shape}, depth={y_batch.shape}\")\n    \nfor x_batch, y_batch in val_dataset.take(1):\n    print(f\"Validation batch shape: SPAD={x_batch.shape}, depth={y_batch.shape}\")\n\n# Check if we should load from checkpoint or train from scratch\n# load_checkpoint = input(\"Load from checkpoint? (y/n): \").lower() == 'y'\nload_checkpoint = False\nif load_checkpoint:\n    checkpoint_path = input(\"Enter checkpoint path: \")\n    generator, discriminator = load_model_from_checkpoint(checkpoint_path)\nelse:\n    # Train the model\n    print(\"Starting model training...\")\n    history = train(generator, discriminator, generator_optimizer, discriminator_optimizer, \n                    train_dataset, val_dataset, checkpoint_filepath=CHECKPOINT_DIR)\n    print(\"Training completed!\")\n    \n    # Evaluate the model\n    print(\"Evaluating model on validation set...\")\n    metrics = evaluate_model(generator, val_dataset)\n    \n    # Save evaluation metrics\n    with open(os.path.join(OUTPUT_DIR, 'evaluation_metrics.txt'), 'w') as f:\n        for metric_name, metric_value in metrics.items():\n            f.write(f\"{metric_name}: {metric_value:.4f}\\n\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:36:25.675420Z","iopub.execute_input":"2025-04-28T03:36:25.675583Z","iopub.status.idle":"2025-04-28T03:41:26.825417Z","shell.execute_reply.started":"2025-04-28T03:36:25.675570Z","shell.execute_reply":"2025-04-28T03:41:26.824686Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1745811386.655034      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Loading validation data...\nFound 836 validation SPAD images and 836 validation depth maps\nFirst validation SPAD image: 01.png\nFirst validation depth map: 01.png\nTraining on 6686 images, validating on 836 images\nTraining batch shape: SPAD=(16, 256, 256, 1), depth=(16, 256, 256, 1)\nValidation batch shape: SPAD=(16, 256, 256, 1), depth=(16, 256, 256, 1)\nStarting model training...\n\nEpoch 1/2\n","output_type":"stream"},{"name":"stderr","text":"E0000 00:00:1745811404.609457      31 meta_optimizer.cc:966] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape infunctional_15_1/sequential_8_1/dropout_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\nI0000 00:00:1745811405.408435      91 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"  [Train] Batch 000 - Gen Loss: 65.5629, GAN: 0.8078, L1: 0.6476, Disc: 1.6827\n  [Train] Batch 010 - Gen Loss: 49.2997, GAN: 0.7750, L1: 0.4852, Disc: 1.5114\n  [Train] Batch 020 - Gen Loss: 29.2474, GAN: 0.7523, L1: 0.2850, Disc: 1.4562\n  [Train] Batch 030 - Gen Loss: 30.0710, GAN: 0.8091, L1: 0.2926, Disc: 1.5872\n  [Train] Batch 040 - Gen Loss: 29.1735, GAN: 0.7412, L1: 0.2843, Disc: 1.4263\n  [Train] Batch 050 - Gen Loss: 31.3994, GAN: 0.7942, L1: 0.3061, Disc: 1.4926\n  [Train] Batch 060 - Gen Loss: 33.8142, GAN: 0.7509, L1: 0.3306, Disc: 1.4162\n  [Train] Batch 070 - Gen Loss: 24.8264, GAN: 0.7515, L1: 0.2407, Disc: 1.3737\n  [Train] Batch 080 - Gen Loss: 28.8464, GAN: 0.7295, L1: 0.2812, Disc: 1.3739\n  [Train] Batch 090 - Gen Loss: 26.9510, GAN: 0.7995, L1: 0.2615, Disc: 1.4692\n  [Train] Batch 100 - Gen Loss: 29.2263, GAN: 0.7198, L1: 0.2851, Disc: 1.3812\n  [Train] Batch 110 - Gen Loss: 21.9750, GAN: 0.7696, L1: 0.2121, Disc: 1.3627\n  [Train] Batch 120 - Gen Loss: 20.0127, GAN: 0.8000, L1: 0.1921, Disc: 1.4102\n  [Train] Batch 130 - Gen Loss: 23.3724, GAN: 0.7645, L1: 0.2261, Disc: 1.3349\n  [Train] Batch 140 - Gen Loss: 25.1225, GAN: 0.8492, L1: 0.2427, Disc: 1.2790\n  [Train] Batch 150 - Gen Loss: 29.7349, GAN: 0.8443, L1: 0.2889, Disc: 1.2535\n  [Train] Batch 160 - Gen Loss: 23.4864, GAN: 0.7409, L1: 0.2275, Disc: 1.3726\n  [Train] Batch 170 - Gen Loss: 29.2634, GAN: 0.8036, L1: 0.2846, Disc: 1.1828\n  [Train] Batch 180 - Gen Loss: 24.1513, GAN: 0.8494, L1: 0.2330, Disc: 1.1617\n  [Train] Batch 190 - Gen Loss: 26.3504, GAN: 0.8780, L1: 0.2547, Disc: 1.1649\n  [Train] Batch 200 - Gen Loss: 26.5326, GAN: 0.8845, L1: 0.2565, Disc: 1.1055\n  [Train] Batch 210 - Gen Loss: 25.3283, GAN: 1.0922, L1: 0.2424, Disc: 1.3316\n  [Train] Batch 220 - Gen Loss: 25.7327, GAN: 1.0154, L1: 0.2472, Disc: 1.1205\n  [Train] Batch 230 - Gen Loss: 31.7609, GAN: 1.0354, L1: 0.3073, Disc: 1.0219\n  [Train] Batch 240 - Gen Loss: 28.4507, GAN: 0.8143, L1: 0.2764, Disc: 1.6025\n  [Train] Batch 250 - Gen Loss: 30.5124, GAN: 1.0832, L1: 0.2943, Disc: 0.9422\n  [Train] Batch 260 - Gen Loss: 35.5278, GAN: 1.0932, L1: 0.3443, Disc: 1.0994\n  [Train] Batch 270 - Gen Loss: 37.7816, GAN: 1.0758, L1: 0.3671, Disc: 0.8301\n  [Train] Batch 280 - Gen Loss: 30.3181, GAN: 1.0290, L1: 0.2929, Disc: 1.4112\n  [Train] Batch 290 - Gen Loss: 26.8144, GAN: 0.9767, L1: 0.2584, Disc: 1.1411\n  [Train] Batch 300 - Gen Loss: 37.1927, GAN: 0.7810, L1: 0.3641, Disc: 1.2721\n  [Train] Batch 310 - Gen Loss: 22.5044, GAN: 0.9136, L1: 0.2159, Disc: 1.1243\n  [Train] Batch 320 - Gen Loss: 21.7216, GAN: 0.7118, L1: 0.2101, Disc: 1.2042\n  [Train] Batch 330 - Gen Loss: 23.9047, GAN: 0.6920, L1: 0.2321, Disc: 1.3103\n  [Train] Batch 340 - Gen Loss: 27.5818, GAN: 0.7288, L1: 0.2685, Disc: 1.4445\n  [Train] Batch 350 - Gen Loss: 19.9946, GAN: 0.9940, L1: 0.1900, Disc: 1.3980\n  [Train] Batch 360 - Gen Loss: 21.6822, GAN: 1.1577, L1: 0.2052, Disc: 1.2043\n  [Train] Batch 370 - Gen Loss: 27.8767, GAN: 1.0081, L1: 0.2687, Disc: 0.8953\n  [Train] Batch 380 - Gen Loss: 20.5012, GAN: 0.8841, L1: 0.1962, Disc: 1.1382\n  [Train] Batch 390 - Gen Loss: 20.6521, GAN: 0.8640, L1: 0.1979, Disc: 1.0648\n  [Train] Batch 400 - Gen Loss: 17.8169, GAN: 0.8401, L1: 0.1698, Disc: 1.0949\n  [Train] Batch 410 - Gen Loss: 18.1042, GAN: 1.0594, L1: 0.1704, Disc: 1.1476\n","output_type":"stream"},{"name":"stderr","text":"E0000 00:00:1745811539.721283      31 meta_optimizer.cc:966] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape infunctional_15_1/sequential_8_1/dropout_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"  [Val] MAE: 0.3127, RMSE: 0.4819, SSIM: 0.5043\nEpoch 1 completed in 156.94 seconds.\n\nEpoch 2/2\n  [Train] Batch 000 - Gen Loss: 34.0834, GAN: 0.9620, L1: 0.3312, Disc: 0.8262\n  [Train] Batch 010 - Gen Loss: 29.9438, GAN: 0.8195, L1: 0.2912, Disc: 1.1860\n  [Train] Batch 020 - Gen Loss: 26.6512, GAN: 1.1487, L1: 0.2550, Disc: 0.9704\n  [Train] Batch 030 - Gen Loss: 26.6534, GAN: 1.0196, L1: 0.2563, Disc: 1.2460\n  [Train] Batch 040 - Gen Loss: 28.1243, GAN: 1.3319, L1: 0.2679, Disc: 0.7023\n  [Train] Batch 050 - Gen Loss: 29.2357, GAN: 1.4598, L1: 0.2778, Disc: 0.9668\n  [Train] Batch 060 - Gen Loss: 27.3224, GAN: 0.9617, L1: 0.2636, Disc: 0.8327\n  [Train] Batch 070 - Gen Loss: 34.4192, GAN: 1.0376, L1: 0.3338, Disc: 0.8503\n  [Train] Batch 080 - Gen Loss: 28.4201, GAN: 1.4162, L1: 0.2700, Disc: 0.7409\n  [Train] Batch 090 - Gen Loss: 22.9360, GAN: 1.4197, L1: 0.2152, Disc: 0.7594\n  [Train] Batch 100 - Gen Loss: 22.2931, GAN: 1.1264, L1: 0.2117, Disc: 0.8306\n  [Train] Batch 110 - Gen Loss: 24.0333, GAN: 1.6567, L1: 0.2238, Disc: 0.9894\n  [Train] Batch 120 - Gen Loss: 22.1143, GAN: 1.6731, L1: 0.2044, Disc: 0.9021\n  [Train] Batch 130 - Gen Loss: 21.8322, GAN: 0.8692, L1: 0.2096, Disc: 1.1384\n  [Train] Batch 140 - Gen Loss: 23.6419, GAN: 1.4981, L1: 0.2214, Disc: 0.7779\n  [Train] Batch 150 - Gen Loss: 29.6004, GAN: 1.7885, L1: 0.2781, Disc: 0.5570\n  [Train] Batch 160 - Gen Loss: 29.1261, GAN: 1.2098, L1: 0.2792, Disc: 0.9770\n  [Train] Batch 170 - Gen Loss: 28.8290, GAN: 1.4162, L1: 0.2741, Disc: 0.7715\n  [Train] Batch 180 - Gen Loss: 33.0133, GAN: 1.2455, L1: 0.3177, Disc: 0.8758\n  [Train] Batch 190 - Gen Loss: 24.9933, GAN: 0.9854, L1: 0.2401, Disc: 0.9662\n  [Train] Batch 200 - Gen Loss: 28.8795, GAN: 1.1020, L1: 0.2778, Disc: 1.1287\n  [Train] Batch 210 - Gen Loss: 26.3694, GAN: 1.1395, L1: 0.2523, Disc: 0.8469\n  [Train] Batch 220 - Gen Loss: 26.5982, GAN: 1.7375, L1: 0.2486, Disc: 1.1330\n  [Train] Batch 230 - Gen Loss: 29.0383, GAN: 1.6178, L1: 0.2742, Disc: 1.0735\n  [Train] Batch 240 - Gen Loss: 30.4340, GAN: 0.9520, L1: 0.2948, Disc: 1.0784\n  [Train] Batch 250 - Gen Loss: 22.4917, GAN: 1.3875, L1: 0.2110, Disc: 0.8548\n  [Train] Batch 260 - Gen Loss: 25.1670, GAN: 0.5786, L1: 0.2459, Disc: 1.0923\n  [Train] Batch 270 - Gen Loss: 36.0265, GAN: 0.7104, L1: 0.3532, Disc: 0.8932\n  [Train] Batch 280 - Gen Loss: 26.2626, GAN: 1.7341, L1: 0.2453, Disc: 0.9599\n  [Train] Batch 290 - Gen Loss: 28.1905, GAN: 1.4027, L1: 0.2679, Disc: 0.8906\n  [Train] Batch 300 - Gen Loss: 31.4243, GAN: 0.6234, L1: 0.3080, Disc: 1.0098\n  [Train] Batch 310 - Gen Loss: 30.4180, GAN: 1.7541, L1: 0.2866, Disc: 0.5974\n  [Train] Batch 320 - Gen Loss: 32.1421, GAN: 0.9641, L1: 0.3118, Disc: 0.7972\n  [Train] Batch 330 - Gen Loss: 28.8790, GAN: 0.8507, L1: 0.2803, Disc: 1.0946\n  [Train] Batch 340 - Gen Loss: 22.8116, GAN: 1.2218, L1: 0.2159, Disc: 1.1381\n  [Train] Batch 350 - Gen Loss: 22.5473, GAN: 1.5941, L1: 0.2095, Disc: 1.2933\n  [Train] Batch 360 - Gen Loss: 24.9160, GAN: 0.8746, L1: 0.2404, Disc: 1.0601\n  [Train] Batch 370 - Gen Loss: 24.5420, GAN: 1.1731, L1: 0.2337, Disc: 1.3233\n  [Train] Batch 380 - Gen Loss: 21.8998, GAN: 1.4814, L1: 0.2042, Disc: 0.8008\n  [Train] Batch 390 - Gen Loss: 21.5463, GAN: 0.7634, L1: 0.2078, Disc: 1.0752\n  [Train] Batch 400 - Gen Loss: 17.4611, GAN: 1.5097, L1: 0.1595, Disc: 1.7785\n  [Train] Batch 410 - Gen Loss: 14.8730, GAN: 0.8294, L1: 0.1404, Disc: 1.1123\n  [Val] MAE: 0.3089, RMSE: 0.4792, SSIM: 0.5057\nCheckpoint saved: /kaggle/working/outputs/checkpoints/ckpt_epoch_2\nEpoch 2 completed in 133.45 seconds.\nTraining completed!\nEvaluating model on validation set...\nEvaluating model...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 53/53 [00:04<00:00, 10.65it/s]","output_type":"stream"},{"name":"stdout","text":"Evaluation metrics:\n  MAE: 0.3089\n  RMSE: 0.4792\n  SSIM: 0.5079\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Get test images\ntest_spad_paths = sorted([os.path.join(SPAD_TEST_PATH, filename) \n                         for filename in os.listdir(SPAD_TEST_PATH) \n                         if filename.endswith('.png')])\n\nprint(f\"Running inference on {len(test_spad_paths)} test images...\")\n\n# Create test dataset\ntest_dataset = create_test_dataset(test_spad_paths, batch_size=BATCH_SIZE)\n\n# Generate depth maps for test data\npredictions_dict = test(generator, test_dataset, test_spad_paths, output_dir=TEST_OUTPUT_DIR)\n\n# Convert predictions to submission CSV\n# submission_file = convert_to_submission(\n#                                        output_dir=TEST_OUTPUT_DIR, \n#                                        output_file=os.path.join(OUTPUT_DIR, \"submission.csv\"))\nsubmission_file = convert_to_submission(image_folder=TEST_OUTPUT_DIR,\n                                       output_csv=os.path.join(OUTPUT_DIR, \"submission.csv\"))\nprint(f\"Submission file created: {submission_file}\")\n\n# Visualize a few test predictions\nprint(\"Creating visualization of test predictions...\")\nvisualize_test_predictions(generator, test_dataset, test_spad_paths, \n                          num_examples=5, output_dir=VISUAL_OUTPUT_DIR)\n\nprint(\"Pipeline completed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:41:26.826277Z","iopub.execute_input":"2025-04-28T03:41:26.826807Z","iopub.status.idle":"2025-04-28T03:43:43.272818Z","shell.execute_reply.started":"2025-04-28T03:41:26.826787Z","shell.execute_reply":"2025-04-28T03:43:43.272102Z"}},"outputs":[{"name":"stdout","text":"Running inference on 836 test images...\n Submission file saved to: /kaggle/working/outputs/submission.csv\nSubmission file created: /kaggle/working/outputs/submission.csv\nCreating visualization of test predictions...\nPipeline completed successfully!\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# print(predicted_depths[j].numpy().shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:43:43.273534Z","iopub.execute_input":"2025-04-28T03:43:43.273831Z","iopub.status.idle":"2025-04-28T03:43:43.277079Z","shell.execute_reply.started":"2025-04-28T03:43:43.273813Z","shell.execute_reply":"2025-04-28T03:43:43.276533Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# # Get test images\n# test_spad_paths = sorted([os.path.join(SPAD_TEST_PATH, filename) \n#                          for filename in os.listdir(SPAD_TEST_PATH) \n#                          if filename.endswith('.png')])\n\n# print(f\"Running inference on {len(test_spad_paths)} test images...\")\n\n# # Create test dataset\n# test_dataset = create_test_dataset(test_spad_paths, batch_size=BATCH_SIZE)\n\n# # Generate depth maps for test data\n# predictions_dict = test(generator, test_dataset, test_spad_paths, output_dir=TEST_OUTPUT_DIR)\n\n# # Convert predictions to submission CSV\n# submission_file = convert_to_submission(predictions_dict, \n#                                        output_dir=TEST_OUTPUT_DIR, \n#                                        output_file=os.path.join(OUTPUT_DIR, \"submission.csv\"))\n\n# print(f\"Submission file created: {submission_file}\")\n\n# # Visualize a few test predictions\n# print(\"Creating visualization of test predictions...\")\n# visualize_test_predictions(generator, test_dataset, test_spad_paths, \n#                           num_examples=5, output_dir=TEST_OUTPUT_DIR)\n\n# print(\"Pipeline completed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T03:43:43.277803Z","iopub.execute_input":"2025-04-28T03:43:43.277966Z","iopub.status.idle":"2025-04-28T03:43:43.293194Z","shell.execute_reply.started":"2025-04-28T03:43:43.277953Z","shell.execute_reply":"2025-04-28T03:43:43.292471Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}