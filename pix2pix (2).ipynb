{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":97555,"databundleVersionId":11670858,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model, optimizers\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport pandas as pd\nimport time\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:16:32.602998Z","iopub.execute_input":"2025-04-26T19:16:32.603487Z","iopub.status.idle":"2025-04-26T19:16:32.608085Z","shell.execute_reply.started":"2025-04-26T19:16:32.603451Z","shell.execute_reply":"2025-04-26T19:16:32.607190Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Configuration\nBATCH_SIZE = 16\nIMG_HEIGHT = 256\nIMG_WIDTH = 256\nOUTPUT_CHANNELS = 1  # Depth map is single-channel\nLAMBDA = 100  # Weight for L1 loss in GAN objective\nEPOCHS = 150","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:16:32.988123Z","iopub.execute_input":"2025-04-26T19:16:32.988382Z","iopub.status.idle":"2025-04-26T19:16:32.992438Z","shell.execute_reply.started":"2025-04-26T19:16:32.988361Z","shell.execute_reply":"2025-04-26T19:16:32.991592Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Define the path for the new directory\ncheckpoint_dir = '/kaggle/working/checkpoints'\n\n# Create the directory if it doesn't exist\nos.makedirs(checkpoint_dir, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:16:33.760216Z","iopub.execute_input":"2025-04-26T19:16:33.760483Z","iopub.status.idle":"2025-04-26T19:16:33.764632Z","shell.execute_reply.started":"2025-04-26T19:16:33.760463Z","shell.execute_reply":"2025-04-26T19:16:33.763863Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Generator network (U-Net architecture)\ndef build_generator():\n    # Encoder\n    inputs = layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, 1])  # SPAD images are binary (1 channel)\n    \n    # Downsampling layers\n    down_stack = [\n        downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n        downsample(128, 4),  # (batch_size, 64, 64, 128)\n        downsample(256, 4),  # (batch_size, 32, 32, 256)\n        downsample(512, 4),  # (batch_size, 16, 16, 512)\n        downsample(512, 4),  # (batch_size, 8, 8, 512)\n        downsample(512, 4),  # (batch_size, 4, 4, 512)\n        downsample(512, 4),  # (batch_size, 2, 2, 512)\n        downsample(512, 4),  # (batch_size, 1, 1, 512)\n    ]\n    \n    # Upsampling layers\n    up_stack = [\n        upsample(512, 4, dropout=True),  # (batch_size, 2, 2, 1024)\n        upsample(512, 4, dropout=True),  # (batch_size, 4, 4, 1024)\n        upsample(512, 4, dropout=True),  # (batch_size, 8, 8, 1024)\n        upsample(512, 4),  # (batch_size, 16, 16, 1024)\n        upsample(256, 4),  # (batch_size, 32, 32, 512)\n        upsample(128, 4),  # (batch_size, 64, 64, 256)\n        upsample(64, 4),  # (batch_size, 128, 128, 128)\n    ]\n    \n    # Final layer (output depth map)\n    last = layers.Conv2DTranspose(\n        OUTPUT_CHANNELS, 4, strides=2, padding='same',\n        activation='tanh'  # Using tanh to get output in [-1, 1] range\n    )  # (batch_size, 256, 256, 1)\n    \n    x = inputs\n    \n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n    \n    skips = reversed(skips[:-1])\n    \n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = layers.Concatenate()([x, skip])\n    \n    x = last(x)\n    \n    return Model(inputs=inputs, outputs=x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:16:37.512034Z","iopub.execute_input":"2025-04-26T19:16:37.512721Z","iopub.status.idle":"2025-04-26T19:16:37.519440Z","shell.execute_reply.started":"2025-04-26T19:16:37.512696Z","shell.execute_reply":"2025-04-26T19:16:37.518640Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Discriminator network (PatchGAN)\ndef build_discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    inp = layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, 1], name='input_image')\n    tar = layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, 1], name='target_image')\n    \n    x = layers.Concatenate()([inp, tar])  # (batch_size, 256, 256, 2)\n    \n    down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)\n    down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)\n    down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n    \n    zero_pad1 = layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n    conv = layers.Conv2D(512, 4, strides=1, kernel_initializer=initializer, \n                         use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n    \n    batchnorm1 = layers.BatchNormalization()(conv)\n    leaky_relu = layers.LeakyReLU()(batchnorm1)\n    \n    zero_pad2 = layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n    last = layers.Conv2D(1, 4, strides=1, \n                         kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n    \n    return Model(inputs=[inp, tar], outputs=last)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:16:53.573462Z","iopub.execute_input":"2025-04-26T19:16:53.573976Z","iopub.status.idle":"2025-04-26T19:16:53.579694Z","shell.execute_reply.started":"2025-04-26T19:16:53.573951Z","shell.execute_reply":"2025-04-26T19:16:53.578963Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Helper functions for generator and discriminator\ndef downsample(filters, size, apply_batchnorm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    result = tf.keras.Sequential()\n    result.add(\n        layers.Conv2D(filters, size, strides=2, padding='same',\n                       kernel_initializer=initializer, use_bias=False))\n    \n    if apply_batchnorm:\n        result.add(layers.BatchNormalization())\n    \n    result.add(layers.LeakyReLU())\n    \n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:16:56.676200Z","iopub.execute_input":"2025-04-26T19:16:56.676771Z","iopub.status.idle":"2025-04-26T19:16:56.680967Z","shell.execute_reply.started":"2025-04-26T19:16:56.676747Z","shell.execute_reply":"2025-04-26T19:16:56.680270Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def upsample(filters, size, dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    result = tf.keras.Sequential()\n    result.add(\n        layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n                               kernel_initializer=initializer, use_bias=False))\n    \n    result.add(layers.BatchNormalization())\n    \n    if dropout:\n        result.add(layers.Dropout(0.5))\n    \n    result.add(layers.ReLU())\n    \n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:16:56.908487Z","iopub.execute_input":"2025-04-26T19:16:56.908747Z","iopub.status.idle":"2025-04-26T19:16:56.913374Z","shell.execute_reply.started":"2025-04-26T19:16:56.908727Z","shell.execute_reply":"2025-04-26T19:16:56.912607Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Loss functions\ndef generator_loss(disc_generated_output, gen_output, target):\n    gan_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(\n        tf.ones_like(disc_generated_output), disc_generated_output)\n    \n    # Mean absolute error (L1 loss) between generated and target depth maps\n    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n    \n    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n    \n    return total_gen_loss, gan_loss, l1_loss\n\ndef discriminator_loss(disc_real_output, disc_generated_output):\n    real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(\n        tf.ones_like(disc_real_output), disc_real_output)\n    \n    generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(\n        tf.zeros_like(disc_generated_output), disc_generated_output)\n    \n    total_disc_loss = real_loss + generated_loss\n    \n    return total_disc_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:17:00.101326Z","iopub.execute_input":"2025-04-26T19:17:00.102005Z","iopub.status.idle":"2025-04-26T19:17:00.108599Z","shell.execute_reply.started":"2025-04-26T19:17:00.101971Z","shell.execute_reply":"2025-04-26T19:17:00.107761Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def load_and_preprocess_data(spad_path, depth_path):\n    # Load SPAD binary image (0 or 1 values)\n    spad_img = tf.io.read_file(spad_path)\n    spad_img = tf.image.decode_png(spad_img, channels=1)\n    spad_img = tf.cast(spad_img, tf.float32)\n    spad_img = (spad_img / 255.0) * 2 - 1  # Normalize to [-1, 1]\n    \n    # Load depth map\n    depth_img = tf.io.read_file(depth_path)\n    depth_img = tf.image.decode_png(depth_img, channels=1)\n    depth_img = tf.cast(depth_img, tf.float32)\n    depth_img = (depth_img / 255.0) * 2 - 1  # Normalize to [-1, 1]\n    \n    return spad_img, depth_img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:17:01.135930Z","iopub.execute_input":"2025-04-26T19:17:01.136522Z","iopub.status.idle":"2025-04-26T19:17:01.141125Z","shell.execute_reply.started":"2025-04-26T19:17:01.136497Z","shell.execute_reply":"2025-04-26T19:17:01.140306Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def random_crop(spad_img, depth_img):\n    stacked_image = tf.stack([spad_img, depth_img], axis=0)\n    cropped_image = tf.image.random_crop(\n        stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 1])\n    \n    return cropped_image[0], cropped_image[1]\n\ndef normalize(spad_img, depth_img):\n    return spad_img, depth_img\n\n# def random_jitter(spad_img, depth_img):\n#     # Resize to bigger height and width\n#     spad_img, depth_img = resize(spad_img, depth_img, 286, 286)\n    \n#     # Random crop back to the target size\n#     spad_img, depth_img = random_crop(spad_img, depth_img)\n    \n#     if tf.random.uniform(()) > 0.5:\n#         # Random mirroring\n#         spad_img = tf.image.flip_left_right(spad_img)\n#         depth_img = tf.image.flip_left_right(depth_img)\n    \n#     return spad_img, depth_img\n\ndef resize(spad_img, depth_img, height, width):\n    spad_img = tf.image.resize(spad_img, [height, width],\n                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    depth_img = tf.image.resize(depth_img, [height, width],\n                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    \n    return spad_img, depth_img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:17:05.182638Z","iopub.execute_input":"2025-04-26T19:17:05.182943Z","iopub.status.idle":"2025-04-26T19:17:05.188572Z","shell.execute_reply.started":"2025-04-26T19:17:05.182923Z","shell.execute_reply":"2025-04-26T19:17:05.187816Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# Create training dataset\ndef create_dataset(spad_paths, depth_paths, batch_size=BATCH_SIZE, \n                   buffer_size=400, train=True):\n    dataset = tf.data.Dataset.from_tensor_slices((spad_paths, depth_paths))\n    \n    dataset = dataset.map(load_and_preprocess_data, \n                          num_parallel_calls=tf.data.AUTOTUNE)\n    \n    # if train:\n    #     dataset = dataset.map(random_jitter, \n    #                          num_parallel_calls=tf.data.AUTOTUNE)\n    \n    dataset = dataset.map(normalize, \n                         num_parallel_calls=tf.data.AUTOTUNE)\n    \n    dataset = dataset.shuffle(buffer_size)\n    dataset = dataset.batch(batch_size)\n    \n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:17:05.405342Z","iopub.execute_input":"2025-04-26T19:17:05.405607Z","iopub.status.idle":"2025-04-26T19:17:05.410122Z","shell.execute_reply.started":"2025-04-26T19:17:05.405585Z","shell.execute_reply":"2025-04-26T19:17:05.409424Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Training the model\n@tf.function\ndef train_step(generator, discriminator, generator_optimizer, discriminator_optimizer, \n               spad_images, depth_maps):\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        # Generate depth maps from SPAD images\n        generated_depths = generator(spad_images, training=True)\n        \n        # Discriminator predictions\n        disc_real_output = discriminator([spad_images, depth_maps], training=True)\n        disc_generated_output = discriminator([spad_images, generated_depths], training=True)\n        \n        # Calculate losses\n        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(\n            disc_generated_output, generated_depths, depth_maps)\n        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n    \n    # Calculate gradients\n    generator_gradients = gen_tape.gradient(\n        gen_total_loss, generator.trainable_variables)\n    discriminator_gradients = disc_tape.gradient(\n        disc_loss, discriminator.trainable_variables)\n    \n    # Apply gradients\n    generator_optimizer.apply_gradients(\n        zip(generator_gradients, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(\n        zip(discriminator_gradients, discriminator.trainable_variables))\n    \n    return gen_total_loss, gen_gan_loss, gen_l1_loss, disc_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:17:10.340439Z","iopub.execute_input":"2025-04-26T19:17:10.340964Z","iopub.status.idle":"2025-04-26T19:17:10.409269Z","shell.execute_reply.started":"2025-04-26T19:17:10.340939Z","shell.execute_reply":"2025-04-26T19:17:10.408446Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def train(generator, discriminator, generator_optimizer, discriminator_optimizer, \n#           train_dataset, checkpoint_filepath='/kaggle/working/outputs/checkpoints'):\ndef train(generator, discriminator, generator_optimizer, discriminator_optimizer,\n          train_dataset, val_dataset=None, checkpoint_filepath='/kaggle/working/outputs/checkpoints'):\n    for epoch in range(EPOCHS):\n        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n        start_time = time.time()\n\n        # Train step\n        for n, (spad_batch, depth_batch) in enumerate(train_dataset):\n            gen_total_loss, gen_gan_loss, gen_l1_loss, disc_loss = train_step(\n                generator, discriminator, generator_optimizer, discriminator_optimizer,\n                spad_batch, depth_batch)\n\n            if n % 10 == 0:\n                print(f\"  [Train] Batch {n:03d} - Gen Loss: {gen_total_loss:.4f}, \"\n                      f\"GAN: {gen_gan_loss:.4f}, L1: {gen_l1_loss:.4f}, Disc: {disc_loss:.4f}\")\n\n        # --- Validation step (optional) ---\n        if val_dataset is not None:\n            mae_metric = tf.keras.metrics.MeanAbsoluteError()\n            mse_metric = tf.keras.metrics.MeanSquaredError()\n            ssim_scores = []\n\n            for spad_val_batch, depth_val_batch in val_dataset:\n                pred_val_batch = generator(spad_val_batch, training=False)\n                mae_metric.update_state(depth_val_batch, pred_val_batch)\n                mse_metric.update_state(depth_val_batch, pred_val_batch)\n                ssim_score = tf.reduce_mean(tf.image.ssim(depth_val_batch, pred_val_batch, max_val=2.0))\n                ssim_scores.append(ssim_score.numpy())\n\n            val_mae = mae_metric.result().numpy()\n            val_rmse = np.sqrt(mse_metric.result().numpy())\n            val_ssim = np.mean(ssim_scores)\n\n            print(f\"  [Val] MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}, SSIM: {val_ssim:.4f}\")\n\n        # Save checkpoint every 10 epochs or at the last one\n        if (epoch + 1) % 10 == 0 or epoch == EPOCHS - 1:\n            checkpoint_prefix = os.path.join(checkpoint_filepath, f\"ckpt_epoch_{epoch+1}\")\n            checkpoint = tf.train.Checkpoint(\n                generator_optimizer=generator_optimizer,\n                discriminator_optimizer=discriminator_optimizer,\n                generator=generator,\n                discriminator=discriminator)\n            checkpoint.save(file_prefix=checkpoint_prefix)\n            print(f\"Checkpoint saved: {checkpoint_prefix}\")\n\n            # Generate and save visualization\n            for idx, (spad, depth) in enumerate(train_dataset.take(1)):\n                generate_and_save_images(generator, epoch + 1, spad, depth)\n                break\n\n        print(f\"Epoch {epoch+1} completed in {time.time() - start_time:.2f} seconds.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:17:15.213328Z","iopub.execute_input":"2025-04-26T19:17:15.213616Z","iopub.status.idle":"2025-04-26T19:17:15.222278Z","shell.execute_reply.started":"2025-04-26T19:17:15.213593Z","shell.execute_reply":"2025-04-26T19:17:15.221565Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def generate_and_save_images(model, epoch, spad_batch, depth_batch):\n    # Generate images\n    predictions = model(spad_batch, training=False)\n    \n    fig = plt.figure(figsize=(15, 10))\n    \n    display_list = [spad_batch[0], depth_batch[0], predictions[0]]\n    title = ['SPAD Input', 'Ground Truth Depth', 'Predicted Depth']\n    \n    for i in range(3):\n        plt.subplot(1, 3, i+1)\n        plt.title(title[i])\n        # Getting the pixel values in the [0, 1] range to plot.\n        plt.imshow(display_list[i] * 0.5 + 0.5, cmap='viridis')\n        plt.axis('off')\n    \n    plt.savefig(f'predictions_epoch_{epoch}.png')\n    plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:17:15.745651Z","iopub.execute_input":"2025-04-26T19:17:15.745948Z","iopub.status.idle":"2025-04-26T19:17:15.751313Z","shell.execute_reply.started":"2025-04-26T19:17:15.745925Z","shell.execute_reply":"2025-04-26T19:17:15.750595Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Inference function for testing on new SPAD images\ndef predict_depth_map(generator, spad_image_path):\n    # Load and preprocess SPAD image\n    spad_img = tf.io.read_file(spad_image_path)\n    spad_img = tf.image.decode_png(spad_img, channels=1)\n    spad_img = tf.image.resize(spad_img, [IMG_HEIGHT, IMG_WIDTH])\n    spad_img = tf.cast(spad_img, tf.float32)\n    spad_img = (spad_img / 255.0) * 2 - 1  # Normalize to [-1, 1]\n    spad_img = tf.expand_dims(spad_img, 0)  # Add batch dimension\n    \n    # Generate depth map\n    predicted_depth = generator(spad_img, training=False)\n    \n    # Denormalize\n    predicted_depth = (predicted_depth[0] + 1) / 2\n    \n    return predicted_depth.numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:17:33.062439Z","iopub.execute_input":"2025-04-26T19:17:33.062950Z","iopub.status.idle":"2025-04-26T19:17:33.067959Z","shell.execute_reply.started":"2025-04-26T19:17:33.062926Z","shell.execute_reply":"2025-04-26T19:17:33.067198Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# Class for img2csv functionality if the module is not available\nclass Img2CSV:\n    @staticmethod\n    def convert_to_csv(image_dir, output_csv):\n        \"\"\"\n        Convert depth map images to a submission CSV file.\n        \n        Args:\n            image_dir (str): Directory containing depth map PNG images\n            output_csv (str): Output CSV file path\n        \"\"\"\n        print(f\"Converting depth maps from {image_dir} to CSV...\")\n        all_data = []\n        \n        # Get all PNG files in the directory\n        image_files = [f for f in os.listdir(image_dir) if f.endswith(\"_depth.png\")]\n        \n        for image_file in tqdm(image_files, desc=\"Processing images\"):\n            # Extract the image ID (remove _depth.png)\n            image_id = image_file.replace(\"_depth.png\", \"\")\n            \n            # Load the depth map\n            depth_map_path = os.path.join(image_dir, image_file)\n            depth_map = plt.imread(depth_map_path)\n            \n            # Handle RGB images by converting to grayscale if necessary\n            if len(depth_map.shape) > 2:\n                depth_map = np.mean(depth_map, axis=2)\n            \n            # Flatten the depth map and create rows\n            flat_depth = depth_map.flatten()\n            \n            for pixel_id, depth_value in enumerate(flat_depth):\n                all_data.append({\n                    'image_id': image_id,\n                    'pixel_id': pixel_id,\n                    'depth': depth_value  # Already in range [0, 1] if read with plt.imread\n                })\n        \n        # Create and save dataframe\n        df = pd.DataFrame(all_data)\n        df.to_csv(output_csv, index=False)\n        print(f\"CSV file created: {output_csv}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:17:33.331424Z","iopub.execute_input":"2025-04-26T19:17:33.331704Z","iopub.status.idle":"2025-04-26T19:17:33.338215Z","shell.execute_reply.started":"2025-04-26T19:17:33.331681Z","shell.execute_reply":"2025-04-26T19:17:33.337521Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# If img2csv.py is not available, use the built-in implementation\nif 'img2csv' not in globals():\n    img2csv = Img2CSV","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:17:36.774550Z","iopub.execute_input":"2025-04-26T19:17:36.775290Z","iopub.status.idle":"2025-04-26T19:17:36.778666Z","shell.execute_reply.started":"2025-04-26T19:17:36.775263Z","shell.execute_reply":"2025-04-26T19:17:36.777900Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# Function to evaluate the model using various metrics\ndef evaluate_model(generator, val_dataset):\n    \"\"\"\n    Evaluate the model using various metrics: MAE, RMSE, SSIM\n    \n    Args:\n        generator: Trained generator model\n        val_dataset: Validation dataset\n    \n    Returns:\n        Dictionary of metrics\n    \"\"\"\n    mae_metric = tf.keras.metrics.MeanAbsoluteError()\n    mse_metric = tf.keras.metrics.MeanSquaredError()\n    \n    # SSIM implementation\n    def ssim(img1, img2):\n        return tf.reduce_mean(tf.image.ssim(img1, img2, max_val=2.0))\n    \n    ssim_scores = []\n    \n    print(\"Evaluating model...\")\n    for i, (spad_batch, depth_batch) in enumerate(tqdm(val_dataset)):\n        # Generate predictions\n        pred_batch = generator(spad_batch, training=False)\n        \n        # Update metrics\n        mae_metric.update_state(depth_batch, pred_batch)\n        mse_metric.update_state(depth_batch, pred_batch)\n        \n        # Calculate and collect SSIM scores\n        ssim_value = ssim(depth_batch, pred_batch)\n        ssim_scores.append(ssim_value.numpy())\n    \n    # Calculate final metrics\n    mae = mae_metric.result().numpy()\n    rmse = np.sqrt(mse_metric.result().numpy())\n    mean_ssim = np.mean(ssim_scores)\n    \n    metrics = {\n        'MAE': mae,\n        'RMSE': rmse,\n        'SSIM': mean_ssim\n    }\n    \n    print(\"Evaluation metrics:\")\n    for metric_name, metric_value in metrics.items():\n        print(f\"  {metric_name}: {metric_value:.4f}\")\n    \n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:17:40.515780Z","iopub.execute_input":"2025-04-26T19:17:40.516494Z","iopub.status.idle":"2025-04-26T19:17:40.522604Z","shell.execute_reply.started":"2025-04-26T19:17:40.516471Z","shell.execute_reply":"2025-04-26T19:17:40.521808Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# Function to load a trained model from checkpoint\ndef load_model_from_checkpoint(checkpoint_path):\n    # Initialize models\n    generator = build_generator()\n    discriminator = build_discriminator()\n    \n    # Initialize optimizers\n    generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    \n    # Create checkpoint\n    checkpoint = tf.train.Checkpoint(\n        generator_optimizer=generator_optimizer,   \n        discriminator_optimizer=discriminator_optimizer,\n        generator=generator,\n        discriminator=discriminator)\n    \n    # Restore checkpoint\n    checkpoint.restore(checkpoint_path).expect_partial()\n    print(f\"Model loaded from checkpoint: {checkpoint_path}\")\n    \n    return generator, discriminator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:17:45.038942Z","iopub.execute_input":"2025-04-26T19:17:45.039204Z","iopub.status.idle":"2025-04-26T19:17:45.044446Z","shell.execute_reply.started":"2025-04-26T19:17:45.039184Z","shell.execute_reply":"2025-04-26T19:17:45.043587Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# # Main function to run the entire pipeline\n# def main():\n#     # Initialize models\n#     generator = build_generator()\n#     discriminator = build_discriminator()\n    \n#     # Optimizers\n#     generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n#     discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    \n#     # Dataset paths (replace with actual paths)\n#     SPAD_TRAIN_PATH = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/training-images'\n#     DEPTH_TRAIN_PATH = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/training-depths'\n#     # Add validation dataset paths\n#     SPAD_VAL_PATH = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/validation-images'\n#     DEPTH_VAL_PATH = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/validation-depths'\n#     SPAD_TEST_PATH = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/testing-images'\n#     OUTPUT_DIR = '/kaggle/working/outputs'\n#     CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, 'checkpoints')\n#     TEST_OUTPUT_DIR = os.path.join(OUTPUT_DIR, 'test_predictions')\n    \n#     # Create output directories\n#     os.makedirs(OUTPUT_DIR, exist_ok=True)\n#     os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n#     os.makedirs(TEST_OUTPUT_DIR, exist_ok=True)\n    \n#     # Get lists of image paths\n#     spad_train_images = sorted([os.path.join(SPAD_TRAIN_PATH, filename) \n#                          for filename in os.listdir(SPAD_TRAIN_PATH) \n#                          if filename.endswith('.png')])\n#     depth_train_maps = sorted([os.path.join(DEPTH_TRAIN_PATH, filename) \n#                         for filename in os.listdir(DEPTH_TRAIN_PATH) \n#                         if filename.endswith('.png')])\n    \n#     # Add validation image paths\n#     print(\"Loading validation data...\")\n#     try:\n#         spad_val_images = sorted([os.path.join(SPAD_VAL_PATH, filename) \n#                            for filename in os.listdir(SPAD_VAL_PATH) \n#                            if filename.endswith('.png')])\n#         depth_val_maps = sorted([os.path.join(DEPTH_VAL_PATH, filename) \n#                           for filename in os.listdir(DEPTH_VAL_PATH) \n#                           if filename.endswith('.png')])\n        \n#         # Debug: Check if validation directories exist and contain images\n#         print(f\"Found {len(spad_val_images)} validation SPAD images and {len(depth_val_maps)} validation depth maps\")\n        \n#         # Debug: Check if validation pairs match\n#         if len(spad_val_images) != len(depth_val_maps):\n#             print(f\"WARNING: Validation dataset has mismatched counts - {len(spad_val_images)} SPAD images vs {len(depth_val_maps)} depth maps\")\n            \n#         # Debug: Print first few validation image paths\n#         if spad_val_images:\n#             print(f\"First validation SPAD image: {os.path.basename(spad_val_images[0])}\")\n#         if depth_val_maps:\n#             print(f\"First validation depth map: {os.path.basename(depth_val_maps[0])}\")\n            \n#     except FileNotFoundError as e:\n#         print(f\"ERROR: Validation directory not found: {e}\")\n#         print(\"Falling back to splitting training data...\")\n#         # Fallback to splitting if validation directories don't exist\n#         train_spad_paths, train_depth_paths, spad_val_images, depth_val_maps = split_dataset(\n#             spad_train_images, depth_train_maps, val_split=0.2)\n#         print(f\"Fallback: {len(spad_val_images)} validation images created from training split\")\n    \n#     print(f\"Training on {len(spad_train_images)} images, validating on {len(spad_val_images)} images\")\n    \n#     # Create datasets\n#     train_dataset = create_dataset(spad_train_images, depth_train_maps, batch_size=BATCH_SIZE)\n#     val_dataset = create_dataset(spad_val_images, depth_val_maps, batch_size=BATCH_SIZE, train=False)\n    \n#     # Debug: Check first batch from each dataset\n#     for x_batch, y_batch in train_dataset.take(1):\n#         print(f\"Training batch shape: SPAD={x_batch.shape}, depth={y_batch.shape}\")\n        \n#     for x_batch, y_batch in val_dataset.take(1):\n#         print(f\"Validation batch shape: SPAD={x_batch.shape}, depth={y_batch.shape}\")\n    \n#     # Check if we should load from checkpoint or train from scratch\n#     load_checkpoint = input(\"Load from checkpoint? (y/n): \").lower() == 'y'\n    \n#     if load_checkpoint:\n#         checkpoint_path = input(\"Enter checkpoint path: \")\n#         generator, discriminator = load_model_from_checkpoint(checkpoint_path)\n#     else:\n#         # Train the model\n#         print(\"Starting model training...\")\n#         history = train(generator, discriminator, generator_optimizer, discriminator_optimizer, \n#                         train_dataset, val_dataset, checkpoint_filepath=CHECKPOINT_DIR)\n#         print(\"Training completed!\")\n        \n#         # Evaluate the model\n#         print(\"Evaluating model on validation set...\")\n#         metrics = evaluate_model(generator, val_dataset)\n        \n#         # Save evaluation metrics\n#         with open(os.path.join(OUTPUT_DIR, 'evaluation_metrics.txt'), 'w') as f:\n#             for metric_name, metric_value in metrics.items():\n#                 f.write(f\"{metric_name}: {metric_value:.4f}\\n\")\n    \n#     # Get test images\n#     test_spad_paths = sorted([os.path.join(SPAD_TEST_PATH, filename) \n#                              for filename in os.listdir(SPAD_TEST_PATH) \n#                              if filename.endswith('.png')])\n    \n#     print(f\"Running inference on {len(test_spad_paths)} test images...\")\n    \n#     # Create test dataset\n#     test_dataset = create_test_dataset(test_spad_paths, batch_size=BATCH_SIZE)\n    \n#     # Generate depth maps for test data\n#     predictions_dict = test(generator, test_dataset, test_spad_paths, output_dir=TEST_OUTPUT_DIR)\n    \n#     # Convert predictions to submission CSV\n#     submission_file = convert_to_submission(predictions_dict, \n#                                            output_dir=TEST_OUTPUT_DIR, \n#                                            output_file=os.path.join(OUTPUT_DIR, \"submission.csv\"))\n    \n#     print(f\"Submission file created: {submission_file}\")\n    \n#     # Visualize a few test predictions\n#     print(\"Creating visualization of test predictions...\")\n#     visualize_test_predictions(generator, test_dataset, test_spad_paths, \n#                               num_examples=5, output_dir=TEST_OUTPUT_DIR)\n    \n#     print(\"Pipeline completed successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_test_dataset(spad_paths, batch_size=BATCH_SIZE):\n    dataset = tf.data.Dataset.from_tensor_slices(spad_paths)\n    dataset = dataset.map(load_and_preprocess_test_data, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(batch_size)\n    return dataset\n\ndef load_and_preprocess_test_data(spad_path):\n    # Load SPAD binary image (0 or 1 values)\n    spad_img = tf.io.read_file(spad_path)\n    spad_img = tf.image.decode_png(spad_img, channels=1)\n    # spad_img = tf.image.resize(spad_img, [IMG_HEIGHT, IMG_WIDTH])  # Resize if necessary\n    spad_img = tf.cast(spad_img, tf.float32)\n    spad_img = (spad_img / 255.0) * 2 - 1  # Normalize to [-1, 1]\n    return spad_img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:17:50.988577Z","iopub.execute_input":"2025-04-26T19:17:50.988879Z","iopub.status.idle":"2025-04-26T19:17:50.994091Z","shell.execute_reply.started":"2025-04-26T19:17:50.988833Z","shell.execute_reply":"2025-04-26T19:17:50.993292Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# Function to visualize test predictions\ndef visualize_test_predictions(generator, test_dataset, test_filenames, num_examples=5, output_dir=\".\"):\n    \"\"\"\n    Visualize test predictions for a few examples\n    \n    Args:\n        generator: Trained generator model\n        test_dataset: Test dataset\n        test_filenames: List of test filenames\n        num_examples: Number of examples to visualize\n        output_dir: Output directory for visualizations\n    \"\"\"\n    plt.figure(figsize=(12, 8))\n    \n    for i, spad_batch in enumerate(test_dataset):\n        if i >= num_examples:\n            break\n            \n        # Generate predictions\n        predicted_depths = generator(spad_batch, training=False)\n        \n        # Denormalize\n        spad_images = (spad_batch + 1) / 2\n        predicted_depths = (predicted_depths + 1) / 2\n        \n        # Display a few examples\n        for j in range(min(3, spad_batch.shape[0])):\n            # Get the filename\n            idx = i * BATCH_SIZE + j\n            if idx >= len(test_filenames):\n                break\n                \n            filename = os.path.basename(test_filenames[idx])\n            \n            plt.subplot(min(3, spad_batch.shape[0]), 2, j*2+1)\n            plt.title(f\"SPAD Input: {filename}\")\n            plt.imshow(spad_images[j], cmap='gray')\n            plt.axis('off')\n            \n            plt.subplot(min(3, spad_batch.shape[0]), 2, j*2+2)\n            plt.title(f\"Predicted Depth\")\n            plt.imshow(predicted_depths[j], cmap='viridis')\n            plt.axis('off')\n        \n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, f\"test_predictions_batch_{i}.png\"))\n        plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:17:53.881166Z","iopub.execute_input":"2025-04-26T19:17:53.881914Z","iopub.status.idle":"2025-04-26T19:17:53.889104Z","shell.execute_reply.started":"2025-04-26T19:17:53.881891Z","shell.execute_reply":"2025-04-26T19:17:53.888023Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# /kaggle/working/checkpoints","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T13:30:09.076334Z","iopub.status.idle":"2025-04-26T13:30:09.076553Z","shell.execute_reply.started":"2025-04-26T13:30:09.076437Z","shell.execute_reply":"2025-04-26T13:30:09.076446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generator = build_generator()\ndiscriminator = build_discriminator()\n\n# Optimizers\ngenerator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n# Dataset paths (replace with actual paths)\nSPAD_TRAIN_PATH = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/training-images'\nDEPTH_TRAIN_PATH = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/training-depths'\n# Add validation dataset paths\nSPAD_VAL_PATH = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/validation-images'\nDEPTH_VAL_PATH = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/validation-depths'\nSPAD_TEST_PATH = '/kaggle/input/ee-5179-modern-computer-vision-course-competition/competition-data/testing-images'\nOUTPUT_DIR = '/kaggle/working/outputs'\nCHECKPOINT_DIR = os.path.join(OUTPUT_DIR, 'checkpoints')\nTEST_OUTPUT_DIR = os.path.join(OUTPUT_DIR, 'test_predictions')\n\n# Create output directories\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nos.makedirs(TEST_OUTPUT_DIR, exist_ok=True)\n\n# Get lists of image paths\nspad_train_images = sorted([os.path.join(SPAD_TRAIN_PATH, filename) \n                     for filename in os.listdir(SPAD_TRAIN_PATH) \n                     if filename.endswith('.png')])\ndepth_train_maps = sorted([os.path.join(DEPTH_TRAIN_PATH, filename) \n                    for filename in os.listdir(DEPTH_TRAIN_PATH) \n                    if filename.endswith('.png')])\n\n# Add validation image paths\nprint(\"Loading validation data...\")\ntry:\n    spad_val_images = sorted([os.path.join(SPAD_VAL_PATH, filename) \n                       for filename in os.listdir(SPAD_VAL_PATH) \n                       if filename.endswith('.png')])\n    depth_val_maps = sorted([os.path.join(DEPTH_VAL_PATH, filename) \n                      for filename in os.listdir(DEPTH_VAL_PATH) \n                      if filename.endswith('.png')])\n    \n    # Debug: Check if validation directories exist and contain images\n    print(f\"Found {len(spad_val_images)} validation SPAD images and {len(depth_val_maps)} validation depth maps\")\n    \n    # Debug: Check if validation pairs match\n    if len(spad_val_images) != len(depth_val_maps):\n        print(f\"WARNING: Validation dataset has mismatched counts - {len(spad_val_images)} SPAD images vs {len(depth_val_maps)} depth maps\")\n        \n    # Debug: Print first few validation image paths\n    if spad_val_images:\n        print(f\"First validation SPAD image: {os.path.basename(spad_val_images[0])}\")\n    if depth_val_maps:\n        print(f\"First validation depth map: {os.path.basename(depth_val_maps[0])}\")\n        \nexcept FileNotFoundError as e:\n    print(f\"ERROR: Validation directory not found: {e}\")\n    print(\"Falling back to splitting training data...\")\n    # Fallback to splitting if validation directories don't exist\n    train_spad_paths, train_depth_paths, spad_val_images, depth_val_maps = split_dataset(\n        spad_train_images, depth_train_maps, val_split=0.2)\n    print(f\"Fallback: {len(spad_val_images)} validation images created from training split\")\n\nprint(f\"Training on {len(spad_train_images)} images, validating on {len(spad_val_images)} images\")\n\n# Create datasets\ntrain_dataset = create_dataset(spad_train_images, depth_train_maps, batch_size=BATCH_SIZE)\nval_dataset = create_dataset(spad_val_images, depth_val_maps, batch_size=BATCH_SIZE, train=False)\n\n# Debug: Check first batch from each dataset\nfor x_batch, y_batch in train_dataset.take(1):\n    print(f\"Training batch shape: SPAD={x_batch.shape}, depth={y_batch.shape}\")\n    \nfor x_batch, y_batch in val_dataset.take(1):\n    print(f\"Validation batch shape: SPAD={x_batch.shape}, depth={y_batch.shape}\")\n\n# Check if we should load from checkpoint or train from scratch\nload_checkpoint = input(\"Load from checkpoint? (y/n): \").lower() == 'y'\n\nif load_checkpoint:\n    checkpoint_path = input(\"Enter checkpoint path: \")\n    generator, discriminator = load_model_from_checkpoint(checkpoint_path)\nelse:\n    # Train the model\n    print(\"Starting model training...\")\n    history = train(generator, discriminator, generator_optimizer, discriminator_optimizer, \n                    train_dataset, val_dataset, checkpoint_filepath=CHECKPOINT_DIR)\n    print(\"Training completed!\")\n    \n    # Evaluate the model\n    print(\"Evaluating model on validation set...\")\n    metrics = evaluate_model(generator, val_dataset)\n    \n    # Save evaluation metrics\n    with open(os.path.join(OUTPUT_DIR, 'evaluation_metrics.txt'), 'w') as f:\n        for metric_name, metric_value in metrics.items():\n            f.write(f\"{metric_name}: {metric_value:.4f}\\n\")\n\n# Get test images\ntest_spad_paths = sorted([os.path.join(SPAD_TEST_PATH, filename) \n                         for filename in os.listdir(SPAD_TEST_PATH) \n                         if filename.endswith('.png')])\n\nprint(f\"Running inference on {len(test_spad_paths)} test images...\")\n\n# Create test dataset\ntest_dataset = create_test_dataset(test_spad_paths, batch_size=BATCH_SIZE)\n\n# Generate depth maps for test data\npredictions_dict = test(generator, test_dataset, test_spad_paths, output_dir=TEST_OUTPUT_DIR)\n\n# Convert predictions to submission CSV\nsubmission_file = convert_to_submission(predictions_dict, \n                                       output_dir=TEST_OUTPUT_DIR, \n                                       output_file=os.path.join(OUTPUT_DIR, \"submission.csv\"))\n\nprint(f\"Submission file created: {submission_file}\")\n\n# Visualize a few test predictions\nprint(\"Creating visualization of test predictions...\")\nvisualize_test_predictions(generator, test_dataset, test_spad_paths, \n                          num_examples=5, output_dir=TEST_OUTPUT_DIR)\n\nprint(\"Pipeline completed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T13:46:55.229760Z","iopub.execute_input":"2025-04-26T13:46:55.230602Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1745675216.227618      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Loading validation data...\nFound 836 validation SPAD images and 836 validation depth maps\nFirst validation SPAD image: 01.png\nFirst validation depth map: 01.png\nTraining on 6686 images, validating on 836 images\nTraining batch shape: SPAD=(16, 256, 256, 1), depth=(16, 256, 256, 1)\nValidation batch shape: SPAD=(16, 256, 256, 1), depth=(16, 256, 256, 1)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Load from checkpoint? (y/n):  n\n"},{"name":"stdout","text":"Starting model training...\n\nEpoch 1/150\n","output_type":"stream"},{"name":"stderr","text":"E0000 00:00:1745675239.494332      31 meta_optimizer.cc:966] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape infunctional_15_1/sequential_8_1/dropout_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\nI0000 00:00:1745675240.296536      91 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"  [Train] Batch 000 - Gen Loss: 65.6779, GAN: 1.2207, L1: 0.6446, Disc: 1.7939\n  [Train] Batch 010 - Gen Loss: 44.3914, GAN: 0.7801, L1: 0.4361, Disc: 1.5270\n  [Train] Batch 020 - Gen Loss: 34.1556, GAN: 0.7639, L1: 0.3339, Disc: 1.4475\n  [Train] Batch 030 - Gen Loss: 29.0326, GAN: 0.7514, L1: 0.2828, Disc: 1.4548\n  [Train] Batch 040 - Gen Loss: 28.4096, GAN: 0.7344, L1: 0.2768, Disc: 1.4314\n  [Train] Batch 050 - Gen Loss: 39.5918, GAN: 0.7720, L1: 0.3882, Disc: 1.4422\n  [Train] Batch 060 - Gen Loss: 35.7187, GAN: 0.7346, L1: 0.3498, Disc: 1.4057\n  [Train] Batch 070 - Gen Loss: 34.8159, GAN: 0.7608, L1: 0.3406, Disc: 1.3895\n  [Train] Batch 080 - Gen Loss: 25.8805, GAN: 0.8092, L1: 0.2507, Disc: 1.4874\n  [Train] Batch 090 - Gen Loss: 22.8128, GAN: 0.7390, L1: 0.2207, Disc: 1.3747\n  [Train] Batch 100 - Gen Loss: 24.5762, GAN: 0.7553, L1: 0.2382, Disc: 1.4257\n  [Train] Batch 110 - Gen Loss: 23.7138, GAN: 1.0026, L1: 0.2271, Disc: 1.8809\n  [Train] Batch 120 - Gen Loss: 18.9818, GAN: 0.7295, L1: 0.1825, Disc: 1.3925\n  [Train] Batch 130 - Gen Loss: 25.4652, GAN: 0.7252, L1: 0.2474, Disc: 1.3620\n  [Train] Batch 140 - Gen Loss: 29.0208, GAN: 0.6904, L1: 0.2833, Disc: 1.3985\n  [Train] Batch 150 - Gen Loss: 29.2566, GAN: 0.7311, L1: 0.2853, Disc: 1.3333\n  [Train] Batch 160 - Gen Loss: 28.1644, GAN: 0.7518, L1: 0.2741, Disc: 1.3178\n  [Train] Batch 170 - Gen Loss: 29.8632, GAN: 0.7480, L1: 0.2912, Disc: 1.2979\n  [Train] Batch 180 - Gen Loss: 32.0450, GAN: 0.7609, L1: 0.3128, Disc: 1.3093\n  [Train] Batch 190 - Gen Loss: 24.8634, GAN: 0.8115, L1: 0.2405, Disc: 1.3675\n  [Train] Batch 200 - Gen Loss: 28.9778, GAN: 0.8556, L1: 0.2812, Disc: 1.2003\n  [Train] Batch 210 - Gen Loss: 26.6667, GAN: 0.7331, L1: 0.2593, Disc: 1.3123\n  [Train] Batch 220 - Gen Loss: 26.1447, GAN: 0.7498, L1: 0.2539, Disc: 1.3221\n  [Train] Batch 230 - Gen Loss: 31.3156, GAN: 1.0218, L1: 0.3029, Disc: 1.3589\n  [Train] Batch 240 - Gen Loss: 28.6519, GAN: 0.8599, L1: 0.2779, Disc: 1.2214\n  [Train] Batch 250 - Gen Loss: 22.1177, GAN: 0.8531, L1: 0.2126, Disc: 1.2056\n  [Train] Batch 260 - Gen Loss: 28.6862, GAN: 1.0518, L1: 0.2763, Disc: 1.3387\n  [Train] Batch 270 - Gen Loss: 25.8956, GAN: 0.9704, L1: 0.2493, Disc: 1.3698\n  [Train] Batch 280 - Gen Loss: 27.3608, GAN: 0.8979, L1: 0.2646, Disc: 1.2547\n  [Train] Batch 290 - Gen Loss: 30.4852, GAN: 1.1625, L1: 0.2932, Disc: 1.3272\n  [Train] Batch 300 - Gen Loss: 32.5730, GAN: 0.8648, L1: 0.3171, Disc: 1.0357\n  [Train] Batch 310 - Gen Loss: 29.0168, GAN: 0.9429, L1: 0.2807, Disc: 1.0785\n  [Train] Batch 320 - Gen Loss: 18.0096, GAN: 0.9268, L1: 0.1708, Disc: 1.2708\n  [Train] Batch 330 - Gen Loss: 25.5892, GAN: 0.9071, L1: 0.2468, Disc: 1.1818\n  [Train] Batch 340 - Gen Loss: 25.4272, GAN: 0.8107, L1: 0.2462, Disc: 1.2355\n  [Train] Batch 350 - Gen Loss: 20.7751, GAN: 0.8470, L1: 0.1993, Disc: 1.2966\n  [Train] Batch 360 - Gen Loss: 27.3576, GAN: 0.6515, L1: 0.2671, Disc: 1.4184\n  [Train] Batch 370 - Gen Loss: 25.1429, GAN: 0.8375, L1: 0.2431, Disc: 1.2978\n  [Train] Batch 380 - Gen Loss: 19.6406, GAN: 1.0942, L1: 0.1855, Disc: 1.2180\n  [Train] Batch 390 - Gen Loss: 19.7072, GAN: 0.9093, L1: 0.1880, Disc: 1.2987\n  [Train] Batch 400 - Gen Loss: 21.4350, GAN: 0.8824, L1: 0.2055, Disc: 1.2843\n  [Train] Batch 410 - Gen Loss: 20.3297, GAN: 0.7117, L1: 0.1962, Disc: 1.2007\n","output_type":"stream"},{"name":"stderr","text":"E0000 00:00:1745675374.789620      31 meta_optimizer.cc:966] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape infunctional_15_1/sequential_8_1/dropout_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"  [Val] MAE: 0.3115, RMSE: 0.4675, SSIM: 0.5084\nEpoch 1 completed in 157.42 seconds.\n\nEpoch 2/150\n  [Train] Batch 000 - Gen Loss: 31.5225, GAN: 0.7894, L1: 0.3073, Disc: 1.2872\n  [Train] Batch 010 - Gen Loss: 33.8213, GAN: 0.6862, L1: 0.3314, Disc: 1.3115\n  [Train] Batch 020 - Gen Loss: 34.5347, GAN: 0.8359, L1: 0.3370, Disc: 1.1415\n  [Train] Batch 030 - Gen Loss: 23.2901, GAN: 1.1002, L1: 0.2219, Disc: 1.3779\n  [Train] Batch 040 - Gen Loss: 30.1086, GAN: 1.2075, L1: 0.2890, Disc: 1.0152\n  [Train] Batch 050 - Gen Loss: 26.3251, GAN: 1.1338, L1: 0.2519, Disc: 1.0772\n  [Train] Batch 060 - Gen Loss: 23.7117, GAN: 0.8306, L1: 0.2288, Disc: 1.0442\n  [Train] Batch 070 - Gen Loss: 26.7544, GAN: 1.0439, L1: 0.2571, Disc: 1.1261\n  [Train] Batch 080 - Gen Loss: 26.0309, GAN: 1.2800, L1: 0.2475, Disc: 1.2043\n  [Train] Batch 090 - Gen Loss: 17.1338, GAN: 0.9609, L1: 0.1617, Disc: 1.1957\n  [Train] Batch 100 - Gen Loss: 17.9277, GAN: 1.1296, L1: 0.1680, Disc: 1.1313\n  [Train] Batch 110 - Gen Loss: 26.1784, GAN: 0.7890, L1: 0.2539, Disc: 1.2438\n  [Train] Batch 120 - Gen Loss: 24.5581, GAN: 1.0889, L1: 0.2347, Disc: 1.3485\n  [Train] Batch 130 - Gen Loss: 20.7220, GAN: 0.9160, L1: 0.1981, Disc: 0.9906\n  [Train] Batch 140 - Gen Loss: 21.9375, GAN: 1.4330, L1: 0.2050, Disc: 0.7936\n  [Train] Batch 150 - Gen Loss: 27.3609, GAN: 0.9607, L1: 0.2640, Disc: 0.9673\n  [Train] Batch 160 - Gen Loss: 29.6841, GAN: 1.2369, L1: 0.2845, Disc: 1.0821\n  [Train] Batch 170 - Gen Loss: 21.9460, GAN: 1.3268, L1: 0.2062, Disc: 0.9885\n  [Train] Batch 180 - Gen Loss: 24.5547, GAN: 1.0039, L1: 0.2355, Disc: 1.3392\n  [Train] Batch 190 - Gen Loss: 23.8455, GAN: 0.8824, L1: 0.2296, Disc: 0.9142\n  [Train] Batch 200 - Gen Loss: 24.0239, GAN: 1.2726, L1: 0.2275, Disc: 0.7840\n  [Train] Batch 210 - Gen Loss: 22.6519, GAN: 1.4841, L1: 0.2117, Disc: 1.1237\n  [Train] Batch 220 - Gen Loss: 29.7075, GAN: 1.0653, L1: 0.2864, Disc: 0.7920\n  [Train] Batch 230 - Gen Loss: 28.0197, GAN: 0.9027, L1: 0.2712, Disc: 0.9180\n  [Train] Batch 240 - Gen Loss: 23.7938, GAN: 1.2540, L1: 0.2254, Disc: 1.0063\n  [Train] Batch 250 - Gen Loss: 24.1916, GAN: 1.1642, L1: 0.2303, Disc: 0.8009\n  [Train] Batch 260 - Gen Loss: 28.7505, GAN: 1.2395, L1: 0.2751, Disc: 0.7836\n  [Train] Batch 270 - Gen Loss: 28.6628, GAN: 1.3043, L1: 0.2736, Disc: 0.7513\n  [Train] Batch 280 - Gen Loss: 22.6437, GAN: 1.2294, L1: 0.2141, Disc: 1.1344\n  [Train] Batch 290 - Gen Loss: 23.3206, GAN: 1.5454, L1: 0.2178, Disc: 0.6700\n  [Train] Batch 300 - Gen Loss: 29.3555, GAN: 1.4054, L1: 0.2795, Disc: 0.8748\n  [Train] Batch 310 - Gen Loss: 23.8266, GAN: 1.7687, L1: 0.2206, Disc: 1.4172\n  [Train] Batch 320 - Gen Loss: 23.6809, GAN: 1.3655, L1: 0.2232, Disc: 2.4940\n  [Train] Batch 330 - Gen Loss: 30.1538, GAN: 1.2203, L1: 0.2893, Disc: 0.9118\n  [Train] Batch 340 - Gen Loss: 31.0986, GAN: 1.3015, L1: 0.2980, Disc: 0.7491\n  [Train] Batch 350 - Gen Loss: 22.2591, GAN: 2.0017, L1: 0.2026, Disc: 1.1471\n  [Train] Batch 360 - Gen Loss: 23.5178, GAN: 0.8553, L1: 0.2266, Disc: 0.9654\n  [Train] Batch 370 - Gen Loss: 23.6219, GAN: 1.4541, L1: 0.2217, Disc: 1.3509\n  [Train] Batch 380 - Gen Loss: 22.5635, GAN: 1.3716, L1: 0.2119, Disc: 0.8590\n  [Train] Batch 390 - Gen Loss: 20.2740, GAN: 1.6289, L1: 0.1865, Disc: 1.3997\n  [Train] Batch 400 - Gen Loss: 16.7444, GAN: 0.8711, L1: 0.1587, Disc: 1.0990\n  [Train] Batch 410 - Gen Loss: 16.8541, GAN: 0.6424, L1: 0.1621, Disc: 1.3243\n  [Val] MAE: 0.2944, RMSE: 0.4560, SSIM: 0.5061\nEpoch 2 completed in 131.05 seconds.\n\nEpoch 3/150\n  [Train] Batch 000 - Gen Loss: 32.0368, GAN: 1.1684, L1: 0.3087, Disc: 0.6855\n  [Train] Batch 010 - Gen Loss: 30.2747, GAN: 0.7420, L1: 0.2953, Disc: 1.1851\n  [Train] Batch 020 - Gen Loss: 30.0222, GAN: 0.7155, L1: 0.2931, Disc: 1.1612\n  [Train] Batch 030 - Gen Loss: 27.5289, GAN: 0.9784, L1: 0.2655, Disc: 1.3138\n  [Train] Batch 040 - Gen Loss: 29.9997, GAN: 1.5419, L1: 0.2846, Disc: 0.9773\n  [Train] Batch 050 - Gen Loss: 29.5325, GAN: 1.1161, L1: 0.2842, Disc: 0.8365\n  [Train] Batch 060 - Gen Loss: 25.7132, GAN: 0.7291, L1: 0.2498, Disc: 1.2095\n  [Train] Batch 070 - Gen Loss: 24.1332, GAN: 0.5522, L1: 0.2358, Disc: 1.5868\n  [Train] Batch 080 - Gen Loss: 19.1870, GAN: 1.6584, L1: 0.1753, Disc: 1.0607\n  [Train] Batch 090 - Gen Loss: 15.3895, GAN: 0.4102, L1: 0.1498, Disc: 1.3832\n  [Train] Batch 100 - Gen Loss: 18.4597, GAN: 0.7557, L1: 0.1770, Disc: 1.0172\n  [Train] Batch 110 - Gen Loss: 17.3862, GAN: 0.6155, L1: 0.1677, Disc: 1.1573\n  [Train] Batch 120 - Gen Loss: 24.1414, GAN: 1.4024, L1: 0.2274, Disc: 0.6681\n  [Train] Batch 130 - Gen Loss: 26.7486, GAN: 1.4344, L1: 0.2531, Disc: 0.9276\n  [Train] Batch 140 - Gen Loss: 19.2175, GAN: 1.3063, L1: 0.1791, Disc: 0.8840\n  [Train] Batch 150 - Gen Loss: 23.6847, GAN: 0.8050, L1: 0.2288, Disc: 0.9060\n  [Train] Batch 160 - Gen Loss: 26.9142, GAN: 1.7328, L1: 0.2518, Disc: 0.5434\n  [Train] Batch 170 - Gen Loss: 25.8641, GAN: 1.1023, L1: 0.2476, Disc: 1.7715\n  [Train] Batch 180 - Gen Loss: 25.2068, GAN: 1.0005, L1: 0.2421, Disc: 1.1357\n  [Train] Batch 190 - Gen Loss: 24.3497, GAN: 1.3396, L1: 0.2301, Disc: 0.7271\n  [Train] Batch 200 - Gen Loss: 28.6058, GAN: 1.0985, L1: 0.2751, Disc: 0.7050\n  [Train] Batch 210 - Gen Loss: 23.0881, GAN: 1.0569, L1: 0.2203, Disc: 0.8359\n  [Train] Batch 220 - Gen Loss: 26.4727, GAN: 0.8209, L1: 0.2565, Disc: 0.8890\n  [Train] Batch 230 - Gen Loss: 26.5921, GAN: 2.1878, L1: 0.2440, Disc: 1.0630\n  [Train] Batch 240 - Gen Loss: 22.1008, GAN: 0.8240, L1: 0.2128, Disc: 1.2308\n  [Train] Batch 250 - Gen Loss: 25.8817, GAN: 1.7451, L1: 0.2414, Disc: 0.8964\n  [Train] Batch 260 - Gen Loss: 21.1090, GAN: 1.4088, L1: 0.1970, Disc: 1.0700\n  [Train] Batch 270 - Gen Loss: 23.8269, GAN: 1.2671, L1: 0.2256, Disc: 0.8811\n  [Train] Batch 280 - Gen Loss: 26.4498, GAN: 0.8649, L1: 0.2558, Disc: 0.9441\n  [Train] Batch 290 - Gen Loss: 25.4352, GAN: 1.8325, L1: 0.2360, Disc: 0.9319\n  [Train] Batch 300 - Gen Loss: 23.2310, GAN: 0.9673, L1: 0.2226, Disc: 1.0851\n  [Train] Batch 310 - Gen Loss: 25.6307, GAN: 0.8917, L1: 0.2474, Disc: 1.1338\n  [Train] Batch 320 - Gen Loss: 29.1910, GAN: 0.5547, L1: 0.2864, Disc: 1.2465\n  [Train] Batch 330 - Gen Loss: 27.0125, GAN: 1.2152, L1: 0.2580, Disc: 0.9375\n  [Train] Batch 340 - Gen Loss: 24.8745, GAN: 1.3184, L1: 0.2356, Disc: 0.7132\n  [Train] Batch 350 - Gen Loss: 25.5756, GAN: 1.1484, L1: 0.2443, Disc: 0.9052\n  [Train] Batch 360 - Gen Loss: 26.0460, GAN: 1.8993, L1: 0.2415, Disc: 1.6297\n  [Train] Batch 370 - Gen Loss: 24.0753, GAN: 1.2106, L1: 0.2286, Disc: 0.9112\n  [Train] Batch 380 - Gen Loss: 19.0583, GAN: 1.8101, L1: 0.1725, Disc: 1.3222\n  [Train] Batch 390 - Gen Loss: 16.8797, GAN: 1.2209, L1: 0.1566, Disc: 1.0290\n  [Train] Batch 400 - Gen Loss: 16.0655, GAN: 1.3798, L1: 0.1469, Disc: 1.2336\n  [Train] Batch 410 - Gen Loss: 20.5449, GAN: 1.4049, L1: 0.1914, Disc: 1.1437\n  [Val] MAE: 0.2574, RMSE: 0.4075, SSIM: 0.5441\nEpoch 3 completed in 131.09 seconds.\n\nEpoch 4/150\n  [Train] Batch 000 - Gen Loss: 36.6196, GAN: 0.4906, L1: 0.3613, Disc: 1.1763\n  [Train] Batch 010 - Gen Loss: 21.4483, GAN: 1.3079, L1: 0.2014, Disc: 1.2913\n  [Train] Batch 020 - Gen Loss: 30.4456, GAN: 0.8272, L1: 0.2962, Disc: 0.9431\n  [Train] Batch 030 - Gen Loss: 27.0128, GAN: 0.4167, L1: 0.2660, Disc: 1.2958\n  [Train] Batch 040 - Gen Loss: 24.0757, GAN: 1.7911, L1: 0.2228, Disc: 1.1837\n  [Train] Batch 050 - Gen Loss: 22.5130, GAN: 1.1316, L1: 0.2138, Disc: 0.8787\n  [Train] Batch 060 - Gen Loss: 26.8378, GAN: 1.1977, L1: 0.2564, Disc: 0.9276\n  [Train] Batch 070 - Gen Loss: 22.8768, GAN: 1.3578, L1: 0.2152, Disc: 1.1563\n  [Train] Batch 080 - Gen Loss: 18.5485, GAN: 1.0042, L1: 0.1754, Disc: 1.3166\n  [Train] Batch 090 - Gen Loss: 13.1702, GAN: 0.7184, L1: 0.1245, Disc: 1.1066\n  [Train] Batch 100 - Gen Loss: 18.2026, GAN: 0.5759, L1: 0.1763, Disc: 0.9949\n  [Train] Batch 110 - Gen Loss: 15.3903, GAN: 0.7593, L1: 0.1463, Disc: 1.1010\n  [Train] Batch 120 - Gen Loss: 17.8990, GAN: 1.7853, L1: 0.1611, Disc: 0.9831\n  [Train] Batch 130 - Gen Loss: 15.7297, GAN: 0.6239, L1: 0.1511, Disc: 1.0922\n  [Train] Batch 140 - Gen Loss: 23.8850, GAN: 0.9002, L1: 0.2298, Disc: 0.7262\n  [Train] Batch 150 - Gen Loss: 26.1575, GAN: 0.4022, L1: 0.2576, Disc: 1.2984\n  [Train] Batch 160 - Gen Loss: 23.5928, GAN: 1.9860, L1: 0.2161, Disc: 1.3536\n  [Train] Batch 170 - Gen Loss: 20.5244, GAN: 1.0412, L1: 0.1948, Disc: 0.8671\n  [Train] Batch 180 - Gen Loss: 21.7742, GAN: 1.6468, L1: 0.2013, Disc: 1.2030\n  [Train] Batch 190 - Gen Loss: 20.9637, GAN: 1.2561, L1: 0.1971, Disc: 0.8244\n  [Train] Batch 200 - Gen Loss: 20.9202, GAN: 2.5137, L1: 0.1841, Disc: 1.5356\n  [Train] Batch 210 - Gen Loss: 22.4253, GAN: 1.0769, L1: 0.2135, Disc: 0.8814\n  [Train] Batch 220 - Gen Loss: 25.1738, GAN: 0.4611, L1: 0.2471, Disc: 1.2841\n  [Train] Batch 230 - Gen Loss: 23.7529, GAN: 0.9104, L1: 0.2284, Disc: 0.9346\n  [Train] Batch 240 - Gen Loss: 24.1146, GAN: 1.1827, L1: 0.2293, Disc: 0.7786\n  [Train] Batch 250 - Gen Loss: 21.9981, GAN: 1.0847, L1: 0.2091, Disc: 0.9048\n  [Train] Batch 260 - Gen Loss: 19.8502, GAN: 0.7585, L1: 0.1909, Disc: 1.0225\n  [Train] Batch 270 - Gen Loss: 21.0050, GAN: 1.3667, L1: 0.1964, Disc: 0.8205\n  [Train] Batch 280 - Gen Loss: 28.8200, GAN: 0.8285, L1: 0.2799, Disc: 1.0452\n  [Train] Batch 290 - Gen Loss: 20.6897, GAN: 1.2026, L1: 0.1949, Disc: 0.9862\n  [Train] Batch 300 - Gen Loss: 26.5254, GAN: 0.6624, L1: 0.2586, Disc: 1.1312\n  [Train] Batch 310 - Gen Loss: 22.4726, GAN: 0.6817, L1: 0.2179, Disc: 1.1023\n  [Train] Batch 320 - Gen Loss: 20.4614, GAN: 1.4947, L1: 0.1897, Disc: 0.9768\n  [Train] Batch 330 - Gen Loss: 21.5833, GAN: 1.2602, L1: 0.2032, Disc: 0.8797\n  [Train] Batch 340 - Gen Loss: 25.3322, GAN: 1.7005, L1: 0.2363, Disc: 0.9881\n  [Train] Batch 350 - Gen Loss: 21.3909, GAN: 1.6698, L1: 0.1972, Disc: 0.7914\n  [Train] Batch 360 - Gen Loss: 19.5498, GAN: 0.7738, L1: 0.1878, Disc: 0.9129\n  [Train] Batch 370 - Gen Loss: 23.0747, GAN: 1.4439, L1: 0.2163, Disc: 0.7278\n  [Train] Batch 380 - Gen Loss: 16.5804, GAN: 1.1096, L1: 0.1547, Disc: 0.9635\n  [Train] Batch 390 - Gen Loss: 19.5433, GAN: 1.1078, L1: 0.1844, Disc: 0.7669\n  [Train] Batch 400 - Gen Loss: 16.0147, GAN: 0.7066, L1: 0.1531, Disc: 1.1131\n  [Train] Batch 410 - Gen Loss: 17.7783, GAN: 0.6864, L1: 0.1709, Disc: 1.2837\n  [Val] MAE: 0.2519, RMSE: 0.3962, SSIM: 0.5452\nEpoch 4 completed in 131.00 seconds.\n\nEpoch 5/150\n  [Train] Batch 000 - Gen Loss: 32.5510, GAN: 0.7704, L1: 0.3178, Disc: 0.8663\n  [Train] Batch 010 - Gen Loss: 20.0648, GAN: 1.4178, L1: 0.1865, Disc: 0.7257\n  [Train] Batch 020 - Gen Loss: 26.4520, GAN: 1.0194, L1: 0.2543, Disc: 0.8282\n  [Train] Batch 030 - Gen Loss: 18.9189, GAN: 1.1123, L1: 0.1781, Disc: 1.0321\n  [Train] Batch 040 - Gen Loss: 22.4646, GAN: 1.2597, L1: 0.2120, Disc: 0.6813\n  [Train] Batch 050 - Gen Loss: 20.6786, GAN: 1.9467, L1: 0.1873, Disc: 0.8166\n  [Train] Batch 060 - Gen Loss: 24.0759, GAN: 1.4644, L1: 0.2261, Disc: 0.6300\n  [Train] Batch 070 - Gen Loss: 21.5959, GAN: 0.6584, L1: 0.2094, Disc: 0.9701\n  [Train] Batch 080 - Gen Loss: 16.9236, GAN: 0.2414, L1: 0.1668, Disc: 1.8384\n  [Train] Batch 090 - Gen Loss: 15.5563, GAN: 1.0324, L1: 0.1452, Disc: 0.9549\n  [Train] Batch 100 - Gen Loss: 18.8659, GAN: 1.1015, L1: 0.1776, Disc: 0.8828\n  [Train] Batch 110 - Gen Loss: 22.9973, GAN: 0.6297, L1: 0.2237, Disc: 0.9785\n  [Train] Batch 120 - Gen Loss: 18.8373, GAN: 0.8625, L1: 0.1797, Disc: 1.0431\n  [Train] Batch 130 - Gen Loss: 25.9108, GAN: 1.8743, L1: 0.2404, Disc: 0.6984\n  [Train] Batch 140 - Gen Loss: 22.4045, GAN: 1.2301, L1: 0.2117, Disc: 0.7315\n  [Train] Batch 150 - Gen Loss: 20.1821, GAN: 1.7245, L1: 0.1846, Disc: 0.8630\n  [Train] Batch 160 - Gen Loss: 21.9749, GAN: 2.2321, L1: 0.1974, Disc: 1.3345\n  [Train] Batch 170 - Gen Loss: 19.7750, GAN: 0.8974, L1: 0.1888, Disc: 1.1063\n  [Train] Batch 180 - Gen Loss: 20.1299, GAN: 0.1614, L1: 0.1997, Disc: 2.2042\n  [Train] Batch 190 - Gen Loss: 18.5256, GAN: 1.3226, L1: 0.1720, Disc: 0.9725\n  [Train] Batch 200 - Gen Loss: 19.7147, GAN: 0.5870, L1: 0.1913, Disc: 1.0988\n  [Train] Batch 210 - Gen Loss: 20.2978, GAN: 1.3157, L1: 0.1898, Disc: 0.8470\n  [Train] Batch 220 - Gen Loss: 23.2902, GAN: 1.4083, L1: 0.2188, Disc: 0.8459\n  [Train] Batch 230 - Gen Loss: 22.7248, GAN: 1.6701, L1: 0.2105, Disc: 0.8869\n  [Train] Batch 240 - Gen Loss: 21.8574, GAN: 0.7408, L1: 0.2112, Disc: 1.2459\n  [Train] Batch 250 - Gen Loss: 15.1276, GAN: 1.4565, L1: 0.1367, Disc: 1.0173\n  [Train] Batch 260 - Gen Loss: 24.9055, GAN: 1.1701, L1: 0.2374, Disc: 0.6067\n  [Train] Batch 270 - Gen Loss: 21.3619, GAN: 1.3560, L1: 0.2001, Disc: 0.9491\n  [Train] Batch 280 - Gen Loss: 22.6056, GAN: 1.3577, L1: 0.2125, Disc: 0.9693\n  [Train] Batch 290 - Gen Loss: 21.4644, GAN: 1.7240, L1: 0.1974, Disc: 1.8043\n  [Train] Batch 300 - Gen Loss: 20.8708, GAN: 2.9482, L1: 0.1792, Disc: 1.3464\n  [Train] Batch 310 - Gen Loss: 23.2389, GAN: 0.4812, L1: 0.2276, Disc: 1.2352\n  [Train] Batch 320 - Gen Loss: 20.7653, GAN: 1.2306, L1: 0.1953, Disc: 1.1613\n  [Train] Batch 330 - Gen Loss: 19.1456, GAN: 0.8043, L1: 0.1834, Disc: 0.9926\n  [Train] Batch 340 - Gen Loss: 19.9009, GAN: 1.7129, L1: 0.1819, Disc: 0.9421\n  [Train] Batch 350 - Gen Loss: 19.0302, GAN: 1.2860, L1: 0.1774, Disc: 1.1214\n  [Train] Batch 360 - Gen Loss: 19.5782, GAN: 1.5519, L1: 0.1803, Disc: 0.9872\n  [Train] Batch 370 - Gen Loss: 18.3299, GAN: 1.1763, L1: 0.1715, Disc: 0.8422\n  [Train] Batch 380 - Gen Loss: 14.8558, GAN: 0.7643, L1: 0.1409, Disc: 1.0105\n  [Train] Batch 390 - Gen Loss: 17.7810, GAN: 1.0511, L1: 0.1673, Disc: 1.1406\n  [Train] Batch 400 - Gen Loss: 14.5243, GAN: 1.1713, L1: 0.1335, Disc: 1.1291\n  [Train] Batch 410 - Gen Loss: 13.4117, GAN: 1.1544, L1: 0.1226, Disc: 1.7878\n  [Val] MAE: 0.2436, RMSE: 0.3838, SSIM: 0.5498\nEpoch 5 completed in 131.02 seconds.\n\nEpoch 6/150\n  [Train] Batch 000 - Gen Loss: 25.3568, GAN: 0.9982, L1: 0.2436, Disc: 2.1085\n  [Train] Batch 010 - Gen Loss: 21.7767, GAN: 0.7537, L1: 0.2102, Disc: 1.4262\n  [Train] Batch 020 - Gen Loss: 19.5665, GAN: 0.7146, L1: 0.1885, Disc: 1.0176\n  [Train] Batch 030 - Gen Loss: 19.1681, GAN: 1.2357, L1: 0.1793, Disc: 0.9379\n  [Train] Batch 040 - Gen Loss: 23.4536, GAN: 1.0379, L1: 0.2242, Disc: 1.1243\n  [Train] Batch 050 - Gen Loss: 18.7511, GAN: 1.3024, L1: 0.1745, Disc: 0.7717\n  [Train] Batch 060 - Gen Loss: 23.3346, GAN: 1.6069, L1: 0.2173, Disc: 0.7891\n  [Train] Batch 070 - Gen Loss: 20.8848, GAN: 1.9877, L1: 0.1890, Disc: 0.8214\n  [Train] Batch 080 - Gen Loss: 18.1923, GAN: 0.9943, L1: 0.1720, Disc: 0.8589\n  [Train] Batch 090 - Gen Loss: 13.8606, GAN: 1.2576, L1: 0.1260, Disc: 1.1736\n  [Train] Batch 100 - Gen Loss: 13.1195, GAN: 1.3588, L1: 0.1176, Disc: 1.1772\n  [Train] Batch 110 - Gen Loss: 18.4318, GAN: 1.9636, L1: 0.1647, Disc: 1.1037\n  [Train] Batch 120 - Gen Loss: 18.3471, GAN: 0.2146, L1: 0.1813, Disc: 1.9843\n  [Train] Batch 130 - Gen Loss: 17.3507, GAN: 1.2067, L1: 0.1614, Disc: 0.9515\n  [Train] Batch 140 - Gen Loss: 14.9814, GAN: 0.1971, L1: 0.1478, Disc: 1.9299\n  [Train] Batch 150 - Gen Loss: 19.4483, GAN: 1.7478, L1: 0.1770, Disc: 0.7145\n  [Train] Batch 160 - Gen Loss: 16.4909, GAN: 0.8365, L1: 0.1565, Disc: 0.9614\n  [Train] Batch 170 - Gen Loss: 18.9010, GAN: 2.1041, L1: 0.1680, Disc: 0.9330\n  [Train] Batch 180 - Gen Loss: 25.7349, GAN: 0.8434, L1: 0.2489, Disc: 0.7990\n  [Train] Batch 190 - Gen Loss: 19.1819, GAN: 1.3351, L1: 0.1785, Disc: 1.0588\n  [Train] Batch 200 - Gen Loss: 17.7459, GAN: 0.5957, L1: 0.1715, Disc: 1.1110\n  [Train] Batch 210 - Gen Loss: 17.3667, GAN: 0.8842, L1: 0.1648, Disc: 0.7604\n  [Train] Batch 220 - Gen Loss: 21.2511, GAN: 0.7532, L1: 0.2050, Disc: 0.9388\n  [Train] Batch 230 - Gen Loss: 18.1662, GAN: 2.4099, L1: 0.1576, Disc: 1.2798\n  [Train] Batch 240 - Gen Loss: 19.7851, GAN: 1.4216, L1: 0.1836, Disc: 1.1625\n  [Train] Batch 250 - Gen Loss: 17.9783, GAN: 0.9405, L1: 0.1704, Disc: 0.7874\n  [Train] Batch 260 - Gen Loss: 21.0667, GAN: 2.5719, L1: 0.1849, Disc: 0.8897\n  [Train] Batch 270 - Gen Loss: 18.1677, GAN: 1.3589, L1: 0.1681, Disc: 0.9011\n  [Train] Batch 280 - Gen Loss: 23.5207, GAN: 1.3271, L1: 0.2219, Disc: 0.8449\n  [Train] Batch 290 - Gen Loss: 17.6247, GAN: 1.5846, L1: 0.1604, Disc: 0.8104\n  [Train] Batch 300 - Gen Loss: 23.8803, GAN: 1.8470, L1: 0.2203, Disc: 0.6782\n  [Train] Batch 310 - Gen Loss: 19.9475, GAN: 2.0191, L1: 0.1793, Disc: 1.4512\n  [Train] Batch 320 - Gen Loss: 15.5212, GAN: 1.3723, L1: 0.1415, Disc: 1.4130\n  [Train] Batch 330 - Gen Loss: 21.2535, GAN: 1.4818, L1: 0.1977, Disc: 0.7825\n  [Train] Batch 340 - Gen Loss: 21.5595, GAN: 1.2270, L1: 0.2033, Disc: 0.7482\n  [Train] Batch 350 - Gen Loss: 17.8351, GAN: 0.7315, L1: 0.1710, Disc: 1.0782\n  [Train] Batch 360 - Gen Loss: 15.6235, GAN: 1.7418, L1: 0.1388, Disc: 1.5053\n  [Train] Batch 370 - Gen Loss: 19.8317, GAN: 1.2599, L1: 0.1857, Disc: 1.0743\n  [Train] Batch 380 - Gen Loss: 16.8960, GAN: 1.3275, L1: 0.1557, Disc: 1.0563\n  [Train] Batch 390 - Gen Loss: 12.1002, GAN: 0.9841, L1: 0.1112, Disc: 1.2771\n  [Train] Batch 400 - Gen Loss: 15.2138, GAN: 1.3581, L1: 0.1386, Disc: 0.9880\n  [Train] Batch 410 - Gen Loss: 15.1287, GAN: 0.6514, L1: 0.1448, Disc: 1.1312\n  [Val] MAE: 0.2401, RMSE: 0.3820, SSIM: 0.5628\nEpoch 6 completed in 130.97 seconds.\n\nEpoch 7/150\n  [Train] Batch 000 - Gen Loss: 19.2801, GAN: 1.0615, L1: 0.1822, Disc: 0.9918\n  [Train] Batch 010 - Gen Loss: 16.9550, GAN: 1.5644, L1: 0.1539, Disc: 0.9010\n  [Train] Batch 020 - Gen Loss: 15.8739, GAN: 1.5478, L1: 0.1433, Disc: 0.9620\n  [Train] Batch 030 - Gen Loss: 19.4655, GAN: 1.3201, L1: 0.1815, Disc: 0.8232\n  [Train] Batch 040 - Gen Loss: 17.1470, GAN: 0.7802, L1: 0.1637, Disc: 1.2306\n  [Train] Batch 050 - Gen Loss: 17.2823, GAN: 0.7299, L1: 0.1655, Disc: 0.9905\n  [Train] Batch 060 - Gen Loss: 21.7731, GAN: 1.4395, L1: 0.2033, Disc: 0.9003\n  [Train] Batch 070 - Gen Loss: 18.9450, GAN: 0.9778, L1: 0.1797, Disc: 0.9515\n  [Train] Batch 080 - Gen Loss: 16.9560, GAN: 1.0086, L1: 0.1595, Disc: 0.8922\n  [Train] Batch 090 - Gen Loss: 14.4620, GAN: 0.9366, L1: 0.1353, Disc: 0.9377\n  [Train] Batch 100 - Gen Loss: 14.1737, GAN: 1.6450, L1: 0.1253, Disc: 0.7600\n  [Train] Batch 110 - Gen Loss: 16.7524, GAN: 2.4464, L1: 0.1431, Disc: 1.4425\n  [Train] Batch 120 - Gen Loss: 20.7548, GAN: 1.1809, L1: 0.1957, Disc: 0.7388\n  [Train] Batch 130 - Gen Loss: 13.0382, GAN: 1.5874, L1: 0.1145, Disc: 1.0158\n  [Train] Batch 140 - Gen Loss: 16.0974, GAN: 1.4939, L1: 0.1460, Disc: 0.8174\n  [Train] Batch 150 - Gen Loss: 19.2117, GAN: 0.7271, L1: 0.1848, Disc: 1.0079\n  [Train] Batch 160 - Gen Loss: 18.7437, GAN: 1.9724, L1: 0.1677, Disc: 1.0630\n  [Train] Batch 170 - Gen Loss: 16.5840, GAN: 1.1571, L1: 0.1543, Disc: 0.9379\n  [Train] Batch 180 - Gen Loss: 17.4434, GAN: 1.6097, L1: 0.1583, Disc: 1.0217\n  [Train] Batch 190 - Gen Loss: 14.0140, GAN: 0.9660, L1: 0.1305, Disc: 1.0724\n  [Train] Batch 200 - Gen Loss: 15.0146, GAN: 1.3162, L1: 0.1370, Disc: 1.1126\n  [Train] Batch 210 - Gen Loss: 15.1290, GAN: 1.4042, L1: 0.1372, Disc: 1.4523\n  [Train] Batch 220 - Gen Loss: 22.7446, GAN: 2.2150, L1: 0.2053, Disc: 0.8863\n  [Train] Batch 230 - Gen Loss: 18.1211, GAN: 1.6223, L1: 0.1650, Disc: 0.7839\n  [Train] Batch 240 - Gen Loss: 16.3104, GAN: 0.6767, L1: 0.1563, Disc: 1.1049\n  [Train] Batch 250 - Gen Loss: 16.3772, GAN: 1.3369, L1: 0.1504, Disc: 0.9475\n  [Train] Batch 260 - Gen Loss: 17.1122, GAN: 0.7671, L1: 0.1635, Disc: 0.9617\n  [Train] Batch 270 - Gen Loss: 23.2460, GAN: 0.8282, L1: 0.2242, Disc: 0.7942\n  [Train] Batch 280 - Gen Loss: 17.4441, GAN: 0.9759, L1: 0.1647, Disc: 0.7681\n  [Train] Batch 290 - Gen Loss: 19.0867, GAN: 0.8796, L1: 0.1821, Disc: 0.9604\n  [Train] Batch 300 - Gen Loss: 16.8117, GAN: 0.4480, L1: 0.1636, Disc: 1.3609\n  [Train] Batch 310 - Gen Loss: 17.5307, GAN: 2.1625, L1: 0.1537, Disc: 1.0780\n  [Train] Batch 320 - Gen Loss: 14.1287, GAN: 1.2559, L1: 0.1287, Disc: 1.2365\n  [Train] Batch 330 - Gen Loss: 19.7198, GAN: 1.0762, L1: 0.1864, Disc: 1.0435\n  [Train] Batch 340 - Gen Loss: 17.7802, GAN: 2.3103, L1: 0.1547, Disc: 0.7624\n  [Train] Batch 350 - Gen Loss: 16.3826, GAN: 1.8907, L1: 0.1449, Disc: 0.6932\n  [Train] Batch 360 - Gen Loss: 19.7798, GAN: 1.7525, L1: 0.1803, Disc: 0.9418\n  [Train] Batch 370 - Gen Loss: 13.4390, GAN: 1.2680, L1: 0.1217, Disc: 1.2875\n  [Train] Batch 380 - Gen Loss: 15.1409, GAN: 0.9236, L1: 0.1422, Disc: 1.1677\n  [Train] Batch 390 - Gen Loss: 13.2943, GAN: 1.3971, L1: 0.1190, Disc: 1.6591\n  [Train] Batch 400 - Gen Loss: 12.5967, GAN: 0.9483, L1: 0.1165, Disc: 1.0086\n  [Train] Batch 410 - Gen Loss: 13.8793, GAN: 1.0110, L1: 0.1287, Disc: 0.9852\n  [Val] MAE: 0.2345, RMSE: 0.3793, SSIM: 0.5754\nEpoch 7 completed in 131.11 seconds.\n\nEpoch 8/150\n  [Train] Batch 000 - Gen Loss: 17.1431, GAN: 1.7610, L1: 0.1538, Disc: 0.8809\n  [Train] Batch 010 - Gen Loss: 14.8402, GAN: 1.6741, L1: 0.1317, Disc: 0.8316\n  [Train] Batch 020 - Gen Loss: 15.8633, GAN: 1.0075, L1: 0.1486, Disc: 0.9873\n  [Train] Batch 030 - Gen Loss: 17.1399, GAN: 0.6461, L1: 0.1649, Disc: 0.9876\n  [Train] Batch 040 - Gen Loss: 19.4808, GAN: 0.9916, L1: 0.1849, Disc: 0.7667\n  [Train] Batch 050 - Gen Loss: 15.9190, GAN: 1.7873, L1: 0.1413, Disc: 0.9253\n  [Train] Batch 060 - Gen Loss: 21.8625, GAN: 0.7170, L1: 0.2115, Disc: 0.9272\n  [Train] Batch 070 - Gen Loss: 16.1284, GAN: 0.9594, L1: 0.1517, Disc: 0.9004\n  [Train] Batch 080 - Gen Loss: 15.8131, GAN: 1.3971, L1: 0.1442, Disc: 1.2555\n  [Train] Batch 090 - Gen Loss: 13.2436, GAN: 1.1370, L1: 0.1211, Disc: 0.9719\n  [Train] Batch 100 - Gen Loss: 15.0127, GAN: 1.6540, L1: 0.1336, Disc: 0.8538\n  [Train] Batch 110 - Gen Loss: 15.7418, GAN: 0.7163, L1: 0.1503, Disc: 1.0417\n  [Train] Batch 120 - Gen Loss: 13.5151, GAN: 1.5421, L1: 0.1197, Disc: 0.9337\n  [Train] Batch 130 - Gen Loss: 13.8207, GAN: 0.8919, L1: 0.1293, Disc: 1.1328\n  [Train] Batch 140 - Gen Loss: 17.0179, GAN: 1.8720, L1: 0.1515, Disc: 0.7179\n  [Train] Batch 150 - Gen Loss: 17.8716, GAN: 1.7530, L1: 0.1612, Disc: 0.6364\n  [Train] Batch 160 - Gen Loss: 16.9482, GAN: 1.4617, L1: 0.1549, Disc: 1.0683\n  [Train] Batch 170 - Gen Loss: 16.2208, GAN: 2.7662, L1: 0.1345, Disc: 1.5335\n  [Train] Batch 180 - Gen Loss: 16.2125, GAN: 1.4711, L1: 0.1474, Disc: 2.3511\n  [Train] Batch 190 - Gen Loss: 17.5972, GAN: 1.2262, L1: 0.1637, Disc: 0.9472\n  [Train] Batch 200 - Gen Loss: 15.9668, GAN: 1.0579, L1: 0.1491, Disc: 0.7870\n  [Train] Batch 210 - Gen Loss: 16.3381, GAN: 0.8299, L1: 0.1551, Disc: 0.9929\n  [Train] Batch 220 - Gen Loss: 16.3973, GAN: 0.7651, L1: 0.1563, Disc: 0.9585\n  [Train] Batch 230 - Gen Loss: 20.6847, GAN: 1.3201, L1: 0.1936, Disc: 0.9266\n  [Train] Batch 240 - Gen Loss: 14.3871, GAN: 1.5837, L1: 0.1280, Disc: 0.8692\n  [Train] Batch 250 - Gen Loss: 17.2598, GAN: 2.7513, L1: 0.1451, Disc: 1.7543\n  [Train] Batch 260 - Gen Loss: 16.4517, GAN: 1.1316, L1: 0.1532, Disc: 0.9480\n  [Train] Batch 270 - Gen Loss: 19.3855, GAN: 1.9038, L1: 0.1748, Disc: 1.0090\n  [Train] Batch 280 - Gen Loss: 21.0779, GAN: 0.9036, L1: 0.2017, Disc: 0.8380\n  [Train] Batch 290 - Gen Loss: 16.1652, GAN: 1.9174, L1: 0.1425, Disc: 0.8782\n  [Train] Batch 300 - Gen Loss: 20.6036, GAN: 1.0264, L1: 0.1958, Disc: 0.8059\n  [Train] Batch 310 - Gen Loss: 17.0789, GAN: 1.2412, L1: 0.1584, Disc: 0.6881\n  [Train] Batch 320 - Gen Loss: 13.7920, GAN: 1.3163, L1: 0.1248, Disc: 1.1134\n  [Train] Batch 330 - Gen Loss: 18.4146, GAN: 1.7263, L1: 0.1669, Disc: 0.8128\n  [Train] Batch 340 - Gen Loss: 16.8377, GAN: 1.7727, L1: 0.1507, Disc: 0.8525\n  [Train] Batch 350 - Gen Loss: 13.7770, GAN: 1.1931, L1: 0.1258, Disc: 1.9275\n  [Train] Batch 360 - Gen Loss: 16.0426, GAN: 1.5127, L1: 0.1453, Disc: 0.8168\n  [Train] Batch 370 - Gen Loss: 15.5804, GAN: 0.9695, L1: 0.1461, Disc: 1.5253\n  [Train] Batch 380 - Gen Loss: 13.5727, GAN: 0.6676, L1: 0.1291, Disc: 1.2244\n  [Train] Batch 390 - Gen Loss: 15.0762, GAN: 0.6882, L1: 0.1439, Disc: 0.9730\n  [Train] Batch 400 - Gen Loss: 12.2368, GAN: 1.1154, L1: 0.1112, Disc: 0.9628\n  [Train] Batch 410 - Gen Loss: 10.4998, GAN: 1.0997, L1: 0.0940, Disc: 1.2798\n  [Val] MAE: 0.2183, RMSE: 0.3562, SSIM: 0.5828\nEpoch 8 completed in 131.03 seconds.\n\nEpoch 9/150\n  [Train] Batch 000 - Gen Loss: 20.0103, GAN: 1.3017, L1: 0.1871, Disc: 0.8153\n  [Train] Batch 010 - Gen Loss: 17.2756, GAN: 0.9599, L1: 0.1632, Disc: 1.0678\n  [Train] Batch 020 - Gen Loss: 16.2557, GAN: 0.7757, L1: 0.1548, Disc: 1.1125\n  [Train] Batch 030 - Gen Loss: 15.2602, GAN: 1.0670, L1: 0.1419, Disc: 0.9834\n  [Train] Batch 040 - Gen Loss: 15.9039, GAN: 2.1049, L1: 0.1380, Disc: 0.7820\n  [Train] Batch 050 - Gen Loss: 15.1502, GAN: 0.3759, L1: 0.1477, Disc: 1.3800\n  [Train] Batch 060 - Gen Loss: 15.4570, GAN: 1.8808, L1: 0.1358, Disc: 0.7791\n  [Train] Batch 070 - Gen Loss: 20.1040, GAN: 0.8029, L1: 0.1930, Disc: 0.8386\n  [Train] Batch 080 - Gen Loss: 14.7046, GAN: 1.8586, L1: 0.1285, Disc: 0.7827\n  [Train] Batch 090 - Gen Loss: 12.6270, GAN: 0.8784, L1: 0.1175, Disc: 1.0780\n  [Train] Batch 100 - Gen Loss: 12.6953, GAN: 1.0870, L1: 0.1161, Disc: 0.9796\n  [Train] Batch 110 - Gen Loss: 15.4572, GAN: 1.7630, L1: 0.1369, Disc: 0.8680\n  [Train] Batch 120 - Gen Loss: 16.8799, GAN: 1.1447, L1: 0.1574, Disc: 0.8423\n  [Train] Batch 130 - Gen Loss: 11.0420, GAN: 1.0514, L1: 0.0999, Disc: 1.0100\n  [Train] Batch 140 - Gen Loss: 14.7681, GAN: 1.7312, L1: 0.1304, Disc: 0.9565\n  [Train] Batch 150 - Gen Loss: 19.2707, GAN: 1.4129, L1: 0.1786, Disc: 0.6211\n  [Train] Batch 160 - Gen Loss: 15.6485, GAN: 0.7953, L1: 0.1485, Disc: 0.8775\n  [Train] Batch 170 - Gen Loss: 15.7123, GAN: 1.9111, L1: 0.1380, Disc: 1.0864\n  [Train] Batch 180 - Gen Loss: 15.3892, GAN: 1.5674, L1: 0.1382, Disc: 1.3610\n  [Train] Batch 190 - Gen Loss: 23.6519, GAN: 1.3626, L1: 0.2229, Disc: 2.5621\n  [Train] Batch 200 - Gen Loss: 16.2257, GAN: 1.3241, L1: 0.1490, Disc: 0.9550\n  [Train] Batch 210 - Gen Loss: 17.8135, GAN: 2.3247, L1: 0.1549, Disc: 0.7869\n  [Train] Batch 220 - Gen Loss: 16.2416, GAN: 1.0472, L1: 0.1519, Disc: 0.8499\n  [Train] Batch 230 - Gen Loss: 15.4066, GAN: 1.2883, L1: 0.1412, Disc: 0.8471\n  [Train] Batch 240 - Gen Loss: 16.4381, GAN: 1.4001, L1: 0.1504, Disc: 1.0725\n  [Train] Batch 250 - Gen Loss: 14.9830, GAN: 1.6518, L1: 0.1333, Disc: 1.0927\n  [Train] Batch 260 - Gen Loss: 16.7566, GAN: 0.8774, L1: 0.1588, Disc: 0.7755\n  [Train] Batch 270 - Gen Loss: 20.8201, GAN: 0.8758, L1: 0.1994, Disc: 0.7534\n  [Train] Batch 280 - Gen Loss: 18.8208, GAN: 0.6495, L1: 0.1817, Disc: 1.0259\n  [Train] Batch 290 - Gen Loss: 15.9084, GAN: 1.1464, L1: 0.1476, Disc: 0.8515\n  [Train] Batch 300 - Gen Loss: 18.4786, GAN: 0.4194, L1: 0.1806, Disc: 1.2592\n  [Train] Batch 310 - Gen Loss: 16.0284, GAN: 1.7638, L1: 0.1426, Disc: 0.7975\n  [Train] Batch 320 - Gen Loss: 15.9773, GAN: 0.9892, L1: 0.1499, Disc: 0.7843\n  [Train] Batch 330 - Gen Loss: 15.1731, GAN: 2.0689, L1: 0.1310, Disc: 1.3754\n  [Train] Batch 340 - Gen Loss: 18.6101, GAN: 1.2305, L1: 0.1738, Disc: 0.6208\n  [Train] Batch 350 - Gen Loss: 15.5592, GAN: 0.7967, L1: 0.1476, Disc: 0.8216\n  [Train] Batch 360 - Gen Loss: 15.2656, GAN: 0.7881, L1: 0.1448, Disc: 1.0332\n  [Train] Batch 370 - Gen Loss: 14.4704, GAN: 1.0677, L1: 0.1340, Disc: 0.8386\n  [Train] Batch 380 - Gen Loss: 13.5054, GAN: 0.8267, L1: 0.1268, Disc: 1.2764\n  [Train] Batch 390 - Gen Loss: 11.4368, GAN: 0.3180, L1: 0.1112, Disc: 1.5854\n  [Train] Batch 400 - Gen Loss: 12.0430, GAN: 1.6405, L1: 0.1040, Disc: 1.1627\n  [Train] Batch 410 - Gen Loss: 13.6025, GAN: 1.6609, L1: 0.1194, Disc: 1.0244\n  [Val] MAE: 0.2112, RMSE: 0.3441, SSIM: 0.5801\nEpoch 9 completed in 131.03 seconds.\n\nEpoch 10/150\n  [Train] Batch 000 - Gen Loss: 13.4127, GAN: 0.5588, L1: 0.1285, Disc: 1.1280\n  [Train] Batch 010 - Gen Loss: 14.5972, GAN: 0.9999, L1: 0.1360, Disc: 0.8850\n  [Train] Batch 020 - Gen Loss: 14.4822, GAN: 0.9719, L1: 0.1351, Disc: 0.9722\n  [Train] Batch 030 - Gen Loss: 14.0827, GAN: 1.1274, L1: 0.1296, Disc: 0.7768\n  [Train] Batch 040 - Gen Loss: 15.4733, GAN: 2.2492, L1: 0.1322, Disc: 0.9719\n  [Train] Batch 050 - Gen Loss: 15.3908, GAN: 1.6584, L1: 0.1373, Disc: 1.7609\n  [Train] Batch 060 - Gen Loss: 11.7620, GAN: 1.6104, L1: 0.1015, Disc: 0.9990\n  [Train] Batch 070 - Gen Loss: 13.5227, GAN: 0.6493, L1: 0.1287, Disc: 1.2188\n  [Train] Batch 080 - Gen Loss: 12.3371, GAN: 1.1071, L1: 0.1123, Disc: 0.8744\n  [Train] Batch 090 - Gen Loss: 11.2400, GAN: 0.4925, L1: 0.1075, Disc: 1.2491\n  [Train] Batch 100 - Gen Loss: 13.0138, GAN: 1.8393, L1: 0.1117, Disc: 1.2336\n  [Train] Batch 110 - Gen Loss: 13.7866, GAN: 0.9810, L1: 0.1281, Disc: 0.9991\n  [Train] Batch 120 - Gen Loss: 16.5600, GAN: 2.3696, L1: 0.1419, Disc: 1.2434\n  [Train] Batch 130 - Gen Loss: 14.6432, GAN: 1.2286, L1: 0.1341, Disc: 0.7810\n  [Train] Batch 140 - Gen Loss: 15.9250, GAN: 1.1566, L1: 0.1477, Disc: 0.7480\n  [Train] Batch 150 - Gen Loss: 13.2656, GAN: 1.4622, L1: 0.1180, Disc: 1.1652\n  [Train] Batch 160 - Gen Loss: 16.6856, GAN: 1.3027, L1: 0.1538, Disc: 0.7130\n  [Train] Batch 170 - Gen Loss: 15.1110, GAN: 2.2053, L1: 0.1291, Disc: 0.8667\n  [Train] Batch 180 - Gen Loss: 16.3991, GAN: 2.0444, L1: 0.1435, Disc: 0.6941\n  [Train] Batch 190 - Gen Loss: 14.7600, GAN: 0.5658, L1: 0.1419, Disc: 1.2571\n  [Train] Batch 200 - Gen Loss: 15.5658, GAN: 1.6720, L1: 0.1389, Disc: 0.9979\n  [Train] Batch 210 - Gen Loss: 16.0008, GAN: 0.4115, L1: 0.1559, Disc: 1.3102\n  [Train] Batch 220 - Gen Loss: 16.8268, GAN: 1.4934, L1: 0.1533, Disc: 0.7701\n  [Train] Batch 230 - Gen Loss: 17.7600, GAN: 1.7779, L1: 0.1598, Disc: 0.8316\n  [Train] Batch 240 - Gen Loss: 15.3716, GAN: 1.4742, L1: 0.1390, Disc: 0.7264\n  [Train] Batch 250 - Gen Loss: 12.3708, GAN: 1.3989, L1: 0.1097, Disc: 0.9396\n  [Train] Batch 260 - Gen Loss: 14.8146, GAN: 1.0886, L1: 0.1373, Disc: 0.8341\n  [Train] Batch 270 - Gen Loss: 18.1922, GAN: 2.5842, L1: 0.1561, Disc: 1.4686\n  [Train] Batch 280 - Gen Loss: 16.5220, GAN: 0.3949, L1: 0.1613, Disc: 1.4176\n  [Train] Batch 290 - Gen Loss: 15.4258, GAN: 0.7164, L1: 0.1471, Disc: 1.0785\n  [Train] Batch 300 - Gen Loss: 13.2308, GAN: 1.2605, L1: 0.1197, Disc: 0.9355\n  [Train] Batch 310 - Gen Loss: 12.8532, GAN: 1.2356, L1: 0.1162, Disc: 1.0196\n  [Train] Batch 320 - Gen Loss: 16.4413, GAN: 1.4623, L1: 0.1498, Disc: 2.6715\n  [Train] Batch 330 - Gen Loss: 18.2876, GAN: 0.7557, L1: 0.1753, Disc: 1.3780\n  [Train] Batch 340 - Gen Loss: 14.1079, GAN: 0.8091, L1: 0.1330, Disc: 0.9154\n  [Train] Batch 350 - Gen Loss: 15.4254, GAN: 2.2406, L1: 0.1318, Disc: 0.9156\n  [Train] Batch 360 - Gen Loss: 17.1467, GAN: 0.7241, L1: 0.1642, Disc: 0.9738\n  [Train] Batch 370 - Gen Loss: 15.0530, GAN: 1.3597, L1: 0.1369, Disc: 0.8753\n  [Train] Batch 380 - Gen Loss: 12.9001, GAN: 1.0704, L1: 0.1183, Disc: 0.9303\n  [Train] Batch 390 - Gen Loss: 11.4999, GAN: 0.8330, L1: 0.1067, Disc: 1.0133\n  [Train] Batch 400 - Gen Loss: 10.6502, GAN: 0.8056, L1: 0.0984, Disc: 1.0501\n  [Train] Batch 410 - Gen Loss: 11.6740, GAN: 1.3280, L1: 0.1035, Disc: 1.1047\n  [Val] MAE: 0.2105, RMSE: 0.3441, SSIM: 0.5806\nCheckpoint saved: /kaggle/working/outputs/checkpoints/ckpt_epoch_10\nEpoch 10 completed in 136.25 seconds.\n\nEpoch 11/150\n  [Train] Batch 000 - Gen Loss: 15.8006, GAN: 1.6240, L1: 0.1418, Disc: 0.7280\n  [Train] Batch 010 - Gen Loss: 13.9809, GAN: 0.9664, L1: 0.1301, Disc: 0.8814\n  [Train] Batch 020 - Gen Loss: 17.1323, GAN: 1.0977, L1: 0.1603, Disc: 0.8162\n  [Train] Batch 030 - Gen Loss: 16.1983, GAN: 1.0941, L1: 0.1510, Disc: 0.8954\n  [Train] Batch 040 - Gen Loss: 15.8888, GAN: 1.1517, L1: 0.1474, Disc: 0.6789\n  [Train] Batch 050 - Gen Loss: 14.5888, GAN: 0.6086, L1: 0.1398, Disc: 1.1096\n  [Train] Batch 060 - Gen Loss: 14.6627, GAN: 1.3973, L1: 0.1327, Disc: 0.6990\n  [Train] Batch 070 - Gen Loss: 17.9687, GAN: 0.7064, L1: 0.1726, Disc: 0.8687\n  [Train] Batch 080 - Gen Loss: 11.9801, GAN: 1.0559, L1: 0.1092, Disc: 1.0991\n  [Train] Batch 090 - Gen Loss: 8.6135, GAN: 1.0702, L1: 0.0754, Disc: 1.3408\n  [Train] Batch 100 - Gen Loss: 11.5052, GAN: 1.0570, L1: 0.1045, Disc: 0.8732\n  [Train] Batch 110 - Gen Loss: 11.4623, GAN: 0.8309, L1: 0.1063, Disc: 1.1217\n  [Train] Batch 120 - Gen Loss: 13.1137, GAN: 0.8688, L1: 0.1224, Disc: 1.0104\n  [Train] Batch 130 - Gen Loss: 12.6175, GAN: 0.7963, L1: 0.1182, Disc: 0.9729\n  [Train] Batch 140 - Gen Loss: 12.8431, GAN: 1.2142, L1: 0.1163, Disc: 0.7689\n  [Train] Batch 150 - Gen Loss: 15.7382, GAN: 1.7442, L1: 0.1399, Disc: 1.0967\n  [Train] Batch 160 - Gen Loss: 13.9322, GAN: 0.9274, L1: 0.1300, Disc: 0.8723\n  [Train] Batch 170 - Gen Loss: 14.5779, GAN: 1.0271, L1: 0.1355, Disc: 0.7272\n  [Train] Batch 180 - Gen Loss: 12.7457, GAN: 0.5842, L1: 0.1216, Disc: 1.1642\n  [Train] Batch 190 - Gen Loss: 12.4635, GAN: 1.7324, L1: 0.1073, Disc: 1.0394\n  [Train] Batch 200 - Gen Loss: 16.0677, GAN: 0.5727, L1: 0.1549, Disc: 1.3829\n  [Train] Batch 210 - Gen Loss: 15.8732, GAN: 1.8776, L1: 0.1400, Disc: 0.8291\n  [Train] Batch 220 - Gen Loss: 15.0270, GAN: 1.3237, L1: 0.1370, Disc: 0.7414\n  [Train] Batch 230 - Gen Loss: 12.3982, GAN: 1.8202, L1: 0.1058, Disc: 1.3310\n  [Train] Batch 240 - Gen Loss: 15.4929, GAN: 0.8947, L1: 0.1460, Disc: 0.8287\n  [Train] Batch 250 - Gen Loss: 16.8202, GAN: 0.7051, L1: 0.1612, Disc: 0.9332\n  [Train] Batch 260 - Gen Loss: 15.4827, GAN: 1.8277, L1: 0.1366, Disc: 0.6198\n  [Train] Batch 270 - Gen Loss: 15.7129, GAN: 2.0632, L1: 0.1365, Disc: 0.8523\n  [Train] Batch 280 - Gen Loss: 14.1772, GAN: 0.8676, L1: 0.1331, Disc: 1.0000\n  [Train] Batch 290 - Gen Loss: 13.4971, GAN: 1.7774, L1: 0.1172, Disc: 1.2540\n  [Train] Batch 300 - Gen Loss: 16.1711, GAN: 0.9020, L1: 0.1527, Disc: 0.7557\n  [Train] Batch 310 - Gen Loss: 12.5773, GAN: 1.5837, L1: 0.1099, Disc: 1.2003\n  [Train] Batch 320 - Gen Loss: 13.5202, GAN: 0.4772, L1: 0.1304, Disc: 1.1834\n  [Train] Batch 330 - Gen Loss: 15.4989, GAN: 2.1253, L1: 0.1337, Disc: 1.2178\n  [Train] Batch 340 - Gen Loss: 16.3080, GAN: 4.7468, L1: 0.1156, Disc: 8.3923\n  [Train] Batch 350 - Gen Loss: 13.8828, GAN: 0.7671, L1: 0.1312, Disc: 1.3117\n  [Train] Batch 360 - Gen Loss: 14.4583, GAN: 1.0371, L1: 0.1342, Disc: 1.0579\n  [Train] Batch 370 - Gen Loss: 13.3063, GAN: 0.4964, L1: 0.1281, Disc: 1.3474\n  [Train] Batch 380 - Gen Loss: 12.7366, GAN: 0.8569, L1: 0.1188, Disc: 1.2632\n  [Train] Batch 390 - Gen Loss: 10.5374, GAN: 1.1704, L1: 0.0937, Disc: 1.2046\n  [Train] Batch 400 - Gen Loss: 9.7280, GAN: 1.1755, L1: 0.0855, Disc: 1.1180\n  [Train] Batch 410 - Gen Loss: 10.9385, GAN: 0.8308, L1: 0.1011, Disc: 0.9742\n  [Val] MAE: 0.2135, RMSE: 0.3518, SSIM: 0.5886\nEpoch 11 completed in 130.89 seconds.\n\nEpoch 12/150\n  [Train] Batch 000 - Gen Loss: 14.0829, GAN: 0.4600, L1: 0.1362, Disc: 1.3714\n  [Train] Batch 010 - Gen Loss: 13.3802, GAN: 1.0462, L1: 0.1233, Disc: 0.9977\n  [Train] Batch 020 - Gen Loss: 12.5283, GAN: 1.2729, L1: 0.1126, Disc: 0.8760\n  [Train] Batch 030 - Gen Loss: 17.3933, GAN: 1.3762, L1: 0.1602, Disc: 0.6558\n  [Train] Batch 040 - Gen Loss: 15.8374, GAN: 1.6001, L1: 0.1424, Disc: 0.9947\n  [Train] Batch 050 - Gen Loss: 13.1731, GAN: 1.0305, L1: 0.1214, Disc: 0.9017\n  [Train] Batch 060 - Gen Loss: 11.9757, GAN: 1.4558, L1: 0.1052, Disc: 1.0019\n  [Train] Batch 070 - Gen Loss: 14.1257, GAN: 0.6038, L1: 0.1352, Disc: 0.9651\n  [Train] Batch 080 - Gen Loss: 11.5510, GAN: 0.6569, L1: 0.1089, Disc: 1.0158\n  [Train] Batch 090 - Gen Loss: 11.9472, GAN: 1.0076, L1: 0.1094, Disc: 1.1681\n  [Train] Batch 100 - Gen Loss: 11.8418, GAN: 1.9969, L1: 0.0984, Disc: 0.9446\n  [Train] Batch 110 - Gen Loss: 14.8032, GAN: 1.2109, L1: 0.1359, Disc: 0.9305\n  [Train] Batch 120 - Gen Loss: 11.5855, GAN: 1.3727, L1: 0.1021, Disc: 0.7922\n  [Train] Batch 130 - Gen Loss: 12.1347, GAN: 1.2334, L1: 0.1090, Disc: 1.1261\n  [Train] Batch 140 - Gen Loss: 13.1318, GAN: 1.1866, L1: 0.1195, Disc: 0.9008\n  [Train] Batch 150 - Gen Loss: 15.2178, GAN: 1.6806, L1: 0.1354, Disc: 0.8406\n  [Train] Batch 160 - Gen Loss: 13.1316, GAN: 2.0689, L1: 0.1106, Disc: 0.8648\n  [Train] Batch 170 - Gen Loss: 13.1810, GAN: 1.3444, L1: 0.1184, Disc: 0.7147\n  [Train] Batch 180 - Gen Loss: 13.3365, GAN: 1.5415, L1: 0.1180, Disc: 0.7674\n  [Train] Batch 190 - Gen Loss: 15.3019, GAN: 1.3712, L1: 0.1393, Disc: 0.5839\n  [Train] Batch 200 - Gen Loss: 13.9994, GAN: 1.4257, L1: 0.1257, Disc: 0.9577\n  [Train] Batch 210 - Gen Loss: 13.3556, GAN: 1.3696, L1: 0.1199, Disc: 0.9016\n  [Train] Batch 220 - Gen Loss: 15.1233, GAN: 0.6415, L1: 0.1448, Disc: 0.9835\n  [Train] Batch 230 - Gen Loss: 16.3329, GAN: 0.9959, L1: 0.1534, Disc: 0.7681\n  [Train] Batch 240 - Gen Loss: 12.5325, GAN: 1.5382, L1: 0.1099, Disc: 0.9566\n  [Train] Batch 250 - Gen Loss: 13.8563, GAN: 0.7756, L1: 0.1308, Disc: 0.8468\n  [Train] Batch 260 - Gen Loss: 14.9044, GAN: 2.2230, L1: 0.1268, Disc: 0.8107\n  [Train] Batch 270 - Gen Loss: 16.1378, GAN: 1.4505, L1: 0.1469, Disc: 0.6598\n  [Train] Batch 280 - Gen Loss: 15.6908, GAN: 1.3798, L1: 0.1431, Disc: 0.6282\n  [Train] Batch 290 - Gen Loss: 13.0334, GAN: 2.1317, L1: 0.1090, Disc: 1.0387\n  [Train] Batch 300 - Gen Loss: 15.3418, GAN: 0.4733, L1: 0.1487, Disc: 1.2773\n  [Train] Batch 310 - Gen Loss: 13.3120, GAN: 2.3380, L1: 0.1097, Disc: 1.4468\n  [Train] Batch 320 - Gen Loss: 12.7615, GAN: 1.7221, L1: 0.1104, Disc: 3.0117\n  [Train] Batch 330 - Gen Loss: 14.2356, GAN: 0.7082, L1: 0.1353, Disc: 1.1295\n  [Train] Batch 340 - Gen Loss: 14.6356, GAN: 0.8291, L1: 0.1381, Disc: 0.8470\n  [Train] Batch 350 - Gen Loss: 12.6967, GAN: 1.7678, L1: 0.1093, Disc: 1.1842\n  [Train] Batch 360 - Gen Loss: 13.5728, GAN: 1.4008, L1: 0.1217, Disc: 0.8530\n  [Train] Batch 370 - Gen Loss: 13.5831, GAN: 1.5426, L1: 0.1204, Disc: 0.7664\n  [Train] Batch 380 - Gen Loss: 12.3182, GAN: 2.3320, L1: 0.0999, Disc: 0.9553\n  [Train] Batch 390 - Gen Loss: 8.5553, GAN: 0.5963, L1: 0.0796, Disc: 1.2090\n  [Train] Batch 400 - Gen Loss: 12.1618, GAN: 1.6774, L1: 0.1048, Disc: 0.8326\n  [Train] Batch 410 - Gen Loss: 11.3064, GAN: 1.2492, L1: 0.1006, Disc: 0.7760\n  [Val] MAE: 0.2110, RMSE: 0.3496, SSIM: 0.5860\nEpoch 12 completed in 130.86 seconds.\n\nEpoch 13/150\n  [Train] Batch 000 - Gen Loss: 14.7228, GAN: 2.4855, L1: 0.1224, Disc: 1.8148\n  [Train] Batch 010 - Gen Loss: 12.0677, GAN: 0.9259, L1: 0.1114, Disc: 0.9231\n  [Train] Batch 020 - Gen Loss: 12.0417, GAN: 0.8024, L1: 0.1124, Disc: 0.9335\n  [Train] Batch 030 - Gen Loss: 13.4944, GAN: 1.4684, L1: 0.1203, Disc: 0.8221\n  [Train] Batch 040 - Gen Loss: 14.5092, GAN: 1.7825, L1: 0.1273, Disc: 1.2204\n  [Train] Batch 050 - Gen Loss: 12.5524, GAN: 1.1795, L1: 0.1137, Disc: 0.8789\n  [Train] Batch 060 - Gen Loss: 12.2604, GAN: 0.9459, L1: 0.1131, Disc: 0.8058\n  [Train] Batch 070 - Gen Loss: 12.9359, GAN: 1.6803, L1: 0.1126, Disc: 0.7440\n  [Train] Batch 080 - Gen Loss: 11.8896, GAN: 0.8897, L1: 0.1100, Disc: 0.9792\n  [Train] Batch 090 - Gen Loss: 10.2590, GAN: 0.7075, L1: 0.0955, Disc: 1.1989\n  [Train] Batch 100 - Gen Loss: 11.6620, GAN: 0.6530, L1: 0.1101, Disc: 1.0194\n  [Train] Batch 110 - Gen Loss: 14.3232, GAN: 2.2428, L1: 0.1208, Disc: 0.9829\n  [Train] Batch 120 - Gen Loss: 13.2923, GAN: 1.4817, L1: 0.1181, Disc: 0.6925\n  [Train] Batch 130 - Gen Loss: 10.6953, GAN: 1.6065, L1: 0.0909, Disc: 0.7855\n  [Train] Batch 140 - Gen Loss: 11.1191, GAN: 1.4653, L1: 0.0965, Disc: 0.8626\n  [Train] Batch 150 - Gen Loss: 13.1275, GAN: 1.1485, L1: 0.1198, Disc: 0.8568\n  [Train] Batch 160 - Gen Loss: 13.2181, GAN: 1.9014, L1: 0.1132, Disc: 0.7751\n  [Train] Batch 170 - Gen Loss: 12.9858, GAN: 1.0893, L1: 0.1190, Disc: 0.8258\n  [Train] Batch 180 - Gen Loss: 11.4942, GAN: 1.5180, L1: 0.0998, Disc: 0.7020\n  [Train] Batch 190 - Gen Loss: 10.9379, GAN: 0.7324, L1: 0.1021, Disc: 1.0979\n  [Train] Batch 200 - Gen Loss: 10.9145, GAN: 1.3252, L1: 0.0959, Disc: 0.7496\n  [Train] Batch 210 - Gen Loss: 14.1078, GAN: 1.7296, L1: 0.1238, Disc: 0.8661\n  [Train] Batch 220 - Gen Loss: 14.3531, GAN: 1.9883, L1: 0.1236, Disc: 0.8319\n  [Train] Batch 230 - Gen Loss: 11.7561, GAN: 1.0406, L1: 0.1072, Disc: 0.9061\n  [Train] Batch 240 - Gen Loss: 13.0905, GAN: 0.9388, L1: 0.1215, Disc: 0.8172\n  [Train] Batch 250 - Gen Loss: 14.2502, GAN: 1.0991, L1: 0.1315, Disc: 0.9583\n  [Train] Batch 260 - Gen Loss: 15.6145, GAN: 2.0289, L1: 0.1359, Disc: 1.0622\n  [Train] Batch 270 - Gen Loss: 15.3163, GAN: 0.9894, L1: 0.1433, Disc: 0.8415\n  [Train] Batch 280 - Gen Loss: 14.2633, GAN: 0.7135, L1: 0.1355, Disc: 1.0931\n  [Train] Batch 290 - Gen Loss: 12.5127, GAN: 1.0073, L1: 0.1151, Disc: 2.7708\n  [Train] Batch 300 - Gen Loss: 13.5757, GAN: 1.6288, L1: 0.1195, Disc: 1.1137\n  [Train] Batch 310 - Gen Loss: 13.0335, GAN: 0.6836, L1: 0.1235, Disc: 1.2655\n  [Train] Batch 320 - Gen Loss: 12.1804, GAN: 1.9996, L1: 0.1018, Disc: 1.6129\n  [Train] Batch 330 - Gen Loss: 12.9927, GAN: 1.3386, L1: 0.1165, Disc: 0.7291\n  [Train] Batch 340 - Gen Loss: 13.3523, GAN: 1.4588, L1: 0.1189, Disc: 0.7977\n  [Train] Batch 350 - Gen Loss: 11.8216, GAN: 0.8135, L1: 0.1101, Disc: 0.8958\n  [Train] Batch 360 - Gen Loss: 13.2750, GAN: 1.2718, L1: 0.1200, Disc: 0.7822\n  [Train] Batch 370 - Gen Loss: 12.6205, GAN: 1.7168, L1: 0.1090, Disc: 0.9607\n  [Train] Batch 380 - Gen Loss: 10.0811, GAN: 1.5068, L1: 0.0857, Disc: 1.1406\n  [Train] Batch 390 - Gen Loss: 11.6854, GAN: 0.6317, L1: 0.1105, Disc: 1.1234\n  [Train] Batch 400 - Gen Loss: 10.1449, GAN: 1.4635, L1: 0.0868, Disc: 1.0467\n  [Train] Batch 410 - Gen Loss: 11.8642, GAN: 1.0293, L1: 0.1083, Disc: 0.8986\n  [Val] MAE: 0.2039, RMSE: 0.3311, SSIM: 0.5890\nEpoch 13 completed in 130.96 seconds.\n\nEpoch 14/150\n  [Train] Batch 000 - Gen Loss: 14.9703, GAN: 0.5995, L1: 0.1437, Disc: 1.1026\n  [Train] Batch 010 - Gen Loss: 16.9660, GAN: 0.8872, L1: 0.1608, Disc: 0.8737\n  [Train] Batch 020 - Gen Loss: 13.6740, GAN: 0.7821, L1: 0.1289, Disc: 0.8024\n  [Train] Batch 030 - Gen Loss: 14.2579, GAN: 1.4307, L1: 0.1283, Disc: 0.9385\n  [Train] Batch 040 - Gen Loss: 13.3282, GAN: 1.8548, L1: 0.1147, Disc: 1.6064\n  [Train] Batch 050 - Gen Loss: 11.1716, GAN: 1.1212, L1: 0.1005, Disc: 0.8778\n  [Train] Batch 060 - Gen Loss: 13.6647, GAN: 1.0771, L1: 0.1259, Disc: 0.6842\n  [Train] Batch 070 - Gen Loss: 13.0265, GAN: 1.2710, L1: 0.1176, Disc: 0.8095\n  [Train] Batch 080 - Gen Loss: 10.2972, GAN: 1.2528, L1: 0.0904, Disc: 1.3091\n  [Train] Batch 090 - Gen Loss: 9.8386, GAN: 0.6782, L1: 0.0916, Disc: 1.2047\n  [Train] Batch 100 - Gen Loss: 9.9463, GAN: 0.8420, L1: 0.0910, Disc: 0.9802\n  [Train] Batch 110 - Gen Loss: 11.4412, GAN: 1.9574, L1: 0.0948, Disc: 1.1151\n  [Train] Batch 120 - Gen Loss: 14.1353, GAN: 1.3807, L1: 0.1275, Disc: 0.8656\n  [Train] Batch 130 - Gen Loss: 11.5791, GAN: 1.3840, L1: 0.1020, Disc: 1.0309\n  [Train] Batch 140 - Gen Loss: 13.1051, GAN: 1.9865, L1: 0.1112, Disc: 0.9594\n  [Train] Batch 150 - Gen Loss: 10.9174, GAN: 0.8130, L1: 0.1010, Disc: 0.9997\n  [Train] Batch 160 - Gen Loss: 12.3040, GAN: 2.2283, L1: 0.1008, Disc: 1.0897\n  [Train] Batch 170 - Gen Loss: 13.6983, GAN: 2.3609, L1: 0.1134, Disc: 0.8611\n  [Train] Batch 180 - Gen Loss: 13.8945, GAN: 0.5369, L1: 0.1336, Disc: 1.1261\n  [Train] Batch 190 - Gen Loss: 12.5876, GAN: 0.8978, L1: 0.1169, Disc: 0.8118\n  [Train] Batch 200 - Gen Loss: 13.2107, GAN: 1.2203, L1: 0.1199, Disc: 0.6996\n  [Train] Batch 210 - Gen Loss: 12.9711, GAN: 1.8232, L1: 0.1115, Disc: 0.8606\n  [Train] Batch 220 - Gen Loss: 14.6285, GAN: 1.9973, L1: 0.1263, Disc: 0.9712\n  [Train] Batch 230 - Gen Loss: 14.3418, GAN: 1.7893, L1: 0.1255, Disc: 0.7893\n  [Train] Batch 240 - Gen Loss: 13.7570, GAN: 0.9659, L1: 0.1279, Disc: 0.8899\n  [Train] Batch 250 - Gen Loss: 11.8341, GAN: 0.6255, L1: 0.1121, Disc: 1.0785\n  [Train] Batch 260 - Gen Loss: 14.4787, GAN: 1.4465, L1: 0.1303, Disc: 0.6812\n  [Train] Batch 270 - Gen Loss: 11.6300, GAN: 1.1757, L1: 0.1045, Disc: 0.8675\n  [Train] Batch 280 - Gen Loss: 14.7013, GAN: 0.9088, L1: 0.1379, Disc: 0.8161\n  [Train] Batch 290 - Gen Loss: 11.8099, GAN: 0.4218, L1: 0.1139, Disc: 1.3872\n  [Train] Batch 300 - Gen Loss: 13.8835, GAN: 0.8354, L1: 0.1305, Disc: 0.7817\n  [Train] Batch 310 - Gen Loss: 13.5250, GAN: 1.1732, L1: 0.1235, Disc: 0.8554\n  [Train] Batch 320 - Gen Loss: 12.4372, GAN: 1.8361, L1: 0.1060, Disc: 1.0742\n  [Train] Batch 330 - Gen Loss: 12.0651, GAN: 1.7907, L1: 0.1027, Disc: 2.0585\n  [Train] Batch 340 - Gen Loss: 10.7409, GAN: 1.0256, L1: 0.0972, Disc: 1.9958\n  [Train] Batch 350 - Gen Loss: 14.2557, GAN: 1.0613, L1: 0.1319, Disc: 0.7948\n  [Train] Batch 360 - Gen Loss: 12.0683, GAN: 1.4046, L1: 0.1066, Disc: 0.7384\n  [Train] Batch 370 - Gen Loss: 12.6753, GAN: 0.3920, L1: 0.1228, Disc: 1.5946\n  [Train] Batch 380 - Gen Loss: 12.1941, GAN: 1.0373, L1: 0.1116, Disc: 1.0655\n  [Train] Batch 390 - Gen Loss: 8.6335, GAN: 0.5897, L1: 0.0804, Disc: 1.2171\n  [Train] Batch 400 - Gen Loss: 11.0570, GAN: 1.0611, L1: 0.1000, Disc: 0.7898\n  [Train] Batch 410 - Gen Loss: 9.4874, GAN: 1.4142, L1: 0.0807, Disc: 1.0703\n  [Val] MAE: 0.2005, RMSE: 0.3309, SSIM: 0.5908\nEpoch 14 completed in 130.96 seconds.\n\nEpoch 15/150\n  [Train] Batch 000 - Gen Loss: 15.6556, GAN: 1.1766, L1: 0.1448, Disc: 0.7738\n  [Train] Batch 010 - Gen Loss: 13.8097, GAN: 0.6476, L1: 0.1316, Disc: 1.0446\n  [Train] Batch 020 - Gen Loss: 14.5077, GAN: 1.1844, L1: 0.1332, Disc: 0.9824\n  [Train] Batch 030 - Gen Loss: 11.8198, GAN: 0.8422, L1: 0.1098, Disc: 0.9131\n  [Train] Batch 040 - Gen Loss: 14.4225, GAN: 1.9466, L1: 0.1248, Disc: 0.6351\n  [Train] Batch 050 - Gen Loss: 18.3747, GAN: 1.2474, L1: 0.1713, Disc: 0.6478\n  [Train] Batch 060 - Gen Loss: 11.7846, GAN: 1.2024, L1: 0.1058, Disc: 0.8001\n  [Train] Batch 070 - Gen Loss: 11.6764, GAN: 0.2419, L1: 0.1143, Disc: 1.8524\n  [Train] Batch 080 - Gen Loss: 9.9944, GAN: 0.2664, L1: 0.0973, Disc: 1.6870\n  [Train] Batch 090 - Gen Loss: 7.5857, GAN: 0.1719, L1: 0.0741, Disc: 2.1105\n  [Train] Batch 100 - Gen Loss: 10.9499, GAN: 1.8698, L1: 0.0908, Disc: 0.8264\n  [Train] Batch 110 - Gen Loss: 14.5577, GAN: 1.9003, L1: 0.1266, Disc: 0.9866\n  [Train] Batch 120 - Gen Loss: 10.3754, GAN: 1.2486, L1: 0.0913, Disc: 1.1394\n  [Train] Batch 130 - Gen Loss: 12.4735, GAN: 3.3311, L1: 0.0914, Disc: 1.4663\n  [Train] Batch 140 - Gen Loss: 12.0292, GAN: 1.5402, L1: 0.1049, Disc: 0.7770\n  [Train] Batch 150 - Gen Loss: 12.6865, GAN: 1.1746, L1: 0.1151, Disc: 0.6776\n  [Train] Batch 160 - Gen Loss: 14.5280, GAN: 1.8137, L1: 0.1271, Disc: 0.6401\n  [Train] Batch 170 - Gen Loss: 14.0080, GAN: 0.9356, L1: 0.1307, Disc: 0.8569\n  [Train] Batch 180 - Gen Loss: 12.4806, GAN: 1.0991, L1: 0.1138, Disc: 0.8584\n  [Train] Batch 190 - Gen Loss: 11.6516, GAN: 0.8581, L1: 0.1079, Disc: 0.7876\n  [Train] Batch 200 - Gen Loss: 12.8627, GAN: 1.2646, L1: 0.1160, Disc: 0.6625\n  [Train] Batch 210 - Gen Loss: 11.9207, GAN: 0.7222, L1: 0.1120, Disc: 0.9430\n  [Train] Batch 220 - Gen Loss: 13.4961, GAN: 2.4831, L1: 0.1101, Disc: 0.8770\n  [Train] Batch 230 - Gen Loss: 13.2073, GAN: 1.9208, L1: 0.1129, Disc: 0.8219\n  [Train] Batch 240 - Gen Loss: 10.5107, GAN: 1.1131, L1: 0.0940, Disc: 0.8558\n  [Train] Batch 250 - Gen Loss: 13.4415, GAN: 2.4695, L1: 0.1097, Disc: 1.0300\n  [Train] Batch 260 - Gen Loss: 13.9318, GAN: 1.1441, L1: 0.1279, Disc: 0.7993\n  [Train] Batch 270 - Gen Loss: 12.7606, GAN: 1.2854, L1: 0.1148, Disc: 0.7286\n  [Train] Batch 280 - Gen Loss: 14.0055, GAN: 0.9578, L1: 0.1305, Disc: 0.7155\n  [Train] Batch 290 - Gen Loss: 11.2570, GAN: 0.8768, L1: 0.1038, Disc: 0.9153\n  [Train] Batch 300 - Gen Loss: 13.6867, GAN: 1.5874, L1: 0.1210, Disc: 0.6413\n  [Train] Batch 310 - Gen Loss: 11.0457, GAN: 1.0917, L1: 0.0995, Disc: 0.9537\n  [Train] Batch 320 - Gen Loss: 13.2583, GAN: 1.8819, L1: 0.1138, Disc: 0.6681\n  [Train] Batch 330 - Gen Loss: 14.2904, GAN: 0.8694, L1: 0.1342, Disc: 1.0576\n  [Train] Batch 340 - Gen Loss: 13.5173, GAN: 2.5715, L1: 0.1095, Disc: 3.0028\n  [Train] Batch 350 - Gen Loss: 12.8263, GAN: 1.4743, L1: 0.1135, Disc: 0.7260\n  [Train] Batch 360 - Gen Loss: 14.1861, GAN: 1.3425, L1: 0.1284, Disc: 0.6380\n  [Train] Batch 370 - Gen Loss: 10.2856, GAN: 0.6318, L1: 0.0965, Disc: 1.6039\n  [Train] Batch 380 - Gen Loss: 11.0632, GAN: 0.3572, L1: 0.1071, Disc: 1.4283\n  [Train] Batch 390 - Gen Loss: 8.5978, GAN: 0.4161, L1: 0.0818, Disc: 1.3957\n  [Train] Batch 400 - Gen Loss: 10.9691, GAN: 1.7395, L1: 0.0923, Disc: 1.0482\n  [Train] Batch 410 - Gen Loss: 10.3369, GAN: 0.6005, L1: 0.0974, Disc: 1.1292\n  [Val] MAE: 0.2054, RMSE: 0.3472, SSIM: 0.5889\nEpoch 15 completed in 131.01 seconds.\n\nEpoch 16/150\n  [Train] Batch 000 - Gen Loss: 11.2957, GAN: 1.0526, L1: 0.1024, Disc: 0.8492\n  [Train] Batch 010 - Gen Loss: 11.1652, GAN: 0.9990, L1: 0.1017, Disc: 0.8775\n  [Train] Batch 020 - Gen Loss: 12.9604, GAN: 1.5005, L1: 0.1146, Disc: 0.9130\n  [Train] Batch 030 - Gen Loss: 13.4408, GAN: 0.9463, L1: 0.1249, Disc: 0.8371\n  [Train] Batch 040 - Gen Loss: 10.8403, GAN: 0.7693, L1: 0.1007, Disc: 0.9141\n  [Train] Batch 050 - Gen Loss: 11.9331, GAN: 2.0035, L1: 0.0993, Disc: 1.1221\n  [Train] Batch 060 - Gen Loss: 11.3841, GAN: 1.0209, L1: 0.1036, Disc: 0.7572\n  [Train] Batch 070 - Gen Loss: 11.0198, GAN: 0.9851, L1: 0.1003, Disc: 0.8164\n  [Train] Batch 080 - Gen Loss: 9.4161, GAN: 0.1952, L1: 0.0922, Disc: 2.0157\n  [Train] Batch 090 - Gen Loss: 10.2030, GAN: 0.7438, L1: 0.0946, Disc: 1.1037\n  [Train] Batch 100 - Gen Loss: 9.7664, GAN: 0.8589, L1: 0.0891, Disc: 0.8392\n  [Train] Batch 110 - Gen Loss: 11.8990, GAN: 2.0305, L1: 0.0987, Disc: 0.9334\n  [Train] Batch 120 - Gen Loss: 11.9077, GAN: 1.6971, L1: 0.1021, Disc: 1.1438\n  [Train] Batch 130 - Gen Loss: 10.5550, GAN: 2.3820, L1: 0.0817, Disc: 1.5382\n  [Train] Batch 140 - Gen Loss: 11.5479, GAN: 1.1038, L1: 0.1044, Disc: 0.7726\n  [Train] Batch 150 - Gen Loss: 14.4104, GAN: 1.9872, L1: 0.1242, Disc: 0.5565\n  [Train] Batch 160 - Gen Loss: 12.1254, GAN: 2.2360, L1: 0.0989, Disc: 0.5595\n  [Train] Batch 170 - Gen Loss: 12.2409, GAN: 1.6925, L1: 0.1055, Disc: 0.6016\n  [Train] Batch 180 - Gen Loss: 11.1955, GAN: 1.0973, L1: 0.1010, Disc: 0.7215\n  [Train] Batch 190 - Gen Loss: 13.2472, GAN: 1.1950, L1: 0.1205, Disc: 0.5855\n  [Train] Batch 200 - Gen Loss: 11.0067, GAN: 0.9088, L1: 0.1010, Disc: 0.7897\n  [Train] Batch 210 - Gen Loss: 11.6204, GAN: 0.7241, L1: 0.1090, Disc: 0.8435\n  [Train] Batch 220 - Gen Loss: 10.9286, GAN: 1.1242, L1: 0.0980, Disc: 0.5736\n  [Train] Batch 230 - Gen Loss: 13.6257, GAN: 2.3769, L1: 0.1125, Disc: 0.8512\n  [Train] Batch 240 - Gen Loss: 12.8432, GAN: 1.9624, L1: 0.1088, Disc: 0.8460\n  [Train] Batch 250 - Gen Loss: 11.1568, GAN: 1.1430, L1: 0.1001, Disc: 0.7469\n  [Train] Batch 260 - Gen Loss: 15.8794, GAN: 2.7443, L1: 0.1314, Disc: 1.0549\n  [Train] Batch 270 - Gen Loss: 11.8896, GAN: 0.8917, L1: 0.1100, Disc: 0.7771\n  [Train] Batch 280 - Gen Loss: 13.0105, GAN: 1.9419, L1: 0.1107, Disc: 0.7597\n  [Train] Batch 290 - Gen Loss: 12.8217, GAN: 1.7030, L1: 0.1112, Disc: 2.8785\n  [Train] Batch 300 - Gen Loss: 11.1038, GAN: 1.8852, L1: 0.0922, Disc: 1.1810\n  [Train] Batch 310 - Gen Loss: 11.7682, GAN: 1.4126, L1: 0.1036, Disc: 0.9901\n  [Train] Batch 320 - Gen Loss: 11.2498, GAN: 1.6493, L1: 0.0960, Disc: 0.7889\n  [Train] Batch 330 - Gen Loss: 10.8128, GAN: 0.7079, L1: 0.1010, Disc: 0.9403\n  [Train] Batch 340 - Gen Loss: 12.8723, GAN: 1.3572, L1: 0.1152, Disc: 0.6086\n  [Train] Batch 350 - Gen Loss: 10.6982, GAN: 0.3746, L1: 0.1032, Disc: 1.4718\n  [Train] Batch 360 - Gen Loss: 13.1644, GAN: 1.4644, L1: 0.1170, Disc: 0.8127\n  [Train] Batch 370 - Gen Loss: 12.7825, GAN: 2.3421, L1: 0.1044, Disc: 0.6225\n  [Train] Batch 380 - Gen Loss: 12.5037, GAN: 1.8092, L1: 0.1069, Disc: 0.5902\n  [Train] Batch 390 - Gen Loss: 9.1526, GAN: 0.7150, L1: 0.0844, Disc: 0.9310\n  [Train] Batch 400 - Gen Loss: 10.7312, GAN: 1.9334, L1: 0.0880, Disc: 0.8589\n  [Train] Batch 410 - Gen Loss: 8.9505, GAN: 0.9920, L1: 0.0796, Disc: 1.0934\n  [Val] MAE: 0.2033, RMSE: 0.3437, SSIM: 0.6024\nEpoch 16 completed in 131.16 seconds.\n\nEpoch 17/150\n  [Train] Batch 000 - Gen Loss: 11.1335, GAN: 1.5512, L1: 0.0958, Disc: 0.7952\n  [Train] Batch 010 - Gen Loss: 14.1451, GAN: 1.2217, L1: 0.1292, Disc: 0.5806\n  [Train] Batch 020 - Gen Loss: 9.6114, GAN: 1.3231, L1: 0.0829, Disc: 0.6289\n  [Train] Batch 030 - Gen Loss: 11.2038, GAN: 1.1243, L1: 0.1008, Disc: 0.7622\n  [Train] Batch 040 - Gen Loss: 11.4787, GAN: 1.9562, L1: 0.0952, Disc: 1.0920\n  [Train] Batch 050 - Gen Loss: 11.4819, GAN: 0.9494, L1: 0.1053, Disc: 0.8017\n  [Train] Batch 060 - Gen Loss: 12.2421, GAN: 1.0899, L1: 0.1115, Disc: 0.7171\n  [Train] Batch 070 - Gen Loss: 11.7362, GAN: 1.7428, L1: 0.0999, Disc: 0.5602\n  [Train] Batch 080 - Gen Loss: 11.9324, GAN: 1.5620, L1: 0.1037, Disc: 0.5557\n  [Train] Batch 090 - Gen Loss: 10.0770, GAN: 1.1049, L1: 0.0897, Disc: 0.9730\n  [Train] Batch 100 - Gen Loss: 8.7932, GAN: 1.0488, L1: 0.0774, Disc: 0.9015\n  [Train] Batch 110 - Gen Loss: 12.5790, GAN: 0.7485, L1: 0.1183, Disc: 1.1806\n  [Train] Batch 120 - Gen Loss: 11.7289, GAN: 1.2262, L1: 0.1050, Disc: 0.6263\n  [Train] Batch 130 - Gen Loss: 11.0806, GAN: 1.1680, L1: 0.0991, Disc: 0.9348\n  [Train] Batch 140 - Gen Loss: 11.6559, GAN: 0.8152, L1: 0.1084, Disc: 0.9816\n  [Train] Batch 150 - Gen Loss: 12.3592, GAN: 2.1155, L1: 0.1024, Disc: 1.0611\n  [Train] Batch 160 - Gen Loss: 13.0523, GAN: 0.8048, L1: 0.1225, Disc: 0.9393\n  [Train] Batch 170 - Gen Loss: 11.3756, GAN: 1.9473, L1: 0.0943, Disc: 0.3982\n  [Train] Batch 180 - Gen Loss: 10.1064, GAN: 1.4130, L1: 0.0869, Disc: 0.6385\n  [Train] Batch 190 - Gen Loss: 12.6622, GAN: 2.0791, L1: 0.1058, Disc: 0.5046\n  [Train] Batch 200 - Gen Loss: 11.0426, GAN: 1.3840, L1: 0.0966, Disc: 0.7236\n  [Train] Batch 210 - Gen Loss: 12.6819, GAN: 1.9563, L1: 0.1073, Disc: 0.3849\n  [Train] Batch 220 - Gen Loss: 11.2578, GAN: 1.2411, L1: 0.1002, Disc: 0.7625\n  [Train] Batch 230 - Gen Loss: 14.3754, GAN: 1.4360, L1: 0.1294, Disc: 0.5658\n  [Train] Batch 240 - Gen Loss: 12.8130, GAN: 2.0804, L1: 0.1073, Disc: 0.7481\n  [Train] Batch 250 - Gen Loss: 12.3028, GAN: 2.2290, L1: 0.1007, Disc: 0.4065\n  [Train] Batch 260 - Gen Loss: 11.8392, GAN: 0.9140, L1: 0.1093, Disc: 0.7778\n  [Train] Batch 270 - Gen Loss: 12.3516, GAN: 1.0189, L1: 0.1133, Disc: 0.6504\n  [Train] Batch 280 - Gen Loss: 13.4085, GAN: 2.3792, L1: 0.1103, Disc: 0.3290\n  [Train] Batch 290 - Gen Loss: 10.6500, GAN: 0.8115, L1: 0.0984, Disc: 0.8235\n  [Train] Batch 300 - Gen Loss: 12.6074, GAN: 1.4962, L1: 0.1111, Disc: 0.7384\n  [Train] Batch 310 - Gen Loss: 12.0693, GAN: 2.8232, L1: 0.0925, Disc: 1.8957\n  [Train] Batch 320 - Gen Loss: 11.6478, GAN: 1.7168, L1: 0.0993, Disc: 7.0459\n  [Train] Batch 330 - Gen Loss: 13.6092, GAN: 2.0894, L1: 0.1152, Disc: 1.8099\n  [Train] Batch 340 - Gen Loss: 11.3401, GAN: 0.9695, L1: 0.1037, Disc: 0.9813\n  [Train] Batch 350 - Gen Loss: 11.9257, GAN: 0.7812, L1: 0.1114, Disc: 0.9269\n  [Train] Batch 360 - Gen Loss: 12.9794, GAN: 1.8979, L1: 0.1108, Disc: 1.1184\n  [Train] Batch 370 - Gen Loss: 11.8295, GAN: 1.6405, L1: 0.1019, Disc: 0.7089\n  [Train] Batch 380 - Gen Loss: 10.1787, GAN: 0.4828, L1: 0.0970, Disc: 1.4377\n  [Train] Batch 390 - Gen Loss: 10.7105, GAN: 1.5936, L1: 0.0912, Disc: 0.9297\n  [Train] Batch 400 - Gen Loss: 9.7510, GAN: 1.6197, L1: 0.0813, Disc: 0.7421\n  [Train] Batch 410 - Gen Loss: 9.4992, GAN: 2.0697, L1: 0.0743, Disc: 0.7875\n  [Val] MAE: 0.1988, RMSE: 0.3345, SSIM: 0.6083\nEpoch 17 completed in 131.08 seconds.\n\nEpoch 18/150\n  [Train] Batch 000 - Gen Loss: 15.1983, GAN: 1.8160, L1: 0.1338, Disc: 1.0660\n  [Train] Batch 010 - Gen Loss: 12.8628, GAN: 2.7581, L1: 0.1010, Disc: 0.6915\n  [Train] Batch 020 - Gen Loss: 10.8468, GAN: 1.4721, L1: 0.0937, Disc: 0.8326\n  [Train] Batch 030 - Gen Loss: 11.7288, GAN: 1.3861, L1: 0.1034, Disc: 0.6802\n  [Train] Batch 040 - Gen Loss: 12.2834, GAN: 0.8271, L1: 0.1146, Disc: 0.9886\n  [Train] Batch 050 - Gen Loss: 11.9077, GAN: 1.8540, L1: 0.1005, Disc: 0.7625\n  [Train] Batch 060 - Gen Loss: 15.9042, GAN: 1.7981, L1: 0.1411, Disc: 0.5241\n  [Train] Batch 070 - Gen Loss: 12.3396, GAN: 1.7794, L1: 0.1056, Disc: 0.7434\n  [Train] Batch 080 - Gen Loss: 8.3437, GAN: 0.6857, L1: 0.0766, Disc: 1.2528\n  [Train] Batch 090 - Gen Loss: 8.7026, GAN: 0.9082, L1: 0.0779, Disc: 1.0368\n  [Train] Batch 100 - Gen Loss: 9.9214, GAN: 1.5232, L1: 0.0840, Disc: 0.8725\n  [Train] Batch 110 - Gen Loss: 9.2851, GAN: 1.1053, L1: 0.0818, Disc: 0.9528\n  [Train] Batch 120 - Gen Loss: 10.4351, GAN: 1.7585, L1: 0.0868, Disc: 1.2743\n  [Train] Batch 130 - Gen Loss: 8.9613, GAN: 0.3027, L1: 0.0866, Disc: 1.6518\n  [Train] Batch 140 - Gen Loss: 11.2793, GAN: 1.2849, L1: 0.0999, Disc: 0.5802\n  [Train] Batch 150 - Gen Loss: 10.8761, GAN: 2.1239, L1: 0.0875, Disc: 0.9480\n  [Train] Batch 160 - Gen Loss: 12.1419, GAN: 2.2315, L1: 0.0991, Disc: 0.5617\n  [Train] Batch 170 - Gen Loss: 12.3799, GAN: 2.7658, L1: 0.0961, Disc: 0.5818\n  [Train] Batch 180 - Gen Loss: 12.1046, GAN: 0.9110, L1: 0.1119, Disc: 0.7114\n  [Train] Batch 190 - Gen Loss: 11.7061, GAN: 1.9826, L1: 0.0972, Disc: 0.8672\n  [Train] Batch 200 - Gen Loss: 11.1384, GAN: 1.2505, L1: 0.0989, Disc: 0.8331\n  [Train] Batch 210 - Gen Loss: 10.3504, GAN: 0.6089, L1: 0.0974, Disc: 1.1370\n  [Train] Batch 220 - Gen Loss: 12.1808, GAN: 0.9570, L1: 0.1122, Disc: 0.6731\n  [Train] Batch 230 - Gen Loss: 12.8808, GAN: 1.0155, L1: 0.1187, Disc: 0.6249\n  [Train] Batch 240 - Gen Loss: 10.4843, GAN: 1.1103, L1: 0.0937, Disc: 0.7608\n  [Train] Batch 250 - Gen Loss: 10.4790, GAN: 0.6729, L1: 0.0981, Disc: 0.9511\n  [Train] Batch 260 - Gen Loss: 10.5611, GAN: 0.7221, L1: 0.0984, Disc: 1.0950\n  [Train] Batch 270 - Gen Loss: 12.8614, GAN: 1.9301, L1: 0.1093, Disc: 0.3939\n  [Train] Batch 280 - Gen Loss: 13.7796, GAN: 3.2767, L1: 0.1050, Disc: 0.8347\n  [Train] Batch 290 - Gen Loss: 10.6397, GAN: 1.5992, L1: 0.0904, Disc: 0.7707\n  [Train] Batch 300 - Gen Loss: 11.6388, GAN: 1.8940, L1: 0.0974, Disc: 0.6319\n  [Train] Batch 310 - Gen Loss: 11.2079, GAN: 2.1354, L1: 0.0907, Disc: 0.8595\n  [Train] Batch 320 - Gen Loss: 11.1505, GAN: 1.4172, L1: 0.0973, Disc: 0.9119\n  [Train] Batch 330 - Gen Loss: 10.3494, GAN: 0.9634, L1: 0.0939, Disc: 0.9386\n  [Train] Batch 340 - Gen Loss: 13.2382, GAN: 2.7509, L1: 0.1049, Disc: 0.8274\n  [Train] Batch 350 - Gen Loss: 10.6370, GAN: 1.2156, L1: 0.0942, Disc: 0.7087\n  [Train] Batch 360 - Gen Loss: 12.8084, GAN: 1.7848, L1: 0.1102, Disc: 0.5424\n  [Train] Batch 370 - Gen Loss: 11.1876, GAN: 2.4074, L1: 0.0878, Disc: 1.1808\n  [Train] Batch 380 - Gen Loss: 9.6287, GAN: 0.2939, L1: 0.0933, Disc: 1.8185\n  [Train] Batch 390 - Gen Loss: 10.1111, GAN: 1.7746, L1: 0.0834, Disc: 1.0772\n  [Train] Batch 400 - Gen Loss: 8.3032, GAN: 0.8552, L1: 0.0745, Disc: 0.9529\n  [Train] Batch 410 - Gen Loss: 10.0548, GAN: 1.8233, L1: 0.0823, Disc: 0.5457\n  [Val] MAE: 0.2003, RMSE: 0.3387, SSIM: 0.6039\nEpoch 18 completed in 131.08 seconds.\n\nEpoch 19/150\n  [Train] Batch 000 - Gen Loss: 11.8917, GAN: 1.3497, L1: 0.1054, Disc: 0.7368\n  [Train] Batch 010 - Gen Loss: 14.9605, GAN: 3.4855, L1: 0.1148, Disc: 0.4578\n  [Train] Batch 020 - Gen Loss: 10.6394, GAN: 0.9130, L1: 0.0973, Disc: 0.8538\n  [Train] Batch 030 - Gen Loss: 12.8246, GAN: 1.1931, L1: 0.1163, Disc: 0.7221\n  [Train] Batch 040 - Gen Loss: 13.0804, GAN: 3.7409, L1: 0.0934, Disc: 0.6958\n  [Train] Batch 050 - Gen Loss: 10.8287, GAN: 1.0685, L1: 0.0976, Disc: 0.5899\n  [Train] Batch 060 - Gen Loss: 12.7811, GAN: 3.1877, L1: 0.0959, Disc: 0.7636\n  [Train] Batch 070 - Gen Loss: 10.0478, GAN: 0.7301, L1: 0.0932, Disc: 0.9204\n  [Train] Batch 080 - Gen Loss: 11.9492, GAN: 2.7768, L1: 0.0917, Disc: 1.1363\n  [Train] Batch 090 - Gen Loss: 8.2521, GAN: 1.7674, L1: 0.0648, Disc: 1.2589\n  [Train] Batch 100 - Gen Loss: 7.9235, GAN: 1.6707, L1: 0.0625, Disc: 1.2355\n  [Train] Batch 110 - Gen Loss: 10.7410, GAN: 1.9272, L1: 0.0881, Disc: 0.8390\n  [Train] Batch 120 - Gen Loss: 13.6862, GAN: 1.9148, L1: 0.1177, Disc: 0.4192\n  [Train] Batch 130 - Gen Loss: 9.4326, GAN: 1.1693, L1: 0.0826, Disc: 0.8659\n  [Train] Batch 140 - Gen Loss: 10.5584, GAN: 2.0041, L1: 0.0855, Disc: 0.4922\n  [Train] Batch 150 - Gen Loss: 11.6930, GAN: 1.3198, L1: 0.1037, Disc: 0.5504\n  [Train] Batch 160 - Gen Loss: 11.6046, GAN: 1.5069, L1: 0.1010, Disc: 0.6210\n  [Train] Batch 170 - Gen Loss: 13.1349, GAN: 2.6665, L1: 0.1047, Disc: 0.4213\n  [Train] Batch 180 - Gen Loss: 11.9550, GAN: 2.5866, L1: 0.0937, Disc: 1.2062\n  [Train] Batch 190 - Gen Loss: 12.8562, GAN: 2.6399, L1: 0.1022, Disc: 0.7970\n  [Train] Batch 200 - Gen Loss: 11.1048, GAN: 1.4922, L1: 0.0961, Disc: 0.6491\n  [Train] Batch 210 - Gen Loss: 10.0870, GAN: 1.3357, L1: 0.0875, Disc: 1.5101\n  [Train] Batch 220 - Gen Loss: 13.4850, GAN: 2.3362, L1: 0.1115, Disc: 0.5599\n  [Train] Batch 230 - Gen Loss: 11.8396, GAN: 1.5390, L1: 0.1030, Disc: 0.9463\n  [Train] Batch 240 - Gen Loss: 10.3774, GAN: 0.6955, L1: 0.0968, Disc: 1.0020\n  [Train] Batch 250 - Gen Loss: 11.5070, GAN: 1.5898, L1: 0.0992, Disc: 0.5714\n  [Train] Batch 260 - Gen Loss: 12.4361, GAN: 2.2869, L1: 0.1015, Disc: 0.5882\n  [Train] Batch 270 - Gen Loss: 12.2838, GAN: 1.1289, L1: 0.1115, Disc: 0.5652\n  [Train] Batch 280 - Gen Loss: 13.8581, GAN: 1.4552, L1: 0.1240, Disc: 0.5856\n  [Train] Batch 290 - Gen Loss: 11.5828, GAN: 0.5817, L1: 0.1100, Disc: 1.0959\n  [Train] Batch 300 - Gen Loss: 10.4162, GAN: 0.8615, L1: 0.0955, Disc: 0.8079\n  [Train] Batch 310 - Gen Loss: 11.8342, GAN: 2.8199, L1: 0.0901, Disc: 0.5396\n  [Train] Batch 320 - Gen Loss: 12.6886, GAN: 3.2478, L1: 0.0944, Disc: 1.0418\n  [Train] Batch 330 - Gen Loss: 10.9649, GAN: 1.1833, L1: 0.0978, Disc: 0.5807\n  [Train] Batch 340 - Gen Loss: 11.9704, GAN: 1.7825, L1: 0.1019, Disc: 0.3945\n  [Train] Batch 350 - Gen Loss: 12.2185, GAN: 2.8172, L1: 0.0940, Disc: 0.8837\n  [Train] Batch 360 - Gen Loss: 13.4132, GAN: 2.5311, L1: 0.1088, Disc: 0.9349\n  [Train] Batch 370 - Gen Loss: 12.1611, GAN: 2.4047, L1: 0.0976, Disc: 0.7281\n  [Train] Batch 380 - Gen Loss: 11.3393, GAN: 1.6130, L1: 0.0973, Disc: 0.4888\n  [Train] Batch 390 - Gen Loss: 8.7480, GAN: 0.9113, L1: 0.0784, Disc: 0.9984\n  [Train] Batch 400 - Gen Loss: 9.5349, GAN: 1.7000, L1: 0.0783, Disc: 1.2689\n  [Train] Batch 410 - Gen Loss: 9.4275, GAN: 1.1227, L1: 0.0830, Disc: 1.0381\n  [Val] MAE: 0.1937, RMSE: 0.3271, SSIM: 0.6079\nEpoch 19 completed in 131.09 seconds.\n\nEpoch 20/150\n  [Train] Batch 000 - Gen Loss: 12.2737, GAN: 1.8184, L1: 0.1046, Disc: 0.6532\n  [Train] Batch 010 - Gen Loss: 9.8397, GAN: 1.1930, L1: 0.0865, Disc: 0.6886\n  [Train] Batch 020 - Gen Loss: 11.9039, GAN: 2.8320, L1: 0.0907, Disc: 0.6702\n  [Train] Batch 030 - Gen Loss: 9.9523, GAN: 0.4880, L1: 0.0946, Disc: 1.2249\n  [Train] Batch 040 - Gen Loss: 12.5426, GAN: 1.1034, L1: 0.1144, Disc: 0.8128\n  [Train] Batch 050 - Gen Loss: 11.3953, GAN: 2.0742, L1: 0.0932, Disc: 0.5216\n  [Train] Batch 060 - Gen Loss: 11.4564, GAN: 1.3690, L1: 0.1009, Disc: 0.5626\n  [Train] Batch 070 - Gen Loss: 12.1327, GAN: 1.9086, L1: 0.1022, Disc: 0.5146\n  [Train] Batch 080 - Gen Loss: 9.4393, GAN: 1.1198, L1: 0.0832, Disc: 0.8682\n  [Train] Batch 090 - Gen Loss: 8.2867, GAN: 1.7900, L1: 0.0650, Disc: 0.9836\n  [Train] Batch 100 - Gen Loss: 8.5549, GAN: 1.1006, L1: 0.0745, Disc: 0.7985\n  [Train] Batch 110 - Gen Loss: 11.4102, GAN: 2.3157, L1: 0.0909, Disc: 1.6990\n  [Train] Batch 120 - Gen Loss: 10.1100, GAN: 1.9432, L1: 0.0817, Disc: 1.0853\n  [Train] Batch 130 - Gen Loss: 9.5231, GAN: 1.4105, L1: 0.0811, Disc: 0.8979\n  [Train] Batch 140 - Gen Loss: 11.3811, GAN: 1.7641, L1: 0.0962, Disc: 0.9352\n  [Train] Batch 150 - Gen Loss: 12.3629, GAN: 2.1730, L1: 0.1019, Disc: 0.4062\n  [Train] Batch 160 - Gen Loss: 11.2277, GAN: 1.3716, L1: 0.0986, Disc: 0.6943\n  [Train] Batch 170 - Gen Loss: 10.6217, GAN: 1.3770, L1: 0.0924, Disc: 0.7085\n  [Train] Batch 180 - Gen Loss: 9.5011, GAN: 0.6747, L1: 0.0883, Disc: 0.9760\n  [Train] Batch 190 - Gen Loss: 11.9488, GAN: 2.1463, L1: 0.0980, Disc: 0.4328\n  [Train] Batch 200 - Gen Loss: 11.3149, GAN: 1.4335, L1: 0.0988, Disc: 0.7355\n  [Train] Batch 210 - Gen Loss: 10.2679, GAN: 1.4795, L1: 0.0879, Disc: 0.5354\n  [Train] Batch 220 - Gen Loss: 12.3781, GAN: 1.7658, L1: 0.1061, Disc: 0.5032\n  [Train] Batch 230 - Gen Loss: 11.6695, GAN: 2.1122, L1: 0.0956, Disc: 0.7276\n  [Train] Batch 240 - Gen Loss: 12.7106, GAN: 2.8499, L1: 0.0986, Disc: 0.6844\n  [Train] Batch 250 - Gen Loss: 10.3964, GAN: 1.3224, L1: 0.0907, Disc: 2.0627\n  [Train] Batch 260 - Gen Loss: 12.8274, GAN: 2.2454, L1: 0.1058, Disc: 0.9161\n  [Train] Batch 270 - Gen Loss: 10.5370, GAN: 2.0270, L1: 0.0851, Disc: 0.6583\n  [Train] Batch 280 - Gen Loss: 14.4415, GAN: 3.1288, L1: 0.1131, Disc: 0.8141\n  [Train] Batch 290 - Gen Loss: 11.9577, GAN: 1.1072, L1: 0.1085, Disc: 0.6309\n  [Train] Batch 300 - Gen Loss: 10.6277, GAN: 0.6656, L1: 0.0996, Disc: 0.9806\n  [Train] Batch 310 - Gen Loss: 11.9754, GAN: 2.6176, L1: 0.0936, Disc: 0.7689\n  [Train] Batch 320 - Gen Loss: 11.0740, GAN: 3.2847, L1: 0.0779, Disc: 1.3750\n  [Train] Batch 330 - Gen Loss: 10.2421, GAN: 1.8365, L1: 0.0841, Disc: 0.5504\n  [Train] Batch 340 - Gen Loss: 10.3193, GAN: 2.0387, L1: 0.0828, Disc: 0.8423\n  [Train] Batch 350 - Gen Loss: 10.9859, GAN: 0.8073, L1: 0.1018, Disc: 0.8553\n  [Train] Batch 360 - Gen Loss: 9.3255, GAN: 0.2240, L1: 0.0910, Disc: 2.0482\n  [Train] Batch 370 - Gen Loss: 9.7411, GAN: 0.8890, L1: 0.0885, Disc: 0.7983\n  [Train] Batch 380 - Gen Loss: 11.4740, GAN: 1.7447, L1: 0.0973, Disc: 0.5170\n  [Train] Batch 390 - Gen Loss: 8.8651, GAN: 0.6966, L1: 0.0817, Disc: 0.9441\n  [Train] Batch 400 - Gen Loss: 10.1190, GAN: 2.4602, L1: 0.0766, Disc: 0.8740\n  [Train] Batch 410 - Gen Loss: 11.2388, GAN: 1.1543, L1: 0.1008, Disc: 0.7426\n  [Val] MAE: 0.1920, RMSE: 0.3261, SSIM: 0.6114\nCheckpoint saved: /kaggle/working/outputs/checkpoints/ckpt_epoch_20\nEpoch 20 completed in 133.64 seconds.\n\nEpoch 21/150\n  [Train] Batch 000 - Gen Loss: 10.7680, GAN: 1.0019, L1: 0.0977, Disc: 0.9192\n  [Train] Batch 010 - Gen Loss: 11.1221, GAN: 0.8735, L1: 0.1025, Disc: 0.7478\n  [Train] Batch 020 - Gen Loss: 13.4381, GAN: 2.2130, L1: 0.1123, Disc: 0.7014\n  [Train] Batch 030 - Gen Loss: 10.8448, GAN: 2.0225, L1: 0.0882, Disc: 0.4439\n  [Train] Batch 040 - Gen Loss: 13.0852, GAN: 2.4330, L1: 0.1065, Disc: 1.0205\n  [Train] Batch 050 - Gen Loss: 10.9874, GAN: 2.6853, L1: 0.0830, Disc: 0.8311\n  [Train] Batch 060 - Gen Loss: 11.3847, GAN: 2.6436, L1: 0.0874, Disc: 0.4679\n  [Train] Batch 070 - Gen Loss: 12.4846, GAN: 2.3840, L1: 0.1010, Disc: 0.3989\n  [Train] Batch 080 - Gen Loss: 9.3491, GAN: 1.5470, L1: 0.0780, Disc: 0.9676\n  [Train] Batch 090 - Gen Loss: 8.6721, GAN: 1.5181, L1: 0.0715, Disc: 0.9619\n  [Train] Batch 100 - Gen Loss: 9.1608, GAN: 1.5214, L1: 0.0764, Disc: 0.7074\n  [Train] Batch 110 - Gen Loss: 10.0199, GAN: 2.0998, L1: 0.0792, Disc: 0.4988\n  [Train] Batch 120 - Gen Loss: 10.9348, GAN: 2.3699, L1: 0.0856, Disc: 0.5518\n  [Train] Batch 130 - Gen Loss: 8.0268, GAN: 1.4261, L1: 0.0660, Disc: 0.9311\n  [Train] Batch 140 - Gen Loss: 9.5571, GAN: 1.2204, L1: 0.0834, Disc: 0.7350\n  [Train] Batch 150 - Gen Loss: 11.4506, GAN: 1.2792, L1: 0.1017, Disc: 0.5936\n  [Train] Batch 160 - Gen Loss: 11.1795, GAN: 1.2068, L1: 0.0997, Disc: 0.6580\n  [Train] Batch 170 - Gen Loss: 10.7014, GAN: 1.6090, L1: 0.0909, Disc: 0.6413\n  [Train] Batch 180 - Gen Loss: 12.3911, GAN: 3.0784, L1: 0.0931, Disc: 0.4349\n  [Train] Batch 190 - Gen Loss: 11.8600, GAN: 1.7055, L1: 0.1015, Disc: 0.3443\n  [Train] Batch 200 - Gen Loss: 11.2635, GAN: 2.1965, L1: 0.0907, Disc: 0.6850\n  [Train] Batch 210 - Gen Loss: 9.5715, GAN: 1.0350, L1: 0.0854, Disc: 0.7856\n  [Train] Batch 220 - Gen Loss: 12.8710, GAN: 2.6558, L1: 0.1022, Disc: 0.2173\n  [Train] Batch 230 - Gen Loss: 11.7575, GAN: 1.3850, L1: 0.1037, Disc: 0.7055\n  [Train] Batch 240 - Gen Loss: 11.0319, GAN: 2.3382, L1: 0.0869, Disc: 1.1937\n  [Train] Batch 250 - Gen Loss: 12.4711, GAN: 3.3960, L1: 0.0908, Disc: 3.5679\n  [Train] Batch 260 - Gen Loss: 13.5611, GAN: 3.3477, L1: 0.1021, Disc: 1.6540\n  [Train] Batch 270 - Gen Loss: 12.8110, GAN: 2.1270, L1: 0.1068, Disc: 0.5301\n  [Train] Batch 280 - Gen Loss: 12.3587, GAN: 1.7293, L1: 0.1063, Disc: 0.5882\n  [Train] Batch 290 - Gen Loss: 11.5167, GAN: 1.0525, L1: 0.1046, Disc: 0.7679\n  [Train] Batch 300 - Gen Loss: 11.1894, GAN: 2.0671, L1: 0.0912, Disc: 0.5795\n  [Train] Batch 310 - Gen Loss: 9.5534, GAN: 1.4124, L1: 0.0814, Disc: 0.5209\n  [Train] Batch 320 - Gen Loss: 12.3167, GAN: 2.8068, L1: 0.0951, Disc: 0.5459\n  [Train] Batch 330 - Gen Loss: 9.1550, GAN: 0.3887, L1: 0.0877, Disc: 1.4646\n  [Train] Batch 340 - Gen Loss: 12.6906, GAN: 2.6329, L1: 0.1006, Disc: 0.2298\n  [Train] Batch 350 - Gen Loss: 11.2685, GAN: 1.1251, L1: 0.1014, Disc: 0.6310\n  [Train] Batch 360 - Gen Loss: 10.7375, GAN: 0.6830, L1: 0.1005, Disc: 1.1535\n  [Train] Batch 370 - Gen Loss: 10.8870, GAN: 1.3814, L1: 0.0951, Disc: 0.6087\n  [Train] Batch 380 - Gen Loss: 8.8724, GAN: 0.9857, L1: 0.0789, Disc: 0.7507\n  [Train] Batch 390 - Gen Loss: 8.5091, GAN: 1.4201, L1: 0.0709, Disc: 0.6803\n  [Train] Batch 400 - Gen Loss: 9.0539, GAN: 1.9943, L1: 0.0706, Disc: 1.2084\n  [Train] Batch 410 - Gen Loss: 8.8658, GAN: 1.3905, L1: 0.0748, Disc: 0.7502\n  [Val] MAE: 0.1928, RMSE: 0.3272, SSIM: 0.6030\nEpoch 21 completed in 130.81 seconds.\n\nEpoch 22/150\n  [Train] Batch 000 - Gen Loss: 12.2983, GAN: 2.7895, L1: 0.0951, Disc: 0.2899\n  [Train] Batch 010 - Gen Loss: 9.6708, GAN: 1.3730, L1: 0.0830, Disc: 0.5015\n  [Train] Batch 020 - Gen Loss: 11.3766, GAN: 0.9056, L1: 0.1047, Disc: 0.8901\n  [Train] Batch 030 - Gen Loss: 11.5223, GAN: 2.0792, L1: 0.0944, Disc: 0.5829\n  [Train] Batch 040 - Gen Loss: 11.5674, GAN: 1.8308, L1: 0.0974, Disc: 0.5493\n  [Train] Batch 050 - Gen Loss: 10.1418, GAN: 2.1659, L1: 0.0798, Disc: 0.8053\n  [Train] Batch 060 - Gen Loss: 10.1558, GAN: 0.6706, L1: 0.0949, Disc: 0.9948\n  [Train] Batch 070 - Gen Loss: 10.5296, GAN: 1.4793, L1: 0.0905, Disc: 0.5027\n  [Train] Batch 080 - Gen Loss: 8.6574, GAN: 1.2771, L1: 0.0738, Disc: 4.0458\n  [Train] Batch 090 - Gen Loss: 8.9058, GAN: 0.6454, L1: 0.0826, Disc: 1.3388\n  [Train] Batch 100 - Gen Loss: 10.8395, GAN: 2.5328, L1: 0.0831, Disc: 0.6762\n  [Train] Batch 110 - Gen Loss: 7.8175, GAN: 0.6739, L1: 0.0714, Disc: 0.9365\n  [Train] Batch 120 - Gen Loss: 9.9408, GAN: 1.0984, L1: 0.0884, Disc: 0.7292\n  [Train] Batch 130 - Gen Loss: 7.9734, GAN: 0.7581, L1: 0.0722, Disc: 0.9203\n  [Train] Batch 140 - Gen Loss: 10.9045, GAN: 2.6830, L1: 0.0822, Disc: 0.9945\n  [Train] Batch 150 - Gen Loss: 9.4755, GAN: 0.5599, L1: 0.0892, Disc: 1.1504\n  [Train] Batch 160 - Gen Loss: 9.4501, GAN: 2.0209, L1: 0.0743, Disc: 0.6243\n  [Train] Batch 170 - Gen Loss: 11.9338, GAN: 2.3910, L1: 0.0954, Disc: 0.4129\n  [Train] Batch 180 - Gen Loss: 12.6049, GAN: 3.7171, L1: 0.0889, Disc: 0.4699\n  [Train] Batch 190 - Gen Loss: 11.1036, GAN: 2.1966, L1: 0.0891, Disc: 0.3285\n  [Train] Batch 200 - Gen Loss: 11.7640, GAN: 2.4998, L1: 0.0926, Disc: 0.3043\n  [Train] Batch 210 - Gen Loss: 10.5027, GAN: 0.7989, L1: 0.0970, Disc: 0.7907\n  [Train] Batch 220 - Gen Loss: 11.6184, GAN: 0.9919, L1: 0.1063, Disc: 0.7280\n  [Train] Batch 230 - Gen Loss: 10.7945, GAN: 1.4501, L1: 0.0934, Disc: 0.6074\n  [Train] Batch 240 - Gen Loss: 13.5568, GAN: 4.0360, L1: 0.0952, Disc: 1.2064\n  [Train] Batch 250 - Gen Loss: 10.4852, GAN: 1.4812, L1: 0.0900, Disc: 0.8154\n  [Train] Batch 260 - Gen Loss: 11.7194, GAN: 0.9982, L1: 0.1072, Disc: 0.7646\n  [Train] Batch 270 - Gen Loss: 12.3675, GAN: 1.8893, L1: 0.1048, Disc: 0.3888\n  [Train] Batch 280 - Gen Loss: 11.6511, GAN: 2.7152, L1: 0.0894, Disc: 0.6346\n  [Train] Batch 290 - Gen Loss: 11.9906, GAN: 0.7561, L1: 0.1123, Disc: 0.9710\n  [Train] Batch 300 - Gen Loss: 10.0219, GAN: 0.8563, L1: 0.0917, Disc: 0.8177\n  [Train] Batch 310 - Gen Loss: 11.9882, GAN: 3.0792, L1: 0.0891, Disc: 0.3246\n  [Train] Batch 320 - Gen Loss: 10.8817, GAN: 3.0210, L1: 0.0786, Disc: 0.9467\n  [Train] Batch 330 - Gen Loss: 11.2811, GAN: 3.5469, L1: 0.0773, Disc: 2.5001\n  [Train] Batch 340 - Gen Loss: 10.3759, GAN: 1.9558, L1: 0.0842, Disc: 3.3957\n  [Train] Batch 350 - Gen Loss: 9.7959, GAN: 1.4145, L1: 0.0838, Disc: 2.6960\n  [Train] Batch 360 - Gen Loss: 9.4363, GAN: 1.9078, L1: 0.0753, Disc: 1.6469\n  [Train] Batch 370 - Gen Loss: 11.3059, GAN: 0.8446, L1: 0.1046, Disc: 1.5743\n  [Train] Batch 380 - Gen Loss: 9.1929, GAN: 0.7561, L1: 0.0844, Disc: 1.6768\n  [Train] Batch 390 - Gen Loss: 8.0522, GAN: 1.0056, L1: 0.0705, Disc: 1.2420\n  [Train] Batch 400 - Gen Loss: 7.3119, GAN: 0.6507, L1: 0.0666, Disc: 1.4864\n  [Train] Batch 410 - Gen Loss: 9.3893, GAN: 1.1786, L1: 0.0821, Disc: 0.8421\n  [Val] MAE: 0.1902, RMSE: 0.3251, SSIM: 0.6267\nEpoch 22 completed in 130.94 seconds.\n\nEpoch 23/150\n  [Train] Batch 000 - Gen Loss: 10.4161, GAN: 1.8055, L1: 0.0861, Disc: 1.4761\n  [Train] Batch 010 - Gen Loss: 9.6815, GAN: 1.4807, L1: 0.0820, Disc: 1.1804\n  [Train] Batch 020 - Gen Loss: 11.9186, GAN: 1.2643, L1: 0.1065, Disc: 0.7424\n  [Train] Batch 030 - Gen Loss: 10.1348, GAN: 1.1847, L1: 0.0895, Disc: 0.8698\n  [Train] Batch 040 - Gen Loss: 11.5326, GAN: 1.5179, L1: 0.1001, Disc: 0.7281\n  [Train] Batch 050 - Gen Loss: 10.5373, GAN: 1.4971, L1: 0.0904, Disc: 0.8234\n  [Train] Batch 060 - Gen Loss: 9.1748, GAN: 0.7704, L1: 0.0840, Disc: 0.8610\n  [Train] Batch 070 - Gen Loss: 10.3274, GAN: 1.6514, L1: 0.0868, Disc: 0.6269\n  [Train] Batch 080 - Gen Loss: 9.8874, GAN: 1.3743, L1: 0.0851, Disc: 0.8263\n  [Train] Batch 090 - Gen Loss: 8.5577, GAN: 1.6402, L1: 0.0692, Disc: 1.1727\n  [Train] Batch 100 - Gen Loss: 9.3142, GAN: 1.6632, L1: 0.0765, Disc: 1.0859\n  [Train] Batch 110 - Gen Loss: 10.1198, GAN: 1.7517, L1: 0.0837, Disc: 0.5448\n  [Train] Batch 120 - Gen Loss: 10.6824, GAN: 2.1804, L1: 0.0850, Disc: 0.6918\n  [Train] Batch 130 - Gen Loss: 8.3898, GAN: 0.9301, L1: 0.0746, Disc: 0.7711\n  [Train] Batch 140 - Gen Loss: 11.9283, GAN: 1.8069, L1: 0.1012, Disc: 0.5509\n  [Train] Batch 150 - Gen Loss: 10.1898, GAN: 1.0297, L1: 0.0916, Disc: 0.7427\n  [Train] Batch 160 - Gen Loss: 9.9934, GAN: 1.2880, L1: 0.0871, Disc: 0.6799\n  [Train] Batch 170 - Gen Loss: 8.3911, GAN: 0.9897, L1: 0.0740, Disc: 0.7025\n  [Train] Batch 180 - Gen Loss: 9.9862, GAN: 2.0096, L1: 0.0798, Disc: 0.3783\n  [Train] Batch 190 - Gen Loss: 11.4228, GAN: 1.7164, L1: 0.0971, Disc: 0.4803\n  [Train] Batch 200 - Gen Loss: 10.9819, GAN: 0.9186, L1: 0.1006, Disc: 0.7379\n  [Train] Batch 210 - Gen Loss: 10.7501, GAN: 2.0011, L1: 0.0875, Disc: 0.5910\n  [Train] Batch 220 - Gen Loss: 11.5156, GAN: 2.3332, L1: 0.0918, Disc: 0.5451\n  [Train] Batch 230 - Gen Loss: 12.5325, GAN: 1.4843, L1: 0.1105, Disc: 0.4939\n  [Train] Batch 240 - Gen Loss: 12.6713, GAN: 4.5526, L1: 0.0812, Disc: 0.4880\n  [Train] Batch 250 - Gen Loss: 12.1628, GAN: 2.9081, L1: 0.0925, Disc: 0.4408\n  [Train] Batch 260 - Gen Loss: 11.4040, GAN: 2.1370, L1: 0.0927, Disc: 0.2865\n  [Train] Batch 270 - Gen Loss: 13.3318, GAN: 2.5839, L1: 0.1075, Disc: 0.2714\n  [Train] Batch 280 - Gen Loss: 13.0013, GAN: 3.6860, L1: 0.0932, Disc: 0.1966\n  [Train] Batch 290 - Gen Loss: 11.2193, GAN: 0.8530, L1: 0.1037, Disc: 0.8147\n  [Train] Batch 300 - Gen Loss: 12.6715, GAN: 3.0638, L1: 0.0961, Disc: 0.4000\n  [Train] Batch 310 - Gen Loss: 9.6067, GAN: 0.9193, L1: 0.0869, Disc: 0.7539\n  [Train] Batch 320 - Gen Loss: 9.8125, GAN: 1.4755, L1: 0.0834, Disc: 0.4265\n  [Train] Batch 330 - Gen Loss: 10.8849, GAN: 2.6285, L1: 0.0826, Disc: 0.2468\n  [Train] Batch 340 - Gen Loss: 11.3131, GAN: 2.2588, L1: 0.0905, Disc: 0.5812\n  [Train] Batch 350 - Gen Loss: 12.2376, GAN: 2.3979, L1: 0.0984, Disc: 0.3012\n  [Train] Batch 360 - Gen Loss: 10.1281, GAN: 1.2718, L1: 0.0886, Disc: 0.6265\n  [Train] Batch 370 - Gen Loss: 12.6205, GAN: 2.0268, L1: 0.1059, Disc: 0.5357\n  [Train] Batch 380 - Gen Loss: 10.2780, GAN: 2.4346, L1: 0.0784, Disc: 1.3292\n  [Train] Batch 390 - Gen Loss: 7.8697, GAN: 0.5366, L1: 0.0733, Disc: 1.4726\n  [Train] Batch 400 - Gen Loss: 7.5125, GAN: 0.2444, L1: 0.0727, Disc: 2.1737\n  [Train] Batch 410 - Gen Loss: 8.9439, GAN: 2.2191, L1: 0.0672, Disc: 1.2104\n  [Val] MAE: 0.1927, RMSE: 0.3300, SSIM: 0.6113\nEpoch 23 completed in 131.28 seconds.\n\nEpoch 24/150\n  [Train] Batch 000 - Gen Loss: 11.1808, GAN: 2.3761, L1: 0.0880, Disc: 0.2659\n  [Train] Batch 010 - Gen Loss: 9.6076, GAN: 0.2434, L1: 0.0936, Disc: 1.8663\n  [Train] Batch 020 - Gen Loss: 9.9664, GAN: 0.5267, L1: 0.0944, Disc: 1.2124\n  [Train] Batch 030 - Gen Loss: 11.0554, GAN: 2.0065, L1: 0.0905, Disc: 0.5072\n  [Train] Batch 040 - Gen Loss: 12.3765, GAN: 3.4230, L1: 0.0895, Disc: 0.6535\n  [Train] Batch 050 - Gen Loss: 10.1153, GAN: 1.3625, L1: 0.0875, Disc: 0.5556\n  [Train] Batch 060 - Gen Loss: 14.4749, GAN: 5.5633, L1: 0.0891, Disc: 0.4198\n  [Train] Batch 070 - Gen Loss: 12.1131, GAN: 1.9778, L1: 0.1014, Disc: 0.4983\n  [Train] Batch 080 - Gen Loss: 11.0195, GAN: 2.7531, L1: 0.0827, Disc: 0.3446\n  [Train] Batch 090 - Gen Loss: 9.1938, GAN: 2.4366, L1: 0.0676, Disc: 1.2417\n  [Train] Batch 100 - Gen Loss: 10.2182, GAN: 2.6563, L1: 0.0756, Disc: 0.2810\n  [Train] Batch 110 - Gen Loss: 9.5183, GAN: 1.6266, L1: 0.0789, Disc: 0.6356\n  [Train] Batch 120 - Gen Loss: 9.4923, GAN: 1.2488, L1: 0.0824, Disc: 0.6621\n  [Train] Batch 130 - Gen Loss: 10.9649, GAN: 2.1635, L1: 0.0880, Disc: 0.3472\n  [Train] Batch 140 - Gen Loss: 11.5618, GAN: 1.9414, L1: 0.0962, Disc: 0.4098\n  [Train] Batch 150 - Gen Loss: 12.0803, GAN: 3.3851, L1: 0.0870, Disc: 0.7522\n  [Train] Batch 160 - Gen Loss: 11.6361, GAN: 3.0392, L1: 0.0860, Disc: 0.3497\n  [Train] Batch 170 - Gen Loss: 12.8632, GAN: 4.1789, L1: 0.0868, Disc: 0.5464\n  [Train] Batch 180 - Gen Loss: 13.1077, GAN: 4.0204, L1: 0.0909, Disc: 0.2196\n  [Train] Batch 190 - Gen Loss: 8.7012, GAN: 1.3675, L1: 0.0733, Disc: 0.4986\n  [Train] Batch 200 - Gen Loss: 10.4446, GAN: 1.8507, L1: 0.0859, Disc: 0.4146\n  [Train] Batch 210 - Gen Loss: 12.3094, GAN: 3.1290, L1: 0.0918, Disc: 0.3914\n  [Train] Batch 220 - Gen Loss: 12.0385, GAN: 3.0459, L1: 0.0899, Disc: 0.7327\n  [Train] Batch 230 - Gen Loss: 15.0502, GAN: 5.2328, L1: 0.0982, Disc: 0.5080\n  [Train] Batch 240 - Gen Loss: 13.1942, GAN: 4.6085, L1: 0.0859, Disc: 0.2862\n  [Train] Batch 250 - Gen Loss: 9.9384, GAN: 1.1904, L1: 0.0875, Disc: 0.6749\n  [Train] Batch 260 - Gen Loss: 10.1697, GAN: 0.9852, L1: 0.0918, Disc: 0.7098\n  [Train] Batch 270 - Gen Loss: 11.1367, GAN: 0.8378, L1: 0.1030, Disc: 0.8082\n  [Train] Batch 280 - Gen Loss: 10.5916, GAN: 1.4235, L1: 0.0917, Disc: 0.7726\n  [Train] Batch 290 - Gen Loss: 11.0341, GAN: 1.6784, L1: 0.0936, Disc: 0.4613\n  [Train] Batch 300 - Gen Loss: 11.0609, GAN: 2.0384, L1: 0.0902, Disc: 0.8210\n  [Train] Batch 310 - Gen Loss: 14.2766, GAN: 5.3881, L1: 0.0889, Disc: 0.2769\n  [Train] Batch 320 - Gen Loss: 16.4784, GAN: 6.3403, L1: 0.1014, Disc: 2.6067\n  [Train] Batch 330 - Gen Loss: 11.4258, GAN: 2.3060, L1: 0.0912, Disc: 0.4376\n  [Train] Batch 340 - Gen Loss: 12.7909, GAN: 4.2027, L1: 0.0859, Disc: 2.6355\n  [Train] Batch 350 - Gen Loss: 11.5824, GAN: 2.3416, L1: 0.0924, Disc: 0.3743\n  [Train] Batch 360 - Gen Loss: 10.7914, GAN: 1.2977, L1: 0.0949, Disc: 0.6187\n  [Train] Batch 370 - Gen Loss: 10.6828, GAN: 2.7697, L1: 0.0791, Disc: 0.4450\n  [Train] Batch 380 - Gen Loss: 9.4472, GAN: 1.3793, L1: 0.0807, Disc: 0.8389\n  [Train] Batch 390 - Gen Loss: 7.7503, GAN: 1.1888, L1: 0.0656, Disc: 0.8641\n  [Train] Batch 400 - Gen Loss: 9.4261, GAN: 1.7362, L1: 0.0769, Disc: 0.4537\n  [Train] Batch 410 - Gen Loss: 9.3938, GAN: 1.8888, L1: 0.0751, Disc: 0.3272\n  [Val] MAE: 0.1929, RMSE: 0.3290, SSIM: 0.6018\nEpoch 24 completed in 131.38 seconds.\n\nEpoch 25/150\n  [Train] Batch 000 - Gen Loss: 10.1848, GAN: 2.1919, L1: 0.0799, Disc: 1.3031\n  [Train] Batch 010 - Gen Loss: 14.8504, GAN: 4.2514, L1: 0.1060, Disc: 1.1285\n  [Train] Batch 020 - Gen Loss: 13.4347, GAN: 3.9270, L1: 0.0951, Disc: 0.1613\n  [Train] Batch 030 - Gen Loss: 12.3814, GAN: 1.0974, L1: 0.1128, Disc: 0.7432\n  [Train] Batch 040 - Gen Loss: 11.7789, GAN: 4.0966, L1: 0.0768, Disc: 0.3412\n  [Train] Batch 050 - Gen Loss: 9.7293, GAN: 0.4997, L1: 0.0923, Disc: 1.2137\n  [Train] Batch 060 - Gen Loss: 12.7321, GAN: 3.8573, L1: 0.0887, Disc: 0.5864\n  [Train] Batch 070 - Gen Loss: 12.0594, GAN: 2.4917, L1: 0.0957, Disc: 0.6734\n  [Train] Batch 080 - Gen Loss: 10.7677, GAN: 3.4597, L1: 0.0731, Disc: 0.9350\n  [Train] Batch 090 - Gen Loss: 8.8761, GAN: 1.0317, L1: 0.0784, Disc: 0.7701\n  [Train] Batch 100 - Gen Loss: 11.9094, GAN: 5.1410, L1: 0.0677, Disc: 0.3397\n  [Train] Batch 110 - Gen Loss: 9.5071, GAN: 2.4120, L1: 0.0710, Disc: 0.6605\n  [Train] Batch 120 - Gen Loss: 8.9406, GAN: 1.2724, L1: 0.0767, Disc: 0.4890\n  [Train] Batch 130 - Gen Loss: 9.5532, GAN: 1.4818, L1: 0.0807, Disc: 0.6373\n  [Train] Batch 140 - Gen Loss: 9.7561, GAN: 2.1332, L1: 0.0762, Disc: 0.3098\n  [Train] Batch 150 - Gen Loss: 11.0193, GAN: 1.6548, L1: 0.0936, Disc: 0.7754\n  [Train] Batch 160 - Gen Loss: 10.6816, GAN: 1.3108, L1: 0.0937, Disc: 0.5954\n  [Train] Batch 170 - Gen Loss: 10.0011, GAN: 0.9421, L1: 0.0906, Disc: 0.7283\n  [Train] Batch 180 - Gen Loss: 12.1048, GAN: 2.0061, L1: 0.1010, Disc: 0.2615\n  [Train] Batch 190 - Gen Loss: 8.4133, GAN: 0.7858, L1: 0.0763, Disc: 0.8588\n  [Train] Batch 200 - Gen Loss: 9.8040, GAN: 1.3693, L1: 0.0843, Disc: 0.5498\n  [Train] Batch 210 - Gen Loss: 11.5143, GAN: 2.0989, L1: 0.0942, Disc: 0.3013\n  [Train] Batch 220 - Gen Loss: 9.6872, GAN: 1.0593, L1: 0.0863, Disc: 0.7042\n  [Train] Batch 230 - Gen Loss: 12.0980, GAN: 1.6218, L1: 0.1048, Disc: 0.4506\n  [Train] Batch 240 - Gen Loss: 11.2425, GAN: 1.8136, L1: 0.0943, Disc: 0.6809\n  [Train] Batch 250 - Gen Loss: 9.8393, GAN: 1.1847, L1: 0.0865, Disc: 0.8733\n  [Train] Batch 260 - Gen Loss: 10.3797, GAN: 1.3252, L1: 0.0905, Disc: 0.9871\n  [Train] Batch 270 - Gen Loss: 11.7226, GAN: 1.9893, L1: 0.0973, Disc: 1.0639\n  [Train] Batch 280 - Gen Loss: 9.6264, GAN: 1.8145, L1: 0.0781, Disc: 0.8159\n  [Train] Batch 290 - Gen Loss: 12.3397, GAN: 3.6903, L1: 0.0865, Disc: 0.5357\n  [Train] Batch 300 - Gen Loss: 11.9202, GAN: 2.3547, L1: 0.0957, Disc: 0.5295\n  [Train] Batch 310 - Gen Loss: 11.6423, GAN: 2.2101, L1: 0.0943, Disc: 0.3733\n  [Train] Batch 320 - Gen Loss: 11.6307, GAN: 2.0257, L1: 0.0961, Disc: 1.1535\n  [Train] Batch 330 - Gen Loss: 11.1968, GAN: 2.2418, L1: 0.0896, Disc: 0.9487\n  [Train] Batch 340 - Gen Loss: 9.7778, GAN: 0.7031, L1: 0.0907, Disc: 1.1982\n  [Train] Batch 350 - Gen Loss: 11.0096, GAN: 2.5380, L1: 0.0847, Disc: 0.5494\n  [Train] Batch 360 - Gen Loss: 9.9528, GAN: 0.6650, L1: 0.0929, Disc: 1.0870\n  [Train] Batch 370 - Gen Loss: 8.9996, GAN: 0.7199, L1: 0.0828, Disc: 1.0647\n  [Train] Batch 380 - Gen Loss: 9.1383, GAN: 1.3971, L1: 0.0774, Disc: 0.5755\n  [Train] Batch 390 - Gen Loss: 8.1039, GAN: 1.1642, L1: 0.0694, Disc: 0.6215\n  [Train] Batch 400 - Gen Loss: 7.7476, GAN: 1.6154, L1: 0.0613, Disc: 1.0406\n  [Train] Batch 410 - Gen Loss: 9.1677, GAN: 1.6079, L1: 0.0756, Disc: 0.6355\n  [Val] MAE: 0.1940, RMSE: 0.3283, SSIM: 0.6139\nEpoch 25 completed in 130.87 seconds.\n\nEpoch 26/150\n  [Train] Batch 000 - Gen Loss: 11.4158, GAN: 3.1383, L1: 0.0828, Disc: 1.2366\n  [Train] Batch 010 - Gen Loss: 10.3560, GAN: 1.3794, L1: 0.0898, Disc: 0.7395\n  [Train] Batch 020 - Gen Loss: 8.9958, GAN: 0.8907, L1: 0.0811, Disc: 0.7837\n  [Train] Batch 030 - Gen Loss: 9.8797, GAN: 1.9616, L1: 0.0792, Disc: 0.8413\n  [Train] Batch 040 - Gen Loss: 13.0897, GAN: 2.4066, L1: 0.1068, Disc: 0.2090\n  [Train] Batch 050 - Gen Loss: 11.1435, GAN: 2.7726, L1: 0.0837, Disc: 0.6063\n  [Train] Batch 060 - Gen Loss: 9.8828, GAN: 0.6464, L1: 0.0924, Disc: 1.0502\n  [Train] Batch 070 - Gen Loss: 10.6731, GAN: 2.7491, L1: 0.0792, Disc: 1.1147\n  [Train] Batch 080 - Gen Loss: 9.3373, GAN: 1.3812, L1: 0.0796, Disc: 0.7235\n  [Train] Batch 090 - Gen Loss: 7.4223, GAN: 0.5847, L1: 0.0684, Disc: 1.1630\n  [Train] Batch 100 - Gen Loss: 9.5720, GAN: 2.1255, L1: 0.0745, Disc: 0.2483\n  [Train] Batch 110 - Gen Loss: 7.9220, GAN: 0.7200, L1: 0.0720, Disc: 0.9306\n  [Train] Batch 120 - Gen Loss: 9.0949, GAN: 1.1489, L1: 0.0795, Disc: 0.6711\n  [Train] Batch 130 - Gen Loss: 9.1149, GAN: 2.4387, L1: 0.0668, Disc: 0.5485\n  [Train] Batch 140 - Gen Loss: 11.1802, GAN: 1.0261, L1: 0.1015, Disc: 0.6442\n  [Train] Batch 150 - Gen Loss: 13.0163, GAN: 3.9077, L1: 0.0911, Disc: 0.4769\n  [Train] Batch 160 - Gen Loss: 11.1856, GAN: 2.8840, L1: 0.0830, Disc: 0.3802\n  [Train] Batch 170 - Gen Loss: 12.4564, GAN: 3.1933, L1: 0.0926, Disc: 0.2396\n  [Train] Batch 180 - Gen Loss: 11.8704, GAN: 3.0253, L1: 0.0885, Disc: 0.2183\n  [Train] Batch 190 - Gen Loss: 9.5605, GAN: 1.1916, L1: 0.0837, Disc: 0.6191\n  [Train] Batch 200 - Gen Loss: 9.1673, GAN: 1.1470, L1: 0.0802, Disc: 0.6497\n  [Train] Batch 210 - Gen Loss: 10.6651, GAN: 1.8860, L1: 0.0878, Disc: 0.4072\n  [Train] Batch 220 - Gen Loss: 11.0082, GAN: 1.3218, L1: 0.0969, Disc: 0.4874\n  [Train] Batch 230 - Gen Loss: 13.0116, GAN: 4.1029, L1: 0.0891, Disc: 0.6521\n  [Train] Batch 240 - Gen Loss: 10.1114, GAN: 1.3183, L1: 0.0879, Disc: 0.5454\n  [Train] Batch 250 - Gen Loss: 10.7407, GAN: 2.7820, L1: 0.0796, Disc: 0.9731\n  [Train] Batch 260 - Gen Loss: 13.0784, GAN: 3.2689, L1: 0.0981, Disc: 0.3799\n  [Train] Batch 270 - Gen Loss: 9.9943, GAN: 1.8728, L1: 0.0812, Disc: 0.9903\n  [Train] Batch 280 - Gen Loss: 10.9026, GAN: 2.5775, L1: 0.0833, Disc: 0.3090\n  [Train] Batch 290 - Gen Loss: 7.8676, GAN: 0.4449, L1: 0.0742, Disc: 1.2984\n  [Train] Batch 300 - Gen Loss: 9.7682, GAN: 0.9970, L1: 0.0877, Disc: 4.7533\n  [Train] Batch 310 - Gen Loss: 10.7086, GAN: 2.3638, L1: 0.0834, Disc: 1.0484\n  [Train] Batch 320 - Gen Loss: 9.8405, GAN: 1.4551, L1: 0.0839, Disc: 0.6827\n  [Train] Batch 330 - Gen Loss: 10.0411, GAN: 1.5362, L1: 0.0850, Disc: 0.5035\n  [Train] Batch 340 - Gen Loss: 9.3158, GAN: 0.7720, L1: 0.0854, Disc: 0.9732\n  [Train] Batch 350 - Gen Loss: 9.2579, GAN: 0.9914, L1: 0.0827, Disc: 0.9279\n  [Train] Batch 360 - Gen Loss: 10.2504, GAN: 1.4728, L1: 0.0878, Disc: 0.5381\n  [Train] Batch 370 - Gen Loss: 10.0999, GAN: 2.0822, L1: 0.0802, Disc: 0.6276\n  [Train] Batch 380 - Gen Loss: 9.0498, GAN: 1.7008, L1: 0.0735, Disc: 0.5569\n  [Train] Batch 390 - Gen Loss: 9.3164, GAN: 1.1682, L1: 0.0815, Disc: 0.7100\n  [Train] Batch 400 - Gen Loss: 8.1531, GAN: 0.9877, L1: 0.0717, Disc: 0.8192\n  [Train] Batch 410 - Gen Loss: 7.8041, GAN: 1.5386, L1: 0.0627, Disc: 0.6817\n  [Val] MAE: 0.1933, RMSE: 0.3314, SSIM: 0.6124\nEpoch 26 completed in 130.83 seconds.\n\nEpoch 27/150\n  [Train] Batch 000 - Gen Loss: 10.3453, GAN: 2.3862, L1: 0.0796, Disc: 0.4464\n  [Train] Batch 010 - Gen Loss: 11.5236, GAN: 2.8729, L1: 0.0865, Disc: 0.8136\n  [Train] Batch 020 - Gen Loss: 10.4291, GAN: 1.3021, L1: 0.0913, Disc: 0.7181\n  [Train] Batch 030 - Gen Loss: 10.9789, GAN: 2.0130, L1: 0.0897, Disc: 0.8007\n  [Train] Batch 040 - Gen Loss: 11.0204, GAN: 2.1960, L1: 0.0882, Disc: 0.7052\n  [Train] Batch 050 - Gen Loss: 8.2481, GAN: 0.8324, L1: 0.0742, Disc: 1.2169\n  [Train] Batch 060 - Gen Loss: 9.4825, GAN: 1.0057, L1: 0.0848, Disc: 0.8011\n  [Train] Batch 070 - Gen Loss: 11.1989, GAN: 1.7208, L1: 0.0948, Disc: 0.4984\n  [Train] Batch 080 - Gen Loss: 9.8780, GAN: 2.8130, L1: 0.0707, Disc: 1.1397\n  [Train] Batch 090 - Gen Loss: 8.8664, GAN: 2.1197, L1: 0.0675, Disc: 0.5843\n  [Train] Batch 100 - Gen Loss: 9.3217, GAN: 1.3885, L1: 0.0793, Disc: 0.7797\n  [Train] Batch 110 - Gen Loss: 7.6864, GAN: 0.8513, L1: 0.0684, Disc: 0.8437\n  [Train] Batch 120 - Gen Loss: 10.0122, GAN: 2.2815, L1: 0.0773, Disc: 0.8657\n  [Train] Batch 130 - Gen Loss: 9.0890, GAN: 1.4656, L1: 0.0762, Disc: 0.6840\n  [Train] Batch 140 - Gen Loss: 9.6440, GAN: 1.0400, L1: 0.0860, Disc: 0.6663\n  [Train] Batch 150 - Gen Loss: 11.1061, GAN: 2.0734, L1: 0.0903, Disc: 0.5451\n  [Train] Batch 160 - Gen Loss: 8.8793, GAN: 1.2999, L1: 0.0758, Disc: 0.5631\n  [Train] Batch 170 - Gen Loss: 10.0512, GAN: 2.1542, L1: 0.0790, Disc: 0.3486\n  [Train] Batch 180 - Gen Loss: 12.0921, GAN: 3.3664, L1: 0.0873, Disc: 0.4356\n  [Train] Batch 190 - Gen Loss: 10.1176, GAN: 2.0656, L1: 0.0805, Disc: 0.5077\n  [Train] Batch 200 - Gen Loss: 12.2936, GAN: 3.8797, L1: 0.0841, Disc: 0.9220\n  [Train] Batch 210 - Gen Loss: 10.5206, GAN: 2.5046, L1: 0.0802, Disc: 0.2707\n  [Train] Batch 220 - Gen Loss: 10.4841, GAN: 1.8317, L1: 0.0865, Disc: 0.3409\n  [Train] Batch 230 - Gen Loss: 10.9592, GAN: 2.4102, L1: 0.0855, Disc: 0.6421\n  [Train] Batch 240 - Gen Loss: 10.1530, GAN: 0.9119, L1: 0.0924, Disc: 0.7387\n  [Train] Batch 250 - Gen Loss: 9.9650, GAN: 1.2393, L1: 0.0873, Disc: 0.6462\n  [Train] Batch 260 - Gen Loss: 11.6122, GAN: 1.9537, L1: 0.0966, Disc: 1.1670\n  [Train] Batch 270 - Gen Loss: 9.8513, GAN: 1.2594, L1: 0.0859, Disc: 0.5573\n  [Train] Batch 280 - Gen Loss: 11.4991, GAN: 2.3854, L1: 0.0911, Disc: 0.6294\n  [Train] Batch 290 - Gen Loss: 11.1677, GAN: 2.0560, L1: 0.0911, Disc: 0.4162\n  [Train] Batch 300 - Gen Loss: 11.7699, GAN: 3.0409, L1: 0.0873, Disc: 1.1400\n  [Train] Batch 310 - Gen Loss: 10.4932, GAN: 1.6319, L1: 0.0886, Disc: 0.5007\n  [Train] Batch 320 - Gen Loss: 9.5989, GAN: 1.9461, L1: 0.0765, Disc: 0.3468\n  [Train] Batch 330 - Gen Loss: 11.2872, GAN: 2.4327, L1: 0.0885, Disc: 0.6487\n  [Train] Batch 340 - Gen Loss: 10.5821, GAN: 3.0548, L1: 0.0753, Disc: 0.4205\n  [Train] Batch 350 - Gen Loss: 10.9988, GAN: 2.2587, L1: 0.0874, Disc: 0.6156\n  [Train] Batch 360 - Gen Loss: 12.0891, GAN: 2.4692, L1: 0.0962, Disc: 0.2651\n  [Train] Batch 370 - Gen Loss: 10.1539, GAN: 2.5172, L1: 0.0764, Disc: 0.7906\n  [Train] Batch 380 - Gen Loss: 10.4396, GAN: 2.4829, L1: 0.0796, Disc: 0.3345\n  [Train] Batch 390 - Gen Loss: 7.5962, GAN: 1.1866, L1: 0.0641, Disc: 0.8067\n  [Train] Batch 400 - Gen Loss: 6.8998, GAN: 0.2471, L1: 0.0665, Disc: 2.0915\n  [Train] Batch 410 - Gen Loss: 8.3830, GAN: 0.9520, L1: 0.0743, Disc: 2.0194\n  [Val] MAE: 0.1915, RMSE: 0.3301, SSIM: 0.6100\nEpoch 27 completed in 131.05 seconds.\n\nEpoch 28/150\n  [Train] Batch 000 - Gen Loss: 11.2666, GAN: 0.7959, L1: 0.1047, Disc: 1.1157\n  [Train] Batch 010 - Gen Loss: 8.8029, GAN: 0.7705, L1: 0.0803, Disc: 0.9555\n  [Train] Batch 020 - Gen Loss: 11.9529, GAN: 1.7267, L1: 0.1023, Disc: 0.7106\n  [Train] Batch 030 - Gen Loss: 10.4665, GAN: 1.6548, L1: 0.0881, Disc: 0.4449\n  [Train] Batch 040 - Gen Loss: 9.7330, GAN: 2.0374, L1: 0.0770, Disc: 0.7634\n  [Train] Batch 050 - Gen Loss: 8.1625, GAN: 0.5327, L1: 0.0763, Disc: 1.4275\n  [Train] Batch 060 - Gen Loss: 9.6786, GAN: 1.0558, L1: 0.0862, Disc: 0.6535\n  [Train] Batch 070 - Gen Loss: 9.7060, GAN: 1.4506, L1: 0.0826, Disc: 0.5340\n  [Train] Batch 080 - Gen Loss: 9.8996, GAN: 1.7328, L1: 0.0817, Disc: 0.4875\n  [Train] Batch 090 - Gen Loss: 8.7112, GAN: 1.7217, L1: 0.0699, Disc: 0.5486\n  [Train] Batch 100 - Gen Loss: 7.9730, GAN: 1.2580, L1: 0.0671, Disc: 0.7523\n  [Train] Batch 110 - Gen Loss: 11.4370, GAN: 2.4663, L1: 0.0897, Disc: 0.9923\n  [Train] Batch 120 - Gen Loss: 9.9445, GAN: 2.1609, L1: 0.0778, Disc: 0.7176\n  [Train] Batch 130 - Gen Loss: 9.9143, GAN: 2.1040, L1: 0.0781, Disc: 0.9005\n  [Train] Batch 140 - Gen Loss: 8.9352, GAN: 1.4781, L1: 0.0746, Disc: 0.6697\n  [Train] Batch 150 - Gen Loss: 10.9446, GAN: 2.3841, L1: 0.0856, Disc: 0.7112\n  [Train] Batch 160 - Gen Loss: 9.7513, GAN: 1.1028, L1: 0.0865, Disc: 0.5946\n  [Train] Batch 170 - Gen Loss: 9.3948, GAN: 1.9553, L1: 0.0744, Disc: 0.5560\n  [Train] Batch 180 - Gen Loss: 9.5186, GAN: 1.7679, L1: 0.0775, Disc: 0.3854\n  [Train] Batch 190 - Gen Loss: 9.0000, GAN: 1.3496, L1: 0.0765, Disc: 0.6731\n  [Train] Batch 200 - Gen Loss: 9.5686, GAN: 0.3079, L1: 0.0926, Disc: 1.6731\n  [Train] Batch 210 - Gen Loss: 12.0450, GAN: 3.6839, L1: 0.0836, Disc: 1.5756\n  [Train] Batch 220 - Gen Loss: 11.7901, GAN: 1.9419, L1: 0.0985, Disc: 0.6741\n  [Train] Batch 230 - Gen Loss: 11.2905, GAN: 2.4338, L1: 0.0886, Disc: 0.4134\n  [Train] Batch 240 - Gen Loss: 9.5707, GAN: 2.3355, L1: 0.0724, Disc: 0.3205\n  [Train] Batch 250 - Gen Loss: 10.0895, GAN: 2.6425, L1: 0.0745, Disc: 1.0061\n  [Train] Batch 260 - Gen Loss: 11.4438, GAN: 2.7080, L1: 0.0874, Disc: 0.4183\n  [Train] Batch 270 - Gen Loss: 9.1429, GAN: 1.1260, L1: 0.0802, Disc: 0.6101\n  [Train] Batch 280 - Gen Loss: 11.8897, GAN: 2.9158, L1: 0.0897, Disc: 0.6482\n  [Train] Batch 290 - Gen Loss: 14.3458, GAN: 5.7016, L1: 0.0864, Disc: 8.6561\n  [Train] Batch 300 - Gen Loss: 10.8224, GAN: 1.3923, L1: 0.0943, Disc: 0.7500\n  [Train] Batch 310 - Gen Loss: 9.1865, GAN: 1.0378, L1: 0.0815, Disc: 0.9024\n  [Train] Batch 320 - Gen Loss: 9.2834, GAN: 1.3437, L1: 0.0794, Disc: 0.9032\n  [Train] Batch 330 - Gen Loss: 13.1239, GAN: 4.0173, L1: 0.0911, Disc: 0.7225\n  [Train] Batch 340 - Gen Loss: 12.5679, GAN: 3.7189, L1: 0.0885, Disc: 0.7030\n  [Train] Batch 350 - Gen Loss: 9.3590, GAN: 0.8998, L1: 0.0846, Disc: 0.7320\n  [Train] Batch 360 - Gen Loss: 11.9144, GAN: 2.5768, L1: 0.0934, Disc: 0.1953\n  [Train] Batch 370 - Gen Loss: 10.8775, GAN: 1.2824, L1: 0.0960, Disc: 0.5863\n  [Train] Batch 380 - Gen Loss: 9.9898, GAN: 1.9250, L1: 0.0806, Disc: 0.4563\n  [Train] Batch 390 - Gen Loss: 9.7208, GAN: 2.4304, L1: 0.0729, Disc: 0.4535\n  [Train] Batch 400 - Gen Loss: 8.4473, GAN: 1.8245, L1: 0.0662, Disc: 0.7060\n  [Train] Batch 410 - Gen Loss: 9.3797, GAN: 2.5632, L1: 0.0682, Disc: 0.8355\n  [Val] MAE: 0.1895, RMSE: 0.3241, SSIM: 0.6050\nEpoch 28 completed in 130.81 seconds.\n\nEpoch 29/150\n  [Train] Batch 000 - Gen Loss: 11.2381, GAN: 1.7981, L1: 0.0944, Disc: 0.3691\n  [Train] Batch 010 - Gen Loss: 11.5556, GAN: 3.5631, L1: 0.0799, Disc: 3.6723\n  [Train] Batch 020 - Gen Loss: 11.3922, GAN: 1.5364, L1: 0.0986, Disc: 1.1816\n  [Train] Batch 030 - Gen Loss: 9.2480, GAN: 0.8005, L1: 0.0845, Disc: 1.4717\n  [Train] Batch 040 - Gen Loss: 8.8736, GAN: 0.8235, L1: 0.0805, Disc: 1.0250\n  [Train] Batch 050 - Gen Loss: 10.2698, GAN: 1.5623, L1: 0.0871, Disc: 0.7885\n  [Train] Batch 060 - Gen Loss: 8.8534, GAN: 0.9235, L1: 0.0793, Disc: 0.8953\n  [Train] Batch 070 - Gen Loss: 10.7936, GAN: 2.1626, L1: 0.0863, Disc: 0.4639\n  [Train] Batch 080 - Gen Loss: 7.3976, GAN: 0.3653, L1: 0.0703, Disc: 1.4843\n  [Train] Batch 090 - Gen Loss: 8.3821, GAN: 1.3295, L1: 0.0705, Disc: 0.7879\n  [Train] Batch 100 - Gen Loss: 8.6021, GAN: 1.8178, L1: 0.0678, Disc: 0.7259\n  [Train] Batch 110 - Gen Loss: 10.6216, GAN: 2.8452, L1: 0.0778, Disc: 0.7108\n  [Train] Batch 120 - Gen Loss: 8.4131, GAN: 2.1286, L1: 0.0628, Disc: 0.6925\n  [Train] Batch 130 - Gen Loss: 8.0872, GAN: 1.5915, L1: 0.0650, Disc: 0.8740\n  [Train] Batch 140 - Gen Loss: 11.7148, GAN: 2.6763, L1: 0.0904, Disc: 0.8257\n  [Train] Batch 150 - Gen Loss: 10.3039, GAN: 1.6657, L1: 0.0864, Disc: 0.4765\n  [Train] Batch 160 - Gen Loss: 11.3911, GAN: 2.6527, L1: 0.0874, Disc: 0.3809\n  [Train] Batch 170 - Gen Loss: 10.4094, GAN: 1.7894, L1: 0.0862, Disc: 0.5328\n  [Train] Batch 180 - Gen Loss: 10.5242, GAN: 1.4651, L1: 0.0906, Disc: 0.4977\n  [Train] Batch 190 - Gen Loss: 9.1219, GAN: 1.6865, L1: 0.0744, Disc: 0.5151\n  [Train] Batch 200 - Gen Loss: 9.2912, GAN: 1.1765, L1: 0.0811, Disc: 0.8202\n  [Train] Batch 210 - Gen Loss: 9.9960, GAN: 1.9062, L1: 0.0809, Disc: 0.6380\n  [Train] Batch 220 - Gen Loss: 11.1402, GAN: 1.4327, L1: 0.0971, Disc: 0.4869\n  [Train] Batch 230 - Gen Loss: 10.8944, GAN: 3.1721, L1: 0.0772, Disc: 0.7501\n  [Train] Batch 240 - Gen Loss: 11.7913, GAN: 3.2643, L1: 0.0853, Disc: 0.4590\n  [Train] Batch 250 - Gen Loss: 10.4405, GAN: 2.5609, L1: 0.0788, Disc: 0.5109\n  [Train] Batch 260 - Gen Loss: 10.5590, GAN: 1.7492, L1: 0.0881, Disc: 0.3894\n  [Train] Batch 270 - Gen Loss: 11.2275, GAN: 2.2910, L1: 0.0894, Disc: 0.4176\n  [Train] Batch 280 - Gen Loss: 10.4248, GAN: 2.7524, L1: 0.0767, Disc: 1.5468\n  [Train] Batch 290 - Gen Loss: 10.4688, GAN: 1.1589, L1: 0.0931, Disc: 0.7967\n  [Train] Batch 300 - Gen Loss: 10.3486, GAN: 2.0886, L1: 0.0826, Disc: 0.5924\n  [Train] Batch 310 - Gen Loss: 11.0191, GAN: 2.3484, L1: 0.0867, Disc: 0.7018\n  [Train] Batch 320 - Gen Loss: 8.9648, GAN: 1.0392, L1: 0.0793, Disc: 0.9084\n  [Train] Batch 330 - Gen Loss: 8.8099, GAN: 1.1462, L1: 0.0766, Disc: 0.7808\n  [Train] Batch 340 - Gen Loss: 11.0434, GAN: 2.0680, L1: 0.0898, Disc: 0.4141\n  [Train] Batch 350 - Gen Loss: 10.9121, GAN: 1.8738, L1: 0.0904, Disc: 0.9463\n  [Train] Batch 360 - Gen Loss: 9.5517, GAN: 1.1854, L1: 0.0837, Disc: 0.7037\n  [Train] Batch 370 - Gen Loss: 8.7365, GAN: 1.2038, L1: 0.0753, Disc: 0.7697\n  [Train] Batch 380 - Gen Loss: 8.5314, GAN: 2.1518, L1: 0.0638, Disc: 0.8932\n  [Train] Batch 390 - Gen Loss: 7.8312, GAN: 0.6932, L1: 0.0714, Disc: 1.0034\n  [Train] Batch 400 - Gen Loss: 8.2405, GAN: 1.8943, L1: 0.0635, Disc: 0.8683\n  [Train] Batch 410 - Gen Loss: 8.0758, GAN: 1.1953, L1: 0.0688, Disc: 0.7283\n  [Val] MAE: 0.1892, RMSE: 0.3234, SSIM: 0.6086\nEpoch 29 completed in 130.82 seconds.\n\nEpoch 30/150\n  [Train] Batch 000 - Gen Loss: 10.6004, GAN: 2.2410, L1: 0.0836, Disc: 0.3893\n  [Train] Batch 010 - Gen Loss: 10.8645, GAN: 1.2619, L1: 0.0960, Disc: 0.5309\n  [Train] Batch 020 - Gen Loss: 12.3033, GAN: 2.3861, L1: 0.0992, Disc: 0.5636\n  [Train] Batch 030 - Gen Loss: 9.2549, GAN: 1.7635, L1: 0.0749, Disc: 0.6997\n  [Train] Batch 040 - Gen Loss: 8.8018, GAN: 1.0230, L1: 0.0778, Disc: 0.7343\n  [Train] Batch 050 - Gen Loss: 10.8143, GAN: 3.0091, L1: 0.0781, Disc: 1.0559\n  [Train] Batch 060 - Gen Loss: 10.0075, GAN: 1.8906, L1: 0.0812, Disc: 0.3978\n  [Train] Batch 070 - Gen Loss: 10.9366, GAN: 1.7490, L1: 0.0919, Disc: 0.4016\n  [Train] Batch 080 - Gen Loss: 10.9425, GAN: 2.9930, L1: 0.0795, Disc: 1.2372\n  [Train] Batch 090 - Gen Loss: 7.3649, GAN: 0.9479, L1: 0.0642, Disc: 1.0667\n  [Train] Batch 100 - Gen Loss: 9.2724, GAN: 2.4008, L1: 0.0687, Disc: 0.6763\n  [Train] Batch 110 - Gen Loss: 8.3543, GAN: 1.2202, L1: 0.0713, Disc: 0.6445\n  [Train] Batch 120 - Gen Loss: 11.4587, GAN: 2.9745, L1: 0.0848, Disc: 0.8784\n  [Train] Batch 130 - Gen Loss: 8.5117, GAN: 1.3365, L1: 0.0718, Disc: 0.6031\n  [Train] Batch 140 - Gen Loss: 10.1331, GAN: 1.9525, L1: 0.0818, Disc: 0.5858\n  [Train] Batch 150 - Gen Loss: 9.9222, GAN: 2.2872, L1: 0.0764, Disc: 0.4422\n  [Train] Batch 160 - Gen Loss: 10.2060, GAN: 1.7078, L1: 0.0850, Disc: 0.5902\n  [Train] Batch 170 - Gen Loss: 11.3998, GAN: 2.8883, L1: 0.0851, Disc: 0.3915\n  [Train] Batch 180 - Gen Loss: 10.8768, GAN: 2.2252, L1: 0.0865, Disc: 0.3676\n  [Train] Batch 190 - Gen Loss: 8.6085, GAN: 0.6352, L1: 0.0797, Disc: 1.0311\n  [Train] Batch 200 - Gen Loss: 8.5534, GAN: 1.1618, L1: 0.0739, Disc: 0.6404\n  [Train] Batch 210 - Gen Loss: 10.4389, GAN: 1.2594, L1: 0.0918, Disc: 0.5105\n  [Train] Batch 220 - Gen Loss: 10.3075, GAN: 1.3133, L1: 0.0899, Disc: 0.4778\n  [Train] Batch 230 - Gen Loss: 10.8681, GAN: 2.4880, L1: 0.0838, Disc: 0.4886\n  [Train] Batch 240 - Gen Loss: 10.7373, GAN: 2.1897, L1: 0.0855, Disc: 0.4282\n  [Train] Batch 250 - Gen Loss: 11.8317, GAN: 2.2569, L1: 0.0957, Disc: 0.5591\n  [Train] Batch 260 - Gen Loss: 11.1332, GAN: 1.8929, L1: 0.0924, Disc: 0.4140\n  [Train] Batch 270 - Gen Loss: 10.7969, GAN: 1.8183, L1: 0.0898, Disc: 0.3603\n  [Train] Batch 280 - Gen Loss: 10.0591, GAN: 1.9354, L1: 0.0812, Disc: 0.3699\n  [Train] Batch 290 - Gen Loss: 10.2061, GAN: 2.4416, L1: 0.0776, Disc: 0.5851\n  [Train] Batch 300 - Gen Loss: 10.1615, GAN: 2.3502, L1: 0.0781, Disc: 0.8958\n  [Train] Batch 310 - Gen Loss: 10.3714, GAN: 1.5417, L1: 0.0883, Disc: 0.4044\n  [Train] Batch 320 - Gen Loss: 7.3729, GAN: 0.2166, L1: 0.0716, Disc: 2.1593\n  [Train] Batch 330 - Gen Loss: 8.8059, GAN: 0.8785, L1: 0.0793, Disc: 1.4973\n  [Train] Batch 340 - Gen Loss: 8.6959, GAN: 0.6572, L1: 0.0804, Disc: 1.0942\n  [Train] Batch 350 - Gen Loss: 10.2980, GAN: 2.3202, L1: 0.0798, Disc: 0.9569\n  [Train] Batch 360 - Gen Loss: 9.6088, GAN: 1.1413, L1: 0.0847, Disc: 0.5783\n  [Train] Batch 370 - Gen Loss: 11.4037, GAN: 2.8380, L1: 0.0857, Disc: 0.8039\n  [Train] Batch 380 - Gen Loss: 9.2852, GAN: 2.4849, L1: 0.0680, Disc: 0.7633\n  [Train] Batch 390 - Gen Loss: 8.5940, GAN: 1.5165, L1: 0.0708, Disc: 0.5602\n  [Train] Batch 400 - Gen Loss: 7.3449, GAN: 1.3940, L1: 0.0595, Disc: 0.5828\n  [Train] Batch 410 - Gen Loss: 7.3496, GAN: 0.7275, L1: 0.0662, Disc: 1.0197\n  [Val] MAE: 0.1866, RMSE: 0.3201, SSIM: 0.6196\nCheckpoint saved: /kaggle/working/outputs/checkpoints/ckpt_epoch_30\nEpoch 30 completed in 133.20 seconds.\n\nEpoch 31/150\n  [Train] Batch 000 - Gen Loss: 8.7916, GAN: 1.3164, L1: 0.0748, Disc: 0.5739\n  [Train] Batch 010 - Gen Loss: 8.7785, GAN: 2.7712, L1: 0.0601, Disc: 0.9518\n  [Train] Batch 020 - Gen Loss: 10.9044, GAN: 2.8473, L1: 0.0806, Disc: 0.6602\n  [Train] Batch 030 - Gen Loss: 11.5872, GAN: 1.6235, L1: 0.0996, Disc: 0.4351\n  [Train] Batch 040 - Gen Loss: 9.5701, GAN: 2.2330, L1: 0.0734, Disc: 0.6037\n  [Train] Batch 050 - Gen Loss: 9.7234, GAN: 2.1440, L1: 0.0758, Disc: 0.5991\n  [Train] Batch 060 - Gen Loss: 11.2129, GAN: 2.8307, L1: 0.0838, Disc: 0.1613\n  [Train] Batch 070 - Gen Loss: 10.0830, GAN: 1.8826, L1: 0.0820, Disc: 0.3680\n  [Train] Batch 080 - Gen Loss: 11.4876, GAN: 4.4453, L1: 0.0704, Disc: 0.8715\n  [Train] Batch 090 - Gen Loss: 8.9223, GAN: 1.7684, L1: 0.0715, Disc: 0.5784\n  [Train] Batch 100 - Gen Loss: 7.8355, GAN: 2.1854, L1: 0.0565, Disc: 0.8911\n  [Train] Batch 110 - Gen Loss: 11.6975, GAN: 1.9318, L1: 0.0977, Disc: 0.4360\n  [Train] Batch 120 - Gen Loss: 11.8909, GAN: 3.2783, L1: 0.0861, Disc: 0.4943\n  [Train] Batch 130 - Gen Loss: 7.5645, GAN: 0.7199, L1: 0.0684, Disc: 0.9886\n  [Train] Batch 140 - Gen Loss: 9.0154, GAN: 2.0926, L1: 0.0692, Disc: 0.7340\n  [Train] Batch 150 - Gen Loss: 8.1657, GAN: 0.3756, L1: 0.0779, Disc: 1.4625\n  [Train] Batch 160 - Gen Loss: 10.2010, GAN: 2.1407, L1: 0.0806, Disc: 0.3674\n  [Train] Batch 170 - Gen Loss: 9.9069, GAN: 1.6092, L1: 0.0830, Disc: 0.4884\n  [Train] Batch 180 - Gen Loss: 8.2781, GAN: 1.0367, L1: 0.0724, Disc: 0.6742\n  [Train] Batch 190 - Gen Loss: 8.1881, GAN: 1.2229, L1: 0.0697, Disc: 0.6377\n  [Train] Batch 200 - Gen Loss: 9.5696, GAN: 1.6502, L1: 0.0792, Disc: 0.4523\n  [Train] Batch 210 - Gen Loss: 8.9036, GAN: 1.8169, L1: 0.0709, Disc: 0.4739\n  [Train] Batch 220 - Gen Loss: 11.7947, GAN: 2.5183, L1: 0.0928, Disc: 0.5750\n  [Train] Batch 230 - Gen Loss: 10.1182, GAN: 1.7015, L1: 0.0842, Disc: 0.5153\n  [Train] Batch 240 - Gen Loss: 10.1158, GAN: 2.1713, L1: 0.0794, Disc: 0.6485\n  [Train] Batch 250 - Gen Loss: 12.9461, GAN: 5.2845, L1: 0.0766, Disc: 0.8049\n  [Train] Batch 260 - Gen Loss: 8.9665, GAN: 0.6347, L1: 0.0833, Disc: 1.0807\n  [Train] Batch 270 - Gen Loss: 12.4143, GAN: 3.1382, L1: 0.0928, Disc: 1.3948\n  [Train] Batch 280 - Gen Loss: 11.1634, GAN: 2.2553, L1: 0.0891, Disc: 1.1164\n  [Train] Batch 290 - Gen Loss: 8.9010, GAN: 1.4028, L1: 0.0750, Disc: 0.8522\n  [Train] Batch 300 - Gen Loss: 8.9942, GAN: 1.4980, L1: 0.0750, Disc: 0.5889\n  [Train] Batch 310 - Gen Loss: 8.0207, GAN: 1.0932, L1: 0.0693, Disc: 0.8519\n  [Train] Batch 320 - Gen Loss: 10.1562, GAN: 2.8608, L1: 0.0730, Disc: 1.3378\n  [Train] Batch 330 - Gen Loss: 9.5398, GAN: 1.9168, L1: 0.0762, Disc: 1.2780\n  [Train] Batch 340 - Gen Loss: 11.8717, GAN: 3.8276, L1: 0.0804, Disc: 0.6332\n  [Train] Batch 350 - Gen Loss: 10.1149, GAN: 2.0783, L1: 0.0804, Disc: 0.4338\n  [Train] Batch 360 - Gen Loss: 10.8797, GAN: 1.7814, L1: 0.0910, Disc: 0.5786\n  [Train] Batch 370 - Gen Loss: 9.8509, GAN: 2.1336, L1: 0.0772, Disc: 0.4858\n  [Train] Batch 380 - Gen Loss: 8.8363, GAN: 2.1688, L1: 0.0667, Disc: 0.9396\n  [Train] Batch 390 - Gen Loss: 7.0935, GAN: 0.9962, L1: 0.0610, Disc: 0.9630\n  [Train] Batch 400 - Gen Loss: 7.2224, GAN: 1.4366, L1: 0.0579, Disc: 0.8954\n  [Train] Batch 410 - Gen Loss: 9.6517, GAN: 3.0256, L1: 0.0663, Disc: 1.2446\n  [Val] MAE: 0.1860, RMSE: 0.3209, SSIM: 0.6199\nEpoch 31 completed in 130.82 seconds.\n\nEpoch 32/150\n  [Train] Batch 000 - Gen Loss: 8.5855, GAN: 1.0493, L1: 0.0754, Disc: 0.8004\n  [Train] Batch 010 - Gen Loss: 10.8991, GAN: 1.6713, L1: 0.0923, Disc: 0.5013\n  [Train] Batch 020 - Gen Loss: 9.2309, GAN: 1.5744, L1: 0.0766, Disc: 0.6710\n  [Train] Batch 030 - Gen Loss: 8.6834, GAN: 0.6492, L1: 0.0803, Disc: 1.0159\n  [Train] Batch 040 - Gen Loss: 10.4087, GAN: 1.2336, L1: 0.0918, Disc: 0.6009\n  [Train] Batch 050 - Gen Loss: 8.3713, GAN: 1.4029, L1: 0.0697, Disc: 0.4745\n  [Train] Batch 060 - Gen Loss: 11.0825, GAN: 4.5931, L1: 0.0649, Disc: 0.8690\n  [Train] Batch 070 - Gen Loss: 9.4099, GAN: 1.9061, L1: 0.0750, Disc: 0.6772\n  [Train] Batch 080 - Gen Loss: 8.6764, GAN: 2.0513, L1: 0.0663, Disc: 0.7335\n  [Train] Batch 090 - Gen Loss: 9.0894, GAN: 2.7578, L1: 0.0633, Disc: 0.5941\n  [Train] Batch 100 - Gen Loss: 8.7538, GAN: 1.8435, L1: 0.0691, Disc: 0.6671\n  [Train] Batch 110 - Gen Loss: 7.7656, GAN: 0.2582, L1: 0.0751, Disc: 1.8959\n  [Train] Batch 120 - Gen Loss: 8.7435, GAN: 1.4546, L1: 0.0729, Disc: 0.6295\n  [Train] Batch 130 - Gen Loss: 8.2861, GAN: 1.3810, L1: 0.0691, Disc: 0.7951\n  [Train] Batch 140 - Gen Loss: 11.0106, GAN: 3.1823, L1: 0.0783, Disc: 0.7425\n  [Train] Batch 150 - Gen Loss: 9.8177, GAN: 2.0250, L1: 0.0779, Disc: 0.6291\n  [Train] Batch 160 - Gen Loss: 9.5141, GAN: 1.7894, L1: 0.0772, Disc: 0.4769\n  [Train] Batch 170 - Gen Loss: 11.3035, GAN: 1.5508, L1: 0.0975, Disc: 0.5969\n  [Train] Batch 180 - Gen Loss: 10.3312, GAN: 1.4402, L1: 0.0889, Disc: 0.4644\n  [Train] Batch 190 - Gen Loss: 9.5013, GAN: 2.5694, L1: 0.0693, Disc: 0.2458\n  [Train] Batch 200 - Gen Loss: 9.9493, GAN: 1.6618, L1: 0.0829, Disc: 0.4566\n  [Train] Batch 210 - Gen Loss: 10.4868, GAN: 2.8650, L1: 0.0762, Disc: 0.8382\n  [Train] Batch 220 - Gen Loss: 9.3326, GAN: 1.1959, L1: 0.0814, Disc: 0.6809\n  [Train] Batch 230 - Gen Loss: 11.0186, GAN: 1.8073, L1: 0.0921, Disc: 0.4118\n  [Train] Batch 240 - Gen Loss: 11.3134, GAN: 2.8254, L1: 0.0849, Disc: 0.8193\n  [Train] Batch 250 - Gen Loss: 12.1024, GAN: 3.9639, L1: 0.0814, Disc: 2.1543\n  [Train] Batch 260 - Gen Loss: 11.4352, GAN: 2.4682, L1: 0.0897, Disc: 0.7241\n  [Train] Batch 270 - Gen Loss: 11.8898, GAN: 4.2381, L1: 0.0765, Disc: 0.4308\n  [Train] Batch 280 - Gen Loss: 10.3570, GAN: 2.6020, L1: 0.0775, Disc: 0.8992\n  [Train] Batch 290 - Gen Loss: 9.1774, GAN: 1.1880, L1: 0.0799, Disc: 0.6462\n  [Train] Batch 300 - Gen Loss: 9.5537, GAN: 1.1838, L1: 0.0837, Disc: 0.5499\n  [Train] Batch 310 - Gen Loss: 9.4336, GAN: 1.7330, L1: 0.0770, Disc: 0.4428\n  [Train] Batch 320 - Gen Loss: 9.1309, GAN: 2.3681, L1: 0.0676, Disc: 0.2810\n  [Train] Batch 330 - Gen Loss: 9.1974, GAN: 1.6570, L1: 0.0754, Disc: 0.4631\n  [Train] Batch 340 - Gen Loss: 11.1356, GAN: 2.6508, L1: 0.0848, Disc: 0.3982\n  [Train] Batch 350 - Gen Loss: 8.4198, GAN: 0.8124, L1: 0.0761, Disc: 0.8722\n  [Train] Batch 360 - Gen Loss: 8.2748, GAN: 0.7678, L1: 0.0751, Disc: 0.9852\n  [Train] Batch 370 - Gen Loss: 9.9705, GAN: 2.1451, L1: 0.0783, Disc: 1.5333\n  [Train] Batch 380 - Gen Loss: 8.7548, GAN: 1.3869, L1: 0.0737, Disc: 0.6544\n  [Train] Batch 390 - Gen Loss: 8.1907, GAN: 0.6072, L1: 0.0758, Disc: 1.2109\n  [Train] Batch 400 - Gen Loss: 7.7837, GAN: 0.8529, L1: 0.0693, Disc: 0.9796\n  [Train] Batch 410 - Gen Loss: 9.1469, GAN: 3.1328, L1: 0.0601, Disc: 2.9144\n  [Val] MAE: 0.1925, RMSE: 0.3298, SSIM: 0.6108\nEpoch 32 completed in 131.01 seconds.\n\nEpoch 33/150\n  [Train] Batch 000 - Gen Loss: 9.7237, GAN: 1.4327, L1: 0.0829, Disc: 0.8223\n  [Train] Batch 010 - Gen Loss: 10.6871, GAN: 0.7809, L1: 0.0991, Disc: 0.8490\n  [Train] Batch 020 - Gen Loss: 10.5364, GAN: 1.5783, L1: 0.0896, Disc: 0.7658\n  [Train] Batch 030 - Gen Loss: 11.0387, GAN: 2.5904, L1: 0.0845, Disc: 0.8362\n  [Train] Batch 040 - Gen Loss: 9.7924, GAN: 1.1721, L1: 0.0862, Disc: 0.6828\n  [Train] Batch 050 - Gen Loss: 8.8511, GAN: 2.1434, L1: 0.0671, Disc: 0.8896\n  [Train] Batch 060 - Gen Loss: 9.7875, GAN: 3.1527, L1: 0.0663, Disc: 1.0506\n  [Train] Batch 070 - Gen Loss: 10.4516, GAN: 1.7436, L1: 0.0871, Disc: 0.4528\n  [Train] Batch 080 - Gen Loss: 9.6844, GAN: 2.0243, L1: 0.0766, Disc: 0.7576\n  [Train] Batch 090 - Gen Loss: 9.6630, GAN: 2.9854, L1: 0.0668, Disc: 1.3015\n  [Train] Batch 100 - Gen Loss: 8.3319, GAN: 2.1611, L1: 0.0617, Disc: 0.6677\n  [Train] Batch 110 - Gen Loss: 7.7568, GAN: 0.7783, L1: 0.0698, Disc: 0.9016\n  [Train] Batch 120 - Gen Loss: 11.1863, GAN: 3.1186, L1: 0.0807, Disc: 0.6398\n  [Train] Batch 130 - Gen Loss: 8.4370, GAN: 1.3541, L1: 0.0708, Disc: 0.8419\n  [Train] Batch 140 - Gen Loss: 10.8402, GAN: 2.4345, L1: 0.0841, Disc: 0.5685\n  [Train] Batch 150 - Gen Loss: 11.1471, GAN: 2.5057, L1: 0.0864, Disc: 0.5546\n  [Train] Batch 160 - Gen Loss: 8.0390, GAN: 1.1776, L1: 0.0686, Disc: 0.6132\n  [Train] Batch 170 - Gen Loss: 9.3857, GAN: 1.2004, L1: 0.0819, Disc: 0.5974\n  [Train] Batch 180 - Gen Loss: 8.2755, GAN: 0.9676, L1: 0.0731, Disc: 0.6598\n  [Train] Batch 190 - Gen Loss: 8.0777, GAN: 0.6425, L1: 0.0744, Disc: 1.0140\n  [Train] Batch 200 - Gen Loss: 7.8880, GAN: 1.0896, L1: 0.0680, Disc: 0.6475\n  [Train] Batch 210 - Gen Loss: 9.7713, GAN: 2.4376, L1: 0.0733, Disc: 0.3075\n  [Train] Batch 220 - Gen Loss: 10.2769, GAN: 2.5847, L1: 0.0769, Disc: 0.4660\n  [Train] Batch 230 - Gen Loss: 9.8570, GAN: 2.0986, L1: 0.0776, Disc: 0.5135\n  [Train] Batch 240 - Gen Loss: 11.2222, GAN: 1.7488, L1: 0.0947, Disc: 0.4494\n  [Train] Batch 250 - Gen Loss: 9.9595, GAN: 3.0957, L1: 0.0686, Disc: 0.6718\n  [Train] Batch 260 - Gen Loss: 10.1604, GAN: 0.7501, L1: 0.0941, Disc: 0.9419\n  [Train] Batch 270 - Gen Loss: 10.7010, GAN: 1.5712, L1: 0.0913, Disc: 0.4365\n  [Train] Batch 280 - Gen Loss: 8.8822, GAN: 1.3839, L1: 0.0750, Disc: 0.5013\n  [Train] Batch 290 - Gen Loss: 9.8728, GAN: 1.9257, L1: 0.0795, Disc: 0.4971\n  [Train] Batch 300 - Gen Loss: 10.3029, GAN: 2.1250, L1: 0.0818, Disc: 0.4556\n  [Train] Batch 310 - Gen Loss: 10.3535, GAN: 1.9165, L1: 0.0844, Disc: 0.3059\n  [Train] Batch 320 - Gen Loss: 9.1433, GAN: 2.0430, L1: 0.0710, Disc: 1.9074\n  [Train] Batch 330 - Gen Loss: 7.8551, GAN: 0.3053, L1: 0.0755, Disc: 4.6715\n  [Train] Batch 340 - Gen Loss: 7.6144, GAN: 1.4141, L1: 0.0620, Disc: 1.3618\n  [Train] Batch 350 - Gen Loss: 9.6155, GAN: 0.3900, L1: 0.0923, Disc: 1.6637\n  [Train] Batch 360 - Gen Loss: 8.4778, GAN: 1.5591, L1: 0.0692, Disc: 1.5420\n  [Train] Batch 370 - Gen Loss: 9.0721, GAN: 1.2195, L1: 0.0785, Disc: 0.9042\n  [Train] Batch 380 - Gen Loss: 9.5988, GAN: 2.7142, L1: 0.0688, Disc: 1.1521\n  [Train] Batch 390 - Gen Loss: 6.8697, GAN: 1.0207, L1: 0.0585, Disc: 0.8473\n  [Train] Batch 400 - Gen Loss: 8.6841, GAN: 1.1601, L1: 0.0752, Disc: 0.6397\n  [Train] Batch 410 - Gen Loss: 8.5315, GAN: 1.0546, L1: 0.0748, Disc: 0.8768\n  [Val] MAE: 0.1879, RMSE: 0.3235, SSIM: 0.6196\nEpoch 33 completed in 131.01 seconds.\n\nEpoch 34/150\n  [Train] Batch 000 - Gen Loss: 8.5127, GAN: 1.3463, L1: 0.0717, Disc: 0.6699\n  [Train] Batch 010 - Gen Loss: 9.1728, GAN: 1.3624, L1: 0.0781, Disc: 0.9490\n  [Train] Batch 020 - Gen Loss: 10.5775, GAN: 2.1366, L1: 0.0844, Disc: 0.5277\n  [Train] Batch 030 - Gen Loss: 9.5581, GAN: 1.9505, L1: 0.0761, Disc: 0.8013\n  [Train] Batch 040 - Gen Loss: 9.7578, GAN: 1.9300, L1: 0.0783, Disc: 0.6766\n  [Train] Batch 050 - Gen Loss: 8.9171, GAN: 1.1803, L1: 0.0774, Disc: 0.7852\n  [Train] Batch 060 - Gen Loss: 11.2541, GAN: 4.0267, L1: 0.0723, Disc: 1.2191\n  [Train] Batch 070 - Gen Loss: 8.8955, GAN: 1.5661, L1: 0.0733, Disc: 0.4184\n  [Train] Batch 080 - Gen Loss: 6.8672, GAN: 0.3136, L1: 0.0655, Disc: 1.7798\n  [Train] Batch 090 - Gen Loss: 7.1903, GAN: 1.7477, L1: 0.0544, Disc: 1.0828\n  [Train] Batch 100 - Gen Loss: 8.4034, GAN: 2.0750, L1: 0.0633, Disc: 0.5420\n  [Train] Batch 110 - Gen Loss: 8.4967, GAN: 1.6593, L1: 0.0684, Disc: 0.6579\n  [Train] Batch 120 - Gen Loss: 9.8602, GAN: 1.6574, L1: 0.0820, Disc: 0.9378\n  [Train] Batch 130 - Gen Loss: 7.5035, GAN: 1.0420, L1: 0.0646, Disc: 0.8804\n  [Train] Batch 140 - Gen Loss: 9.4670, GAN: 2.0040, L1: 0.0746, Disc: 0.6773\n  [Train] Batch 150 - Gen Loss: 9.1246, GAN: 0.7102, L1: 0.0841, Disc: 0.8668\n  [Train] Batch 160 - Gen Loss: 10.1156, GAN: 1.9614, L1: 0.0815, Disc: 0.3555\n  [Train] Batch 170 - Gen Loss: 12.1601, GAN: 4.2260, L1: 0.0793, Disc: 0.8463\n  [Train] Batch 180 - Gen Loss: 11.4751, GAN: 2.5562, L1: 0.0892, Disc: 0.5846\n  [Train] Batch 190 - Gen Loss: 9.6500, GAN: 2.0668, L1: 0.0758, Disc: 0.4649\n  [Train] Batch 200 - Gen Loss: 9.3717, GAN: 2.4676, L1: 0.0690, Disc: 0.2430\n  [Train] Batch 210 - Gen Loss: 10.0025, GAN: 1.0494, L1: 0.0895, Disc: 0.7548\n  [Train] Batch 220 - Gen Loss: 9.8608, GAN: 1.8147, L1: 0.0805, Disc: 0.5296\n  [Train] Batch 230 - Gen Loss: 9.8651, GAN: 1.9574, L1: 0.0791, Disc: 0.6834\n  [Train] Batch 240 - Gen Loss: 9.7891, GAN: 2.9103, L1: 0.0688, Disc: 1.3164\n  [Train] Batch 250 - Gen Loss: 9.5225, GAN: 2.3882, L1: 0.0713, Disc: 0.4786\n  [Train] Batch 260 - Gen Loss: 10.0575, GAN: 1.7781, L1: 0.0828, Disc: 0.3603\n  [Train] Batch 270 - Gen Loss: 10.4086, GAN: 1.5727, L1: 0.0884, Disc: 0.4616\n  [Train] Batch 280 - Gen Loss: 8.1551, GAN: 0.9487, L1: 0.0721, Disc: 0.7216\n  [Train] Batch 290 - Gen Loss: 9.4523, GAN: 1.6064, L1: 0.0785, Disc: 0.4489\n  [Train] Batch 300 - Gen Loss: 10.7446, GAN: 2.5003, L1: 0.0824, Disc: 0.8232\n  [Train] Batch 310 - Gen Loss: 9.5674, GAN: 1.8140, L1: 0.0775, Disc: 0.5771\n  [Train] Batch 320 - Gen Loss: 8.7077, GAN: 1.3118, L1: 0.0740, Disc: 0.6438\n  [Train] Batch 330 - Gen Loss: 9.8431, GAN: 2.7251, L1: 0.0712, Disc: 0.7300\n  [Train] Batch 340 - Gen Loss: 10.4564, GAN: 2.3831, L1: 0.0807, Disc: 0.9516\n  [Train] Batch 350 - Gen Loss: 11.6526, GAN: 3.6133, L1: 0.0804, Disc: 0.8842\n  [Train] Batch 360 - Gen Loss: 9.8632, GAN: 2.8477, L1: 0.0702, Disc: 0.7343\n  [Train] Batch 370 - Gen Loss: 8.6535, GAN: 1.3978, L1: 0.0726, Disc: 0.6137\n  [Train] Batch 380 - Gen Loss: 7.5735, GAN: 1.0971, L1: 0.0648, Disc: 0.7716\n  [Train] Batch 390 - Gen Loss: 8.0136, GAN: 1.4471, L1: 0.0657, Disc: 0.6688\n  [Train] Batch 400 - Gen Loss: 8.5646, GAN: 3.0432, L1: 0.0552, Disc: 2.9654\n  [Train] Batch 410 - Gen Loss: 8.0292, GAN: 1.7419, L1: 0.0629, Disc: 0.5995\n  [Val] MAE: 0.1863, RMSE: 0.3204, SSIM: 0.6149\nEpoch 34 completed in 130.87 seconds.\n\nEpoch 35/150\n  [Train] Batch 000 - Gen Loss: 7.5754, GAN: 0.9868, L1: 0.0659, Disc: 0.8314\n  [Train] Batch 010 - Gen Loss: 7.6128, GAN: 1.3717, L1: 0.0624, Disc: 0.7543\n  [Train] Batch 020 - Gen Loss: 10.2284, GAN: 1.9165, L1: 0.0831, Disc: 0.4855\n  [Train] Batch 030 - Gen Loss: 8.1646, GAN: 1.1723, L1: 0.0699, Disc: 0.7439\n  [Train] Batch 040 - Gen Loss: 10.0333, GAN: 1.5587, L1: 0.0847, Disc: 0.4703\n  [Train] Batch 050 - Gen Loss: 10.1722, GAN: 2.5520, L1: 0.0762, Disc: 0.6901\n  [Train] Batch 060 - Gen Loss: 8.5398, GAN: 0.8963, L1: 0.0764, Disc: 0.8212\n  [Train] Batch 070 - Gen Loss: 10.2384, GAN: 3.2847, L1: 0.0695, Disc: 1.2481\n  [Train] Batch 080 - Gen Loss: 7.6844, GAN: 1.3307, L1: 0.0635, Disc: 0.8220\n  [Train] Batch 090 - Gen Loss: 7.0535, GAN: 1.1053, L1: 0.0595, Disc: 1.0392\n  [Train] Batch 100 - Gen Loss: 6.5657, GAN: 0.7649, L1: 0.0580, Disc: 0.9253\n  [Train] Batch 110 - Gen Loss: 9.4127, GAN: 2.2647, L1: 0.0715, Disc: 0.3306\n  [Train] Batch 120 - Gen Loss: 9.0506, GAN: 1.6442, L1: 0.0741, Disc: 1.3448\n  [Train] Batch 130 - Gen Loss: 8.7542, GAN: 1.7582, L1: 0.0700, Disc: 2.1434\n  [Train] Batch 140 - Gen Loss: 10.0920, GAN: 1.7965, L1: 0.0830, Disc: 0.6400\n  [Train] Batch 150 - Gen Loss: 10.9329, GAN: 2.1421, L1: 0.0879, Disc: 0.4903\n  [Train] Batch 160 - Gen Loss: 8.4954, GAN: 1.1867, L1: 0.0731, Disc: 0.6188\n  [Train] Batch 170 - Gen Loss: 8.9232, GAN: 1.7118, L1: 0.0721, Disc: 0.6191\n  [Train] Batch 180 - Gen Loss: 9.0204, GAN: 1.5974, L1: 0.0742, Disc: 0.6370\n  [Train] Batch 190 - Gen Loss: 9.7518, GAN: 2.5366, L1: 0.0722, Disc: 0.4730\n  [Train] Batch 200 - Gen Loss: 8.8623, GAN: 1.2543, L1: 0.0761, Disc: 0.5282\n  [Train] Batch 210 - Gen Loss: 11.8975, GAN: 2.4210, L1: 0.0948, Disc: 0.4853\n  [Train] Batch 220 - Gen Loss: 11.2634, GAN: 2.9876, L1: 0.0828, Disc: 0.7632\n  [Train] Batch 230 - Gen Loss: 10.1178, GAN: 1.6008, L1: 0.0852, Disc: 0.3989\n  [Train] Batch 240 - Gen Loss: 12.1744, GAN: 3.2984, L1: 0.0888, Disc: 0.7221\n  [Train] Batch 250 - Gen Loss: 9.1453, GAN: 1.9585, L1: 0.0719, Disc: 0.4361\n  [Train] Batch 260 - Gen Loss: 10.9162, GAN: 1.3242, L1: 0.0959, Disc: 0.4958\n  [Train] Batch 270 - Gen Loss: 9.8422, GAN: 1.6074, L1: 0.0823, Disc: 0.5543\n  [Train] Batch 280 - Gen Loss: 9.6029, GAN: 1.6604, L1: 0.0794, Disc: 0.5392\n  [Train] Batch 290 - Gen Loss: 8.8604, GAN: 1.0958, L1: 0.0776, Disc: 0.7292\n  [Train] Batch 300 - Gen Loss: 9.5650, GAN: 2.1357, L1: 0.0743, Disc: 0.3470\n  [Train] Batch 310 - Gen Loss: 9.7546, GAN: 2.7050, L1: 0.0705, Disc: 0.4591\n  [Train] Batch 320 - Gen Loss: 10.0790, GAN: 3.2605, L1: 0.0682, Disc: 1.0883\n  [Train] Batch 330 - Gen Loss: 10.0992, GAN: 2.8354, L1: 0.0726, Disc: 0.2420\n  [Train] Batch 340 - Gen Loss: 10.5376, GAN: 3.5197, L1: 0.0702, Disc: 2.1613\n  [Train] Batch 350 - Gen Loss: 9.8671, GAN: 2.1056, L1: 0.0776, Disc: 4.8822\n  [Train] Batch 360 - Gen Loss: 9.3890, GAN: 1.4396, L1: 0.0795, Disc: 1.5967\n  [Train] Batch 370 - Gen Loss: 8.4743, GAN: 1.5128, L1: 0.0696, Disc: 1.8107\n  [Train] Batch 380 - Gen Loss: 7.0415, GAN: 0.4322, L1: 0.0661, Disc: 1.6394\n  [Train] Batch 390 - Gen Loss: 7.5774, GAN: 1.2369, L1: 0.0634, Disc: 1.4664\n  [Train] Batch 400 - Gen Loss: 6.9361, GAN: 0.9648, L1: 0.0597, Disc: 1.4025\n  [Train] Batch 410 - Gen Loss: 7.5957, GAN: 1.4963, L1: 0.0610, Disc: 1.1137\n  [Val] MAE: 0.1836, RMSE: 0.3184, SSIM: 0.6268\nEpoch 35 completed in 131.04 seconds.\n\nEpoch 36/150\n  [Train] Batch 000 - Gen Loss: 9.5158, GAN: 1.3381, L1: 0.0818, Disc: 0.8434\n  [Train] Batch 010 - Gen Loss: 9.7591, GAN: 1.0174, L1: 0.0874, Disc: 1.0501\n  [Train] Batch 020 - Gen Loss: 8.2139, GAN: 0.3239, L1: 0.0789, Disc: 1.6603\n  [Train] Batch 030 - Gen Loss: 7.9218, GAN: 1.1573, L1: 0.0676, Disc: 0.9402\n  [Train] Batch 040 - Gen Loss: 8.5347, GAN: 1.0109, L1: 0.0752, Disc: 0.7546\n  [Train] Batch 050 - Gen Loss: 7.8199, GAN: 0.6599, L1: 0.0716, Disc: 1.0663\n  [Train] Batch 060 - Gen Loss: 9.5516, GAN: 2.2400, L1: 0.0731, Disc: 0.9742\n  [Train] Batch 070 - Gen Loss: 8.5896, GAN: 1.3553, L1: 0.0723, Disc: 0.4877\n  [Train] Batch 080 - Gen Loss: 6.9305, GAN: 0.5798, L1: 0.0635, Disc: 1.1374\n  [Train] Batch 090 - Gen Loss: 7.3987, GAN: 2.1344, L1: 0.0526, Disc: 1.0941\n  [Train] Batch 100 - Gen Loss: 5.5464, GAN: 0.3253, L1: 0.0522, Disc: 1.7721\n  [Train] Batch 110 - Gen Loss: 9.1934, GAN: 2.8481, L1: 0.0635, Disc: 1.3075\n  [Train] Batch 120 - Gen Loss: 8.6591, GAN: 0.9360, L1: 0.0772, Disc: 0.7925\n  [Train] Batch 130 - Gen Loss: 8.3909, GAN: 1.9721, L1: 0.0642, Disc: 0.7032\n  [Train] Batch 140 - Gen Loss: 9.4831, GAN: 1.7393, L1: 0.0774, Disc: 0.6038\n  [Train] Batch 150 - Gen Loss: 8.8687, GAN: 1.7813, L1: 0.0709, Disc: 0.6768\n  [Train] Batch 160 - Gen Loss: 8.8495, GAN: 2.5421, L1: 0.0631, Disc: 0.3619\n  [Train] Batch 170 - Gen Loss: 10.0290, GAN: 1.6768, L1: 0.0835, Disc: 0.5725\n  [Train] Batch 180 - Gen Loss: 8.9042, GAN: 1.7585, L1: 0.0715, Disc: 0.4504\n  [Train] Batch 190 - Gen Loss: 8.2186, GAN: 1.4005, L1: 0.0682, Disc: 0.6428\n  [Train] Batch 200 - Gen Loss: 9.0103, GAN: 1.6251, L1: 0.0739, Disc: 0.4337\n  [Train] Batch 210 - Gen Loss: 8.1244, GAN: 1.2045, L1: 0.0692, Disc: 0.6909\n  [Train] Batch 220 - Gen Loss: 9.1799, GAN: 1.4613, L1: 0.0772, Disc: 0.4757\n  [Train] Batch 230 - Gen Loss: 9.6662, GAN: 2.3212, L1: 0.0735, Disc: 0.7575\n  [Train] Batch 240 - Gen Loss: 8.9339, GAN: 1.7191, L1: 0.0721, Disc: 0.4791\n  [Train] Batch 250 - Gen Loss: 9.3226, GAN: 1.1580, L1: 0.0816, Disc: 0.6598\n  [Train] Batch 260 - Gen Loss: 8.9345, GAN: 1.4373, L1: 0.0750, Disc: 0.7880\n  [Train] Batch 270 - Gen Loss: 10.1338, GAN: 0.9213, L1: 0.0921, Disc: 0.7556\n  [Train] Batch 280 - Gen Loss: 9.4299, GAN: 1.4068, L1: 0.0802, Disc: 0.5366\n  [Train] Batch 290 - Gen Loss: 8.8249, GAN: 0.7998, L1: 0.0803, Disc: 0.9297\n  [Train] Batch 300 - Gen Loss: 10.3424, GAN: 2.4735, L1: 0.0787, Disc: 0.4384\n  [Train] Batch 310 - Gen Loss: 9.4174, GAN: 1.2322, L1: 0.0819, Disc: 0.5632\n  [Train] Batch 320 - Gen Loss: 9.1197, GAN: 1.5703, L1: 0.0755, Disc: 0.4706\n  [Train] Batch 330 - Gen Loss: 9.2929, GAN: 2.9795, L1: 0.0631, Disc: 3.6060\n  [Train] Batch 340 - Gen Loss: 7.7699, GAN: 1.3375, L1: 0.0643, Disc: 0.8498\n  [Train] Batch 350 - Gen Loss: 8.8695, GAN: 1.4019, L1: 0.0747, Disc: 0.7566\n  [Train] Batch 360 - Gen Loss: 9.1210, GAN: 1.1386, L1: 0.0798, Disc: 0.6049\n  [Train] Batch 370 - Gen Loss: 9.7207, GAN: 2.2773, L1: 0.0744, Disc: 0.8625\n  [Train] Batch 380 - Gen Loss: 8.6158, GAN: 1.9096, L1: 0.0671, Disc: 0.6923\n  [Train] Batch 390 - Gen Loss: 8.1575, GAN: 1.8722, L1: 0.0629, Disc: 0.9760\n  [Train] Batch 400 - Gen Loss: 8.4932, GAN: 2.1474, L1: 0.0635, Disc: 1.4152\n  [Train] Batch 410 - Gen Loss: 7.6047, GAN: 0.9943, L1: 0.0661, Disc: 0.7834\n  [Val] MAE: 0.1858, RMSE: 0.3186, SSIM: 0.6143\nEpoch 36 completed in 130.85 seconds.\n\nEpoch 37/150\n  [Train] Batch 000 - Gen Loss: 9.0372, GAN: 1.6227, L1: 0.0741, Disc: 0.7574\n  [Train] Batch 010 - Gen Loss: 9.5789, GAN: 2.2472, L1: 0.0733, Disc: 0.3317\n  [Train] Batch 020 - Gen Loss: 11.1977, GAN: 2.8010, L1: 0.0840, Disc: 0.3192\n  [Train] Batch 030 - Gen Loss: 8.6433, GAN: 1.1055, L1: 0.0754, Disc: 0.6588\n  [Train] Batch 040 - Gen Loss: 7.7285, GAN: 1.1369, L1: 0.0659, Disc: 0.8191\n  [Train] Batch 050 - Gen Loss: 7.3687, GAN: 0.3773, L1: 0.0699, Disc: 1.6535\n  [Train] Batch 060 - Gen Loss: 9.2155, GAN: 2.2522, L1: 0.0696, Disc: 0.3568\n  [Train] Batch 070 - Gen Loss: 10.0561, GAN: 1.8251, L1: 0.0823, Disc: 0.3794\n  [Train] Batch 080 - Gen Loss: 8.3986, GAN: 1.1882, L1: 0.0721, Disc: 0.8136\n  [Train] Batch 090 - Gen Loss: 7.5718, GAN: 1.6802, L1: 0.0589, Disc: 0.7525\n  [Train] Batch 100 - Gen Loss: 7.7095, GAN: 1.4100, L1: 0.0630, Disc: 0.5713\n  [Train] Batch 110 - Gen Loss: 10.0646, GAN: 2.9587, L1: 0.0711, Disc: 0.6660\n  [Train] Batch 120 - Gen Loss: 8.2344, GAN: 1.2231, L1: 0.0701, Disc: 0.6282\n  [Train] Batch 130 - Gen Loss: 7.6372, GAN: 0.6639, L1: 0.0697, Disc: 1.1348\n  [Train] Batch 140 - Gen Loss: 8.9099, GAN: 1.2998, L1: 0.0761, Disc: 0.5853\n  [Train] Batch 150 - Gen Loss: 9.1763, GAN: 1.7160, L1: 0.0746, Disc: 0.4672\n  [Train] Batch 160 - Gen Loss: 9.5122, GAN: 2.4486, L1: 0.0706, Disc: 0.2913\n  [Train] Batch 170 - Gen Loss: 9.6726, GAN: 1.2470, L1: 0.0843, Disc: 0.5199\n  [Train] Batch 180 - Gen Loss: 10.1408, GAN: 2.5236, L1: 0.0762, Disc: 0.3004\n  [Train] Batch 190 - Gen Loss: 9.6972, GAN: 2.1103, L1: 0.0759, Disc: 0.3084\n  [Train] Batch 200 - Gen Loss: 8.9545, GAN: 1.0821, L1: 0.0787, Disc: 0.6246\n  [Train] Batch 210 - Gen Loss: 9.1244, GAN: 1.5354, L1: 0.0759, Disc: 0.4432\n  [Train] Batch 220 - Gen Loss: 11.0114, GAN: 3.2166, L1: 0.0779, Disc: 1.2603\n  [Train] Batch 230 - Gen Loss: 9.6718, GAN: 1.4022, L1: 0.0827, Disc: 0.6015\n  [Train] Batch 240 - Gen Loss: 8.4150, GAN: 1.3921, L1: 0.0702, Disc: 0.6085\n  [Train] Batch 250 - Gen Loss: 9.2549, GAN: 1.3108, L1: 0.0794, Disc: 0.6157\n  [Train] Batch 260 - Gen Loss: 10.7785, GAN: 2.9419, L1: 0.0784, Disc: 1.3048\n  [Train] Batch 270 - Gen Loss: 10.3138, GAN: 2.6057, L1: 0.0771, Disc: 0.4826\n  [Train] Batch 280 - Gen Loss: 10.2000, GAN: 2.5685, L1: 0.0763, Disc: 0.5346\n  [Train] Batch 290 - Gen Loss: 11.0840, GAN: 3.2015, L1: 0.0788, Disc: 0.2572\n  [Train] Batch 300 - Gen Loss: 10.2108, GAN: 2.7238, L1: 0.0749, Disc: 1.2721\n  [Train] Batch 310 - Gen Loss: 9.4935, GAN: 2.4501, L1: 0.0704, Disc: 0.8655\n  [Train] Batch 320 - Gen Loss: 10.3975, GAN: 2.2306, L1: 0.0817, Disc: 0.5819\n  [Train] Batch 330 - Gen Loss: 9.3333, GAN: 2.0040, L1: 0.0733, Disc: 0.4946\n  [Train] Batch 340 - Gen Loss: 9.9588, GAN: 2.9086, L1: 0.0705, Disc: 3.9256\n  [Train] Batch 350 - Gen Loss: 9.8472, GAN: 2.1258, L1: 0.0772, Disc: 0.3590\n  [Train] Batch 360 - Gen Loss: 9.0356, GAN: 0.9876, L1: 0.0805, Disc: 0.7038\n  [Train] Batch 370 - Gen Loss: 9.1804, GAN: 2.4998, L1: 0.0668, Disc: 0.9779\n  [Train] Batch 380 - Gen Loss: 7.9223, GAN: 1.4444, L1: 0.0648, Disc: 0.9084\n  [Train] Batch 390 - Gen Loss: 7.7905, GAN: 1.6024, L1: 0.0619, Disc: 0.6904\n  [Train] Batch 400 - Gen Loss: 8.0411, GAN: 1.8468, L1: 0.0619, Disc: 0.4777\n  [Train] Batch 410 - Gen Loss: 6.4189, GAN: 0.7635, L1: 0.0566, Disc: 0.8549\n  [Val] MAE: 0.1864, RMSE: 0.3215, SSIM: 0.6160\nEpoch 37 completed in 130.92 seconds.\n\nEpoch 38/150\n  [Train] Batch 000 - Gen Loss: 8.1679, GAN: 0.7671, L1: 0.0740, Disc: 1.1042\n  [Train] Batch 010 - Gen Loss: 7.9111, GAN: 0.7418, L1: 0.0717, Disc: 0.9223\n  [Train] Batch 020 - Gen Loss: 9.2212, GAN: 0.9169, L1: 0.0830, Disc: 0.8268\n  [Train] Batch 030 - Gen Loss: 10.1233, GAN: 2.2347, L1: 0.0789, Disc: 0.4271\n  [Train] Batch 040 - Gen Loss: 7.9596, GAN: 1.3511, L1: 0.0661, Disc: 0.7001\n  [Train] Batch 050 - Gen Loss: 6.9858, GAN: 0.8166, L1: 0.0617, Disc: 1.0538\n  [Train] Batch 060 - Gen Loss: 9.6822, GAN: 1.8161, L1: 0.0787, Disc: 0.4133\n  [Train] Batch 070 - Gen Loss: 9.1120, GAN: 1.5789, L1: 0.0753, Disc: 0.4613\n  [Train] Batch 080 - Gen Loss: 7.8662, GAN: 1.8643, L1: 0.0600, Disc: 1.1342\n  [Train] Batch 090 - Gen Loss: 6.5198, GAN: 0.5031, L1: 0.0602, Disc: 1.2911\n  [Train] Batch 100 - Gen Loss: 6.8724, GAN: 0.7366, L1: 0.0614, Disc: 0.9434\n  [Train] Batch 110 - Gen Loss: 7.0431, GAN: 0.6213, L1: 0.0642, Disc: 1.0564\n  [Train] Batch 120 - Gen Loss: 9.2175, GAN: 1.7882, L1: 0.0743, Disc: 0.7227\n  [Train] Batch 130 - Gen Loss: 7.1750, GAN: 0.7891, L1: 0.0639, Disc: 0.8929\n  [Train] Batch 140 - Gen Loss: 8.6695, GAN: 0.7495, L1: 0.0792, Disc: 0.9568\n  [Train] Batch 150 - Gen Loss: 8.5378, GAN: 1.5076, L1: 0.0703, Disc: 0.5695\n  [Train] Batch 160 - Gen Loss: 8.7891, GAN: 1.5086, L1: 0.0728, Disc: 0.4869\n  [Train] Batch 170 - Gen Loss: 9.1618, GAN: 1.6790, L1: 0.0748, Disc: 0.6092\n  [Train] Batch 180 - Gen Loss: 9.3196, GAN: 2.1812, L1: 0.0714, Disc: 0.3988\n  [Train] Batch 190 - Gen Loss: 7.2103, GAN: 1.2231, L1: 0.0599, Disc: 0.4942\n  [Train] Batch 200 - Gen Loss: 11.2921, GAN: 3.5907, L1: 0.0770, Disc: 0.4131\n  [Train] Batch 210 - Gen Loss: 8.9832, GAN: 1.7031, L1: 0.0728, Disc: 0.6654\n  [Train] Batch 220 - Gen Loss: 8.9048, GAN: 0.7544, L1: 0.0815, Disc: 0.8857\n  [Train] Batch 230 - Gen Loss: 10.2014, GAN: 2.0981, L1: 0.0810, Disc: 0.4452\n  [Train] Batch 240 - Gen Loss: 9.3887, GAN: 1.9624, L1: 0.0743, Disc: 0.8765\n  [Train] Batch 250 - Gen Loss: 9.7910, GAN: 2.4039, L1: 0.0739, Disc: 0.5834\n  [Train] Batch 260 - Gen Loss: 9.3877, GAN: 1.1739, L1: 0.0821, Disc: 0.7043\n  [Train] Batch 270 - Gen Loss: 9.9803, GAN: 2.5149, L1: 0.0747, Disc: 0.8060\n  [Train] Batch 280 - Gen Loss: 10.0763, GAN: 2.7201, L1: 0.0736, Disc: 0.3340\n  [Train] Batch 290 - Gen Loss: 9.2541, GAN: 1.3740, L1: 0.0788, Disc: 0.4981\n  [Train] Batch 300 - Gen Loss: 9.6887, GAN: 2.7911, L1: 0.0690, Disc: 1.1402\n  [Train] Batch 310 - Gen Loss: 8.5425, GAN: 1.3003, L1: 0.0724, Disc: 1.2917\n  [Train] Batch 320 - Gen Loss: 7.5457, GAN: 0.6457, L1: 0.0690, Disc: 1.0401\n  [Train] Batch 330 - Gen Loss: 10.8117, GAN: 3.4807, L1: 0.0733, Disc: 1.1407\n  [Train] Batch 340 - Gen Loss: 10.0462, GAN: 3.0432, L1: 0.0700, Disc: 0.4274\n  [Train] Batch 350 - Gen Loss: 9.1223, GAN: 1.2750, L1: 0.0785, Disc: 0.6051\n  [Train] Batch 360 - Gen Loss: 10.3332, GAN: 2.1968, L1: 0.0814, Disc: 0.9330\n  [Train] Batch 370 - Gen Loss: 8.5846, GAN: 0.8838, L1: 0.0770, Disc: 0.8436\n  [Train] Batch 380 - Gen Loss: 10.0052, GAN: 3.2107, L1: 0.0679, Disc: 1.2205\n  [Train] Batch 390 - Gen Loss: 6.7878, GAN: 0.5393, L1: 0.0625, Disc: 1.4958\n  [Train] Batch 400 - Gen Loss: 6.5852, GAN: 1.3501, L1: 0.0524, Disc: 0.4953\n  [Train] Batch 410 - Gen Loss: 8.7028, GAN: 2.5462, L1: 0.0616, Disc: 0.4368\n  [Val] MAE: 0.1871, RMSE: 0.3199, SSIM: 0.6095\nEpoch 38 completed in 130.86 seconds.\n\nEpoch 39/150\n  [Train] Batch 000 - Gen Loss: 8.1877, GAN: 0.9345, L1: 0.0725, Disc: 0.7487\n  [Train] Batch 010 - Gen Loss: 8.0487, GAN: 0.8154, L1: 0.0723, Disc: 0.8244\n  [Train] Batch 020 - Gen Loss: 10.0211, GAN: 1.5827, L1: 0.0844, Disc: 0.5337\n  [Train] Batch 030 - Gen Loss: 8.9870, GAN: 0.5068, L1: 0.0848, Disc: 1.1909\n  [Train] Batch 040 - Gen Loss: 10.6032, GAN: 2.0723, L1: 0.0853, Disc: 0.8544\n  [Train] Batch 050 - Gen Loss: 8.5081, GAN: 1.7736, L1: 0.0673, Disc: 0.9735\n  [Train] Batch 060 - Gen Loss: 9.6606, GAN: 2.4369, L1: 0.0722, Disc: 0.8500\n  [Train] Batch 070 - Gen Loss: 8.6329, GAN: 2.3528, L1: 0.0628, Disc: 0.9614\n  [Train] Batch 080 - Gen Loss: 7.3397, GAN: 1.3444, L1: 0.0600, Disc: 0.8646\n  [Train] Batch 090 - Gen Loss: 8.0234, GAN: 2.1684, L1: 0.0586, Disc: 0.7799\n  [Train] Batch 100 - Gen Loss: 7.3136, GAN: 0.9014, L1: 0.0641, Disc: 0.8149\n  [Train] Batch 110 - Gen Loss: 7.2671, GAN: 0.8681, L1: 0.0640, Disc: 0.7556\n  [Train] Batch 120 - Gen Loss: 9.9049, GAN: 3.1722, L1: 0.0673, Disc: 0.6945\n  [Train] Batch 130 - Gen Loss: 8.0766, GAN: 1.6654, L1: 0.0641, Disc: 0.6177\n  [Train] Batch 140 - Gen Loss: 8.3083, GAN: 1.3872, L1: 0.0692, Disc: 0.7073\n  [Train] Batch 150 - Gen Loss: 10.1832, GAN: 1.6796, L1: 0.0850, Disc: 0.5252\n  [Train] Batch 160 - Gen Loss: 9.7295, GAN: 2.0979, L1: 0.0763, Disc: 0.3853\n  [Train] Batch 170 - Gen Loss: 9.1574, GAN: 2.3142, L1: 0.0684, Disc: 0.2708\n  [Train] Batch 180 - Gen Loss: 9.6317, GAN: 1.8407, L1: 0.0779, Disc: 0.3882\n  [Train] Batch 190 - Gen Loss: 8.0436, GAN: 1.1272, L1: 0.0692, Disc: 0.6752\n  [Train] Batch 200 - Gen Loss: 6.8057, GAN: 0.9584, L1: 0.0585, Disc: 0.7801\n  [Train] Batch 210 - Gen Loss: 8.5174, GAN: 1.5084, L1: 0.0701, Disc: 0.6034\n  [Train] Batch 220 - Gen Loss: 9.8704, GAN: 1.3702, L1: 0.0850, Disc: 0.5433\n  [Train] Batch 230 - Gen Loss: 9.3527, GAN: 1.4547, L1: 0.0790, Disc: 0.6830\n  [Train] Batch 240 - Gen Loss: 8.9923, GAN: 0.8804, L1: 0.0811, Disc: 0.8029\n  [Train] Batch 250 - Gen Loss: 8.2208, GAN: 1.4877, L1: 0.0673, Disc: 0.4631\n  [Train] Batch 260 - Gen Loss: 9.9130, GAN: 1.4158, L1: 0.0850, Disc: 0.5253\n  [Train] Batch 270 - Gen Loss: 8.9386, GAN: 1.6968, L1: 0.0724, Disc: 0.7054\n  [Train] Batch 280 - Gen Loss: 9.9778, GAN: 2.4587, L1: 0.0752, Disc: 0.3507\n  [Train] Batch 290 - Gen Loss: 8.7012, GAN: 1.4023, L1: 0.0730, Disc: 0.5938\n  [Train] Batch 300 - Gen Loss: 9.4835, GAN: 1.6337, L1: 0.0785, Disc: 0.4271\n  [Train] Batch 310 - Gen Loss: 7.9858, GAN: 1.2941, L1: 0.0669, Disc: 0.6537\n  [Train] Batch 320 - Gen Loss: 10.2688, GAN: 2.7385, L1: 0.0753, Disc: 0.7050\n  [Train] Batch 330 - Gen Loss: 11.8322, GAN: 4.6912, L1: 0.0714, Disc: 8.4914\n  [Train] Batch 340 - Gen Loss: 7.8943, GAN: 1.1627, L1: 0.0673, Disc: 1.9014\n  [Train] Batch 350 - Gen Loss: 9.7491, GAN: 1.4662, L1: 0.0828, Disc: 2.1261\n  [Train] Batch 360 - Gen Loss: 8.8016, GAN: 1.2068, L1: 0.0759, Disc: 1.2736\n  [Train] Batch 370 - Gen Loss: 9.6203, GAN: 1.1695, L1: 0.0845, Disc: 1.1016\n  [Train] Batch 380 - Gen Loss: 8.6877, GAN: 2.4213, L1: 0.0627, Disc: 1.8851\n  [Train] Batch 390 - Gen Loss: 6.5841, GAN: 0.4958, L1: 0.0609, Disc: 1.8011\n  [Train] Batch 400 - Gen Loss: 7.5404, GAN: 2.4607, L1: 0.0508, Disc: 2.2146\n  [Train] Batch 410 - Gen Loss: 7.1973, GAN: 1.3738, L1: 0.0582, Disc: 0.7808\n  [Val] MAE: 0.1850, RMSE: 0.3182, SSIM: 0.6226\nEpoch 39 completed in 131.00 seconds.\n\nEpoch 40/150\n  [Train] Batch 000 - Gen Loss: 8.2356, GAN: 1.0944, L1: 0.0714, Disc: 0.9142\n  [Train] Batch 010 - Gen Loss: 7.7682, GAN: 1.2370, L1: 0.0653, Disc: 1.0616\n  [Train] Batch 020 - Gen Loss: 8.5428, GAN: 0.9776, L1: 0.0757, Disc: 0.9384\n  [Train] Batch 030 - Gen Loss: 8.1846, GAN: 1.0885, L1: 0.0710, Disc: 0.8236\n  [Train] Batch 040 - Gen Loss: 8.8408, GAN: 1.3265, L1: 0.0751, Disc: 0.7318\n  [Train] Batch 050 - Gen Loss: 8.2392, GAN: 0.8258, L1: 0.0741, Disc: 0.7954\n  [Train] Batch 060 - Gen Loss: 8.0617, GAN: 1.1858, L1: 0.0688, Disc: 0.6269\n  [Train] Batch 070 - Gen Loss: 9.7683, GAN: 2.0965, L1: 0.0767, Disc: 0.5615\n  [Train] Batch 080 - Gen Loss: 8.6311, GAN: 2.0300, L1: 0.0660, Disc: 0.4738\n  [Train] Batch 090 - Gen Loss: 7.7362, GAN: 1.9082, L1: 0.0583, Disc: 0.8245\n  [Train] Batch 100 - Gen Loss: 6.1455, GAN: 0.9074, L1: 0.0524, Disc: 0.7317\n  [Train] Batch 110 - Gen Loss: 8.4337, GAN: 2.2595, L1: 0.0617, Disc: 0.7622\n  [Train] Batch 120 - Gen Loss: 8.7133, GAN: 2.1336, L1: 0.0658, Disc: 0.4761\n  [Train] Batch 130 - Gen Loss: 7.8651, GAN: 1.5922, L1: 0.0627, Disc: 0.7597\n  [Train] Batch 140 - Gen Loss: 8.6276, GAN: 1.5042, L1: 0.0712, Disc: 0.5111\n  [Train] Batch 150 - Gen Loss: 9.2680, GAN: 1.5994, L1: 0.0767, Disc: 0.5853\n  [Train] Batch 160 - Gen Loss: 8.4038, GAN: 1.5905, L1: 0.0681, Disc: 0.4942\n  [Train] Batch 170 - Gen Loss: 9.8169, GAN: 2.2546, L1: 0.0756, Disc: 0.5990\n  [Train] Batch 180 - Gen Loss: 9.4071, GAN: 1.8365, L1: 0.0757, Disc: 0.4277\n  [Train] Batch 190 - Gen Loss: 8.1617, GAN: 1.8473, L1: 0.0631, Disc: 0.6319\n  [Train] Batch 200 - Gen Loss: 7.7659, GAN: 0.9741, L1: 0.0679, Disc: 0.6890\n  [Train] Batch 210 - Gen Loss: 8.9690, GAN: 2.6260, L1: 0.0634, Disc: 1.0044\n  [Train] Batch 220 - Gen Loss: 9.9581, GAN: 1.9391, L1: 0.0802, Disc: 0.4008\n  [Train] Batch 230 - Gen Loss: 9.3719, GAN: 1.5048, L1: 0.0787, Disc: 0.5790\n  [Train] Batch 240 - Gen Loss: 9.2017, GAN: 1.5013, L1: 0.0770, Disc: 0.5359\n  [Train] Batch 250 - Gen Loss: 7.8036, GAN: 0.7006, L1: 0.0710, Disc: 1.0918\n  [Train] Batch 260 - Gen Loss: 9.3304, GAN: 1.4277, L1: 0.0790, Disc: 0.4998\n  [Train] Batch 270 - Gen Loss: 11.2129, GAN: 3.0389, L1: 0.0817, Disc: 0.5170\n  [Train] Batch 280 - Gen Loss: 7.7422, GAN: 0.5881, L1: 0.0715, Disc: 1.5767\n  [Train] Batch 290 - Gen Loss: 8.3655, GAN: 1.6296, L1: 0.0674, Disc: 0.6273\n  [Train] Batch 300 - Gen Loss: 9.0439, GAN: 1.1806, L1: 0.0786, Disc: 0.6214\n  [Train] Batch 310 - Gen Loss: 9.7692, GAN: 2.7912, L1: 0.0698, Disc: 0.8622\n  [Train] Batch 320 - Gen Loss: 10.3310, GAN: 1.9363, L1: 0.0839, Disc: 0.4020\n  [Train] Batch 330 - Gen Loss: 9.5161, GAN: 2.6138, L1: 0.0690, Disc: 0.8089\n  [Train] Batch 340 - Gen Loss: 8.9702, GAN: 1.9888, L1: 0.0698, Disc: 0.5127\n  [Train] Batch 350 - Gen Loss: 9.9098, GAN: 2.7524, L1: 0.0716, Disc: 0.5546\n  [Train] Batch 360 - Gen Loss: 10.6406, GAN: 3.2210, L1: 0.0742, Disc: 0.2663\n  [Train] Batch 370 - Gen Loss: 10.8844, GAN: 3.7140, L1: 0.0717, Disc: 0.5597\n  [Train] Batch 380 - Gen Loss: 10.9016, GAN: 4.2507, L1: 0.0665, Disc: 1.4089\n  [Train] Batch 390 - Gen Loss: 8.7258, GAN: 2.4264, L1: 0.0630, Disc: 5.8067\n  [Train] Batch 400 - Gen Loss: 6.2320, GAN: 1.4574, L1: 0.0477, Disc: 2.1974\n  [Train] Batch 410 - Gen Loss: 7.6535, GAN: 1.4780, L1: 0.0618, Disc: 1.8416\n  [Val] MAE: 0.1871, RMSE: 0.3237, SSIM: 0.6236\nCheckpoint saved: /kaggle/working/outputs/checkpoints/ckpt_epoch_40\nEpoch 40 completed in 133.56 seconds.\n\nEpoch 41/150\n  [Train] Batch 000 - Gen Loss: 7.5758, GAN: 1.0840, L1: 0.0649, Disc: 1.2495\n  [Train] Batch 010 - Gen Loss: 8.6845, GAN: 0.7678, L1: 0.0792, Disc: 1.2793\n  [Train] Batch 020 - Gen Loss: 8.3948, GAN: 0.6041, L1: 0.0779, Disc: 1.5767\n  [Train] Batch 030 - Gen Loss: 8.8844, GAN: 0.8352, L1: 0.0805, Disc: 1.7417\n  [Train] Batch 040 - Gen Loss: 8.9001, GAN: 1.5187, L1: 0.0738, Disc: 1.1174\n  [Train] Batch 050 - Gen Loss: 6.1365, GAN: 0.2239, L1: 0.0591, Disc: 1.9895\n  [Train] Batch 060 - Gen Loss: 8.5519, GAN: 2.4964, L1: 0.0606, Disc: 1.4881\n  [Train] Batch 070 - Gen Loss: 8.2336, GAN: 0.6855, L1: 0.0755, Disc: 0.9568\n  [Train] Batch 080 - Gen Loss: 7.1364, GAN: 0.5946, L1: 0.0654, Disc: 1.0779\n  [Train] Batch 090 - Gen Loss: 6.3930, GAN: 0.8032, L1: 0.0559, Disc: 0.8856\n  [Train] Batch 100 - Gen Loss: 9.2995, GAN: 2.8455, L1: 0.0645, Disc: 1.1176\n  [Train] Batch 110 - Gen Loss: 8.6586, GAN: 1.7602, L1: 0.0690, Disc: 0.7385\n  [Train] Batch 120 - Gen Loss: 7.4933, GAN: 1.1469, L1: 0.0635, Disc: 0.7451\n  [Train] Batch 130 - Gen Loss: 8.2515, GAN: 1.2870, L1: 0.0696, Disc: 0.7866\n  [Train] Batch 140 - Gen Loss: 9.0412, GAN: 0.9001, L1: 0.0814, Disc: 0.8018\n  [Train] Batch 150 - Gen Loss: 8.5114, GAN: 1.7036, L1: 0.0681, Disc: 0.6033\n  [Train] Batch 160 - Gen Loss: 9.7276, GAN: 1.9513, L1: 0.0778, Disc: 0.4430\n  [Train] Batch 170 - Gen Loss: 9.8412, GAN: 2.4865, L1: 0.0735, Disc: 0.4999\n  [Train] Batch 180 - Gen Loss: 8.8921, GAN: 1.9282, L1: 0.0696, Disc: 0.3600\n  [Train] Batch 190 - Gen Loss: 7.4171, GAN: 1.0232, L1: 0.0639, Disc: 0.8412\n  [Train] Batch 200 - Gen Loss: 6.9610, GAN: 0.7233, L1: 0.0624, Disc: 0.9314\n  [Train] Batch 210 - Gen Loss: 9.0593, GAN: 2.2925, L1: 0.0677, Disc: 0.4651\n  [Train] Batch 220 - Gen Loss: 8.5462, GAN: 1.8250, L1: 0.0672, Disc: 0.5388\n  [Train] Batch 230 - Gen Loss: 11.3629, GAN: 3.4959, L1: 0.0787, Disc: 0.8936\n  [Train] Batch 240 - Gen Loss: 10.8374, GAN: 2.3454, L1: 0.0849, Disc: 0.6183\n  [Train] Batch 250 - Gen Loss: 7.5135, GAN: 0.9048, L1: 0.0661, Disc: 0.8835\n  [Train] Batch 260 - Gen Loss: 11.9899, GAN: 2.3862, L1: 0.0960, Disc: 0.6667\n  [Train] Batch 270 - Gen Loss: 10.7502, GAN: 3.3351, L1: 0.0742, Disc: 0.8058\n  [Train] Batch 280 - Gen Loss: 8.9285, GAN: 1.3232, L1: 0.0761, Disc: 0.6669\n  [Train] Batch 290 - Gen Loss: 8.0876, GAN: 1.2730, L1: 0.0681, Disc: 0.5634\n  [Train] Batch 300 - Gen Loss: 8.7927, GAN: 0.6257, L1: 0.0817, Disc: 1.0098\n  [Train] Batch 310 - Gen Loss: 9.1142, GAN: 2.1842, L1: 0.0693, Disc: 0.6409\n  [Train] Batch 320 - Gen Loss: 6.9366, GAN: 0.6926, L1: 0.0624, Disc: 0.9222\n  [Train] Batch 330 - Gen Loss: 10.5904, GAN: 4.3203, L1: 0.0627, Disc: 3.2906\n  [Train] Batch 340 - Gen Loss: 11.8770, GAN: 5.0607, L1: 0.0682, Disc: 1.0573\n  [Train] Batch 350 - Gen Loss: 8.9228, GAN: 2.2762, L1: 0.0665, Disc: 1.4199\n  [Train] Batch 360 - Gen Loss: 8.1367, GAN: 1.1380, L1: 0.0700, Disc: 0.6306\n  [Train] Batch 370 - Gen Loss: 8.8351, GAN: 1.3881, L1: 0.0745, Disc: 0.6045\n  [Train] Batch 380 - Gen Loss: 7.3006, GAN: 0.8858, L1: 0.0641, Disc: 0.7967\n  [Train] Batch 390 - Gen Loss: 7.7493, GAN: 2.1577, L1: 0.0559, Disc: 0.9717\n  [Train] Batch 400 - Gen Loss: 6.1936, GAN: 0.6142, L1: 0.0558, Disc: 1.1094\n  [Train] Batch 410 - Gen Loss: 7.3388, GAN: 1.4908, L1: 0.0585, Disc: 0.5991\n  [Val] MAE: 0.1866, RMSE: 0.3220, SSIM: 0.6108\nEpoch 41 completed in 131.36 seconds.\n\nEpoch 42/150\n  [Train] Batch 000 - Gen Loss: 7.3749, GAN: 0.9759, L1: 0.0640, Disc: 0.7363\n  [Train] Batch 010 - Gen Loss: 8.3212, GAN: 0.9052, L1: 0.0742, Disc: 0.7338\n  [Train] Batch 020 - Gen Loss: 10.0343, GAN: 2.3939, L1: 0.0764, Disc: 0.8264\n  [Train] Batch 030 - Gen Loss: 10.2962, GAN: 2.9603, L1: 0.0734, Disc: 0.5063\n  [Train] Batch 040 - Gen Loss: 10.2186, GAN: 2.3625, L1: 0.0786, Disc: 0.6735\n  [Train] Batch 050 - Gen Loss: 8.9208, GAN: 1.5706, L1: 0.0735, Disc: 0.7438\n  [Train] Batch 060 - Gen Loss: 8.3566, GAN: 1.2125, L1: 0.0714, Disc: 0.6316\n  [Train] Batch 070 - Gen Loss: 7.8708, GAN: 1.3477, L1: 0.0652, Disc: 0.7775\n  [Train] Batch 080 - Gen Loss: 7.3575, GAN: 0.8995, L1: 0.0646, Disc: 0.8466\n  [Train] Batch 090 - Gen Loss: 7.2395, GAN: 1.5850, L1: 0.0565, Disc: 0.8423\n  [Train] Batch 100 - Gen Loss: 6.9216, GAN: 0.6572, L1: 0.0626, Disc: 1.0070\n  [Train] Batch 110 - Gen Loss: 7.3193, GAN: 0.9131, L1: 0.0641, Disc: 0.9153\n  [Train] Batch 120 - Gen Loss: 6.8179, GAN: 1.2333, L1: 0.0558, Disc: 0.8028\n  [Train] Batch 130 - Gen Loss: 7.5657, GAN: 1.2500, L1: 0.0632, Disc: 0.9747\n  [Train] Batch 140 - Gen Loss: 10.4126, GAN: 1.7885, L1: 0.0862, Disc: 0.4675\n  [Train] Batch 150 - Gen Loss: 8.8577, GAN: 1.7913, L1: 0.0707, Disc: 0.6429\n  [Train] Batch 160 - Gen Loss: 9.1817, GAN: 1.6897, L1: 0.0749, Disc: 0.5628\n  [Train] Batch 170 - Gen Loss: 7.6941, GAN: 1.2762, L1: 0.0642, Disc: 0.7004\n  [Train] Batch 180 - Gen Loss: 9.1228, GAN: 2.4049, L1: 0.0672, Disc: 0.4998\n  [Train] Batch 190 - Gen Loss: 7.8128, GAN: 1.4301, L1: 0.0638, Disc: 0.7286\n  [Train] Batch 200 - Gen Loss: 8.1209, GAN: 1.3254, L1: 0.0680, Disc: 0.5700\n  [Train] Batch 210 - Gen Loss: 10.3710, GAN: 3.0495, L1: 0.0732, Disc: 0.6544\n  [Train] Batch 220 - Gen Loss: 8.4578, GAN: 2.0024, L1: 0.0646, Disc: 0.4554\n  [Train] Batch 230 - Gen Loss: 9.1205, GAN: 1.6438, L1: 0.0748, Disc: 0.5311\n  [Train] Batch 240 - Gen Loss: 8.8987, GAN: 0.9988, L1: 0.0790, Disc: 0.7062\n  [Train] Batch 250 - Gen Loss: 7.3077, GAN: 0.7334, L1: 0.0657, Disc: 0.8781\n  [Train] Batch 260 - Gen Loss: 8.2162, GAN: 0.2595, L1: 0.0796, Disc: 1.8959\n  [Train] Batch 270 - Gen Loss: 10.6273, GAN: 3.1024, L1: 0.0752, Disc: 0.5908\n  [Train] Batch 280 - Gen Loss: 8.6354, GAN: 1.4593, L1: 0.0718, Disc: 0.6289\n  [Train] Batch 290 - Gen Loss: 8.8095, GAN: 2.0315, L1: 0.0678, Disc: 0.3591\n  [Train] Batch 300 - Gen Loss: 9.8055, GAN: 2.3404, L1: 0.0747, Disc: 1.2031\n  [Train] Batch 310 - Gen Loss: 8.1539, GAN: 1.1276, L1: 0.0703, Disc: 0.7517\n  [Train] Batch 320 - Gen Loss: 8.5566, GAN: 1.7896, L1: 0.0677, Disc: 0.5541\n  [Train] Batch 330 - Gen Loss: 8.2484, GAN: 1.2167, L1: 0.0703, Disc: 0.5630\n  [Train] Batch 340 - Gen Loss: 8.9525, GAN: 1.6417, L1: 0.0731, Disc: 0.4090\n  [Train] Batch 350 - Gen Loss: 9.0723, GAN: 2.6483, L1: 0.0642, Disc: 0.2943\n  [Train] Batch 360 - Gen Loss: 7.9203, GAN: 0.9997, L1: 0.0692, Disc: 0.6455\n  [Train] Batch 370 - Gen Loss: 9.1795, GAN: 1.7443, L1: 0.0744, Disc: 0.7775\n  [Train] Batch 380 - Gen Loss: 7.6479, GAN: 1.5741, L1: 0.0607, Disc: 0.5546\n  [Train] Batch 390 - Gen Loss: 5.7285, GAN: 0.1633, L1: 0.0557, Disc: 2.3382\n  [Train] Batch 400 - Gen Loss: 6.8575, GAN: 0.9380, L1: 0.0592, Disc: 0.9262\n  [Train] Batch 410 - Gen Loss: 8.3553, GAN: 1.9733, L1: 0.0638, Disc: 0.6351\n  [Val] MAE: 0.1854, RMSE: 0.3207, SSIM: 0.6106\nEpoch 42 completed in 131.39 seconds.\n\nEpoch 43/150\n  [Train] Batch 000 - Gen Loss: 8.0297, GAN: 0.7733, L1: 0.0726, Disc: 1.0079\n  [Train] Batch 010 - Gen Loss: 9.1919, GAN: 1.8806, L1: 0.0731, Disc: 1.1871\n  [Train] Batch 020 - Gen Loss: 9.3779, GAN: 1.4097, L1: 0.0797, Disc: 1.2821\n  [Train] Batch 030 - Gen Loss: 8.6195, GAN: 1.2843, L1: 0.0734, Disc: 0.7132\n  [Train] Batch 040 - Gen Loss: 7.9075, GAN: 1.1159, L1: 0.0679, Disc: 0.7102\n  [Train] Batch 050 - Gen Loss: 8.2154, GAN: 1.7353, L1: 0.0648, Disc: 0.5188\n  [Train] Batch 060 - Gen Loss: 8.7978, GAN: 2.2971, L1: 0.0650, Disc: 0.7488\n  [Train] Batch 070 - Gen Loss: 8.7020, GAN: 2.1976, L1: 0.0650, Disc: 0.4873\n  [Train] Batch 080 - Gen Loss: 9.0286, GAN: 2.3292, L1: 0.0670, Disc: 1.2215\n  [Train] Batch 090 - Gen Loss: 8.2431, GAN: 1.8894, L1: 0.0635, Disc: 0.4566\n  [Train] Batch 100 - Gen Loss: 7.4667, GAN: 1.3425, L1: 0.0612, Disc: 0.7203\n  [Train] Batch 110 - Gen Loss: 7.9299, GAN: 2.0006, L1: 0.0593, Disc: 0.4848\n  [Train] Batch 120 - Gen Loss: 10.3698, GAN: 2.7709, L1: 0.0760, Disc: 0.2664\n  [Train] Batch 130 - Gen Loss: 7.8367, GAN: 1.7481, L1: 0.0609, Disc: 0.6195\n  [Train] Batch 140 - Gen Loss: 9.0068, GAN: 2.2526, L1: 0.0675, Disc: 0.7352\n  [Train] Batch 150 - Gen Loss: 8.5327, GAN: 0.8545, L1: 0.0768, Disc: 0.7602\n  [Train] Batch 160 - Gen Loss: 10.0490, GAN: 1.5639, L1: 0.0849, Disc: 0.4904\n  [Train] Batch 170 - Gen Loss: 8.5210, GAN: 1.8905, L1: 0.0663, Disc: 0.3323\n  [Train] Batch 180 - Gen Loss: 9.9917, GAN: 2.5832, L1: 0.0741, Disc: 0.4488\n  [Train] Batch 190 - Gen Loss: 8.9808, GAN: 2.0341, L1: 0.0695, Disc: 0.5398\n  [Train] Batch 200 - Gen Loss: 11.6001, GAN: 4.1240, L1: 0.0748, Disc: 0.8952\n  [Train] Batch 210 - Gen Loss: 8.2619, GAN: 1.3471, L1: 0.0691, Disc: 0.4907\n  [Train] Batch 220 - Gen Loss: 8.8965, GAN: 2.1606, L1: 0.0674, Disc: 0.4335\n  [Train] Batch 230 - Gen Loss: 9.0217, GAN: 1.6714, L1: 0.0735, Disc: 0.5671\n  [Train] Batch 240 - Gen Loss: 9.1126, GAN: 1.8605, L1: 0.0725, Disc: 0.5357\n  [Train] Batch 250 - Gen Loss: 8.4892, GAN: 1.7337, L1: 0.0676, Disc: 0.4023\n  [Train] Batch 260 - Gen Loss: 12.8119, GAN: 5.1463, L1: 0.0767, Disc: 1.1252\n  [Train] Batch 270 - Gen Loss: 10.1671, GAN: 1.6510, L1: 0.0852, Disc: 0.5006\n  [Train] Batch 280 - Gen Loss: 9.2907, GAN: 1.1095, L1: 0.0818, Disc: 0.6691\n  [Train] Batch 290 - Gen Loss: 8.0489, GAN: 1.1241, L1: 0.0692, Disc: 0.6205\n  [Train] Batch 300 - Gen Loss: 7.7613, GAN: 0.8033, L1: 0.0696, Disc: 0.8385\n  [Train] Batch 310 - Gen Loss: 7.4429, GAN: 0.5483, L1: 0.0689, Disc: 1.2821\n  [Train] Batch 320 - Gen Loss: 9.4076, GAN: 2.6406, L1: 0.0677, Disc: 0.8135\n  [Train] Batch 330 - Gen Loss: 7.7868, GAN: 1.6388, L1: 0.0615, Disc: 0.6719\n  [Train] Batch 340 - Gen Loss: 10.2038, GAN: 3.3044, L1: 0.0690, Disc: 0.9731\n  [Train] Batch 350 - Gen Loss: 12.0088, GAN: 4.5556, L1: 0.0745, Disc: 1.1467\n  [Train] Batch 360 - Gen Loss: 9.1617, GAN: 1.5936, L1: 0.0757, Disc: 0.4989\n  [Train] Batch 370 - Gen Loss: 9.4411, GAN: 2.4440, L1: 0.0700, Disc: 0.8623\n  [Train] Batch 380 - Gen Loss: 7.9507, GAN: 1.1055, L1: 0.0685, Disc: 0.7016\n  [Train] Batch 390 - Gen Loss: 6.3226, GAN: 0.9769, L1: 0.0535, Disc: 1.0370\n  [Train] Batch 400 - Gen Loss: 7.1417, GAN: 0.7396, L1: 0.0640, Disc: 0.9999\n  [Train] Batch 410 - Gen Loss: 7.9305, GAN: 1.7315, L1: 0.0620, Disc: 0.5916\n  [Val] MAE: 0.1866, RMSE: 0.3227, SSIM: 0.6198\nEpoch 43 completed in 131.32 seconds.\n\nEpoch 44/150\n  [Train] Batch 000 - Gen Loss: 8.5891, GAN: 1.4191, L1: 0.0717, Disc: 0.5262\n  [Train] Batch 010 - Gen Loss: 9.9073, GAN: 2.6438, L1: 0.0726, Disc: 0.5028\n  [Train] Batch 020 - Gen Loss: 8.8411, GAN: 1.2770, L1: 0.0756, Disc: 0.5412\n  [Train] Batch 030 - Gen Loss: 8.5080, GAN: 0.7979, L1: 0.0771, Disc: 0.8717\n  [Train] Batch 040 - Gen Loss: 9.7277, GAN: 2.3191, L1: 0.0741, Disc: 0.6849\n  [Train] Batch 050 - Gen Loss: 9.0432, GAN: 2.3355, L1: 0.0671, Disc: 1.1122\n  [Train] Batch 060 - Gen Loss: 8.6592, GAN: 2.0269, L1: 0.0663, Disc: 1.5052\n  [Train] Batch 070 - Gen Loss: 7.1545, GAN: 0.4035, L1: 0.0675, Disc: 1.4148\n  [Train] Batch 080 - Gen Loss: 7.0691, GAN: 0.9915, L1: 0.0608, Disc: 0.8239\n  [Train] Batch 090 - Gen Loss: 7.6465, GAN: 1.9523, L1: 0.0569, Disc: 1.1883\n  [Train] Batch 100 - Gen Loss: 5.1296, GAN: 0.4296, L1: 0.0470, Disc: 1.4384\n  [Train] Batch 110 - Gen Loss: 8.4487, GAN: 2.4498, L1: 0.0600, Disc: 1.0446\n  [Train] Batch 120 - Gen Loss: 7.5351, GAN: 1.3164, L1: 0.0622, Disc: 0.6270\n  [Train] Batch 130 - Gen Loss: 6.8135, GAN: 0.7020, L1: 0.0611, Disc: 0.9710\n  [Train] Batch 140 - Gen Loss: 11.0719, GAN: 2.7463, L1: 0.0833, Disc: 0.3196\n  [Train] Batch 150 - Gen Loss: 9.2177, GAN: 1.8103, L1: 0.0741, Disc: 0.4766\n  [Train] Batch 160 - Gen Loss: 10.8207, GAN: 3.4734, L1: 0.0735, Disc: 0.3426\n  [Train] Batch 170 - Gen Loss: 8.5819, GAN: 1.9992, L1: 0.0658, Disc: 0.3930\n  [Train] Batch 180 - Gen Loss: 7.5850, GAN: 1.3766, L1: 0.0621, Disc: 0.6187\n  [Train] Batch 190 - Gen Loss: 10.3451, GAN: 3.4582, L1: 0.0689, Disc: 1.1979\n  [Train] Batch 200 - Gen Loss: 9.5045, GAN: 2.0880, L1: 0.0742, Disc: 0.3715\n  [Train] Batch 210 - Gen Loss: 9.2467, GAN: 2.8616, L1: 0.0639, Disc: 0.4732\n  [Train] Batch 220 - Gen Loss: 8.8951, GAN: 1.8230, L1: 0.0707, Disc: 0.5794\n  [Train] Batch 230 - Gen Loss: 8.5483, GAN: 0.7049, L1: 0.0784, Disc: 0.9931\n  [Train] Batch 240 - Gen Loss: 9.4109, GAN: 2.8642, L1: 0.0655, Disc: 1.0325\n  [Train] Batch 250 - Gen Loss: 8.6565, GAN: 1.2939, L1: 0.0736, Disc: 0.5016\n  [Train] Batch 260 - Gen Loss: 9.1646, GAN: 2.2516, L1: 0.0691, Disc: 0.3272\n  [Train] Batch 270 - Gen Loss: 10.3205, GAN: 2.3615, L1: 0.0796, Disc: 0.3483\n  [Train] Batch 280 - Gen Loss: 10.6456, GAN: 3.7099, L1: 0.0694, Disc: 1.5785\n  [Train] Batch 290 - Gen Loss: 9.5308, GAN: 1.7771, L1: 0.0775, Disc: 1.8994\n  [Train] Batch 300 - Gen Loss: 9.6234, GAN: 2.9173, L1: 0.0671, Disc: 0.3864\n  [Train] Batch 310 - Gen Loss: 9.2584, GAN: 1.9895, L1: 0.0727, Disc: 0.6638\n  [Train] Batch 320 - Gen Loss: 7.6778, GAN: 0.5753, L1: 0.0710, Disc: 1.2186\n  [Train] Batch 330 - Gen Loss: 7.5479, GAN: 1.3352, L1: 0.0621, Disc: 1.0890\n  [Train] Batch 340 - Gen Loss: 10.4790, GAN: 3.3420, L1: 0.0714, Disc: 1.9410\n  [Train] Batch 350 - Gen Loss: 8.3108, GAN: 0.1204, L1: 0.0819, Disc: 2.6913\n  [Train] Batch 360 - Gen Loss: 9.2583, GAN: 2.6334, L1: 0.0662, Disc: 0.7748\n  [Train] Batch 370 - Gen Loss: 8.0846, GAN: 1.4753, L1: 0.0661, Disc: 0.6912\n  [Train] Batch 380 - Gen Loss: 7.0695, GAN: 0.7859, L1: 0.0628, Disc: 0.9194\n  [Train] Batch 390 - Gen Loss: 7.1836, GAN: 2.0133, L1: 0.0517, Disc: 1.2362\n  [Train] Batch 400 - Gen Loss: 6.6987, GAN: 0.6607, L1: 0.0604, Disc: 1.0987\n  [Train] Batch 410 - Gen Loss: 8.9057, GAN: 3.3064, L1: 0.0560, Disc: 0.3340\n  [Val] MAE: 0.1859, RMSE: 0.3217, SSIM: 0.6161\nEpoch 44 completed in 130.90 seconds.\n\nEpoch 45/150\n  [Train] Batch 000 - Gen Loss: 8.4439, GAN: 1.4188, L1: 0.0703, Disc: 0.7843\n  [Train] Batch 010 - Gen Loss: 7.9307, GAN: 1.9578, L1: 0.0597, Disc: 0.6887\n  [Train] Batch 020 - Gen Loss: 9.8589, GAN: 1.9441, L1: 0.0791, Disc: 0.4221\n  [Train] Batch 030 - Gen Loss: 6.5669, GAN: 0.7357, L1: 0.0583, Disc: 1.0003\n  [Train] Batch 040 - Gen Loss: 9.2365, GAN: 2.2638, L1: 0.0697, Disc: 0.4305\n  [Train] Batch 050 - Gen Loss: 9.8752, GAN: 2.0378, L1: 0.0784, Disc: 0.2604\n  [Train] Batch 060 - Gen Loss: 8.2729, GAN: 1.5431, L1: 0.0673, Disc: 0.4808\n  [Train] Batch 070 - Gen Loss: 8.3135, GAN: 1.4110, L1: 0.0690, Disc: 0.5495\n  [Train] Batch 080 - Gen Loss: 7.1960, GAN: 0.9615, L1: 0.0623, Disc: 0.7647\n  [Train] Batch 090 - Gen Loss: 7.3535, GAN: 1.9582, L1: 0.0540, Disc: 1.4508\n  [Train] Batch 100 - Gen Loss: 7.3283, GAN: 0.7750, L1: 0.0655, Disc: 0.9066\n  [Train] Batch 110 - Gen Loss: 6.4984, GAN: 1.2730, L1: 0.0523, Disc: 0.6257\n  [Train] Batch 120 - Gen Loss: 9.7740, GAN: 2.8928, L1: 0.0688, Disc: 1.1125\n  [Train] Batch 130 - Gen Loss: 7.8752, GAN: 1.1568, L1: 0.0672, Disc: 0.6859\n  [Train] Batch 140 - Gen Loss: 8.3115, GAN: 0.9652, L1: 0.0735, Disc: 0.6748\n  [Train] Batch 150 - Gen Loss: 8.4683, GAN: 1.6325, L1: 0.0684, Disc: 0.5754\n  [Train] Batch 160 - Gen Loss: 8.9583, GAN: 1.7366, L1: 0.0722, Disc: 0.6373\n  [Train] Batch 170 - Gen Loss: 9.7951, GAN: 2.6377, L1: 0.0716, Disc: 0.6004\n  [Train] Batch 180 - Gen Loss: 9.3586, GAN: 2.3512, L1: 0.0701, Disc: 0.3002\n  [Train] Batch 190 - Gen Loss: 8.9270, GAN: 3.0582, L1: 0.0587, Disc: 0.3212\n  [Train] Batch 200 - Gen Loss: 7.5430, GAN: 1.1600, L1: 0.0638, Disc: 0.6656\n  [Train] Batch 210 - Gen Loss: 8.3239, GAN: 1.7473, L1: 0.0658, Disc: 0.5053\n  [Train] Batch 220 - Gen Loss: 10.4761, GAN: 2.5846, L1: 0.0789, Disc: 0.7120\n  [Train] Batch 230 - Gen Loss: 10.0563, GAN: 2.3905, L1: 0.0767, Disc: 0.4876\n  [Train] Batch 240 - Gen Loss: 8.8746, GAN: 2.0522, L1: 0.0682, Disc: 0.5513\n  [Train] Batch 250 - Gen Loss: 10.3159, GAN: 3.8935, L1: 0.0642, Disc: 1.6099\n  [Train] Batch 260 - Gen Loss: 9.6455, GAN: 1.4680, L1: 0.0818, Disc: 0.4788\n  [Train] Batch 270 - Gen Loss: 8.7494, GAN: 1.8120, L1: 0.0694, Disc: 0.7434\n  [Train] Batch 280 - Gen Loss: 7.8860, GAN: 0.5507, L1: 0.0734, Disc: 1.1908\n  [Train] Batch 290 - Gen Loss: 11.0253, GAN: 4.4961, L1: 0.0653, Disc: 6.3875\n  [Train] Batch 300 - Gen Loss: 7.5023, GAN: 1.1690, L1: 0.0633, Disc: 1.3573\n  [Train] Batch 310 - Gen Loss: 7.9225, GAN: 0.9080, L1: 0.0701, Disc: 0.9500\n  [Train] Batch 320 - Gen Loss: 9.0053, GAN: 1.7256, L1: 0.0728, Disc: 0.8636\n  [Train] Batch 330 - Gen Loss: 7.7813, GAN: 0.9843, L1: 0.0680, Disc: 1.1063\n  [Train] Batch 340 - Gen Loss: 9.4720, GAN: 1.6216, L1: 0.0785, Disc: 0.5216\n  [Train] Batch 350 - Gen Loss: 8.4183, GAN: 1.5111, L1: 0.0691, Disc: 0.5073\n  [Train] Batch 360 - Gen Loss: 9.8238, GAN: 2.1297, L1: 0.0769, Disc: 0.2896\n  [Train] Batch 370 - Gen Loss: 10.2507, GAN: 2.8560, L1: 0.0739, Disc: 1.1822\n  [Train] Batch 380 - Gen Loss: 6.4716, GAN: 0.6186, L1: 0.0585, Disc: 1.0960\n  [Train] Batch 390 - Gen Loss: 6.5396, GAN: 0.4030, L1: 0.0614, Disc: 1.3986\n  [Train] Batch 400 - Gen Loss: 5.4307, GAN: 0.2373, L1: 0.0519, Disc: 1.9745\n  [Train] Batch 410 - Gen Loss: 7.8710, GAN: 2.1694, L1: 0.0570, Disc: 1.0966\n  [Val] MAE: 0.1857, RMSE: 0.3200, SSIM: 0.6138\nEpoch 45 completed in 130.96 seconds.\n\nEpoch 46/150\n  [Train] Batch 000 - Gen Loss: 8.9728, GAN: 2.1169, L1: 0.0686, Disc: 0.7036\n  [Train] Batch 010 - Gen Loss: 8.0538, GAN: 1.5452, L1: 0.0651, Disc: 0.5842\n  [Train] Batch 020 - Gen Loss: 8.4339, GAN: 1.9299, L1: 0.0650, Disc: 0.8405\n  [Train] Batch 030 - Gen Loss: 8.2483, GAN: 1.5034, L1: 0.0674, Disc: 0.5508\n  [Train] Batch 040 - Gen Loss: 8.8027, GAN: 1.0546, L1: 0.0775, Disc: 0.6811\n  [Train] Batch 050 - Gen Loss: 9.6307, GAN: 2.8682, L1: 0.0676, Disc: 0.5197\n  [Train] Batch 060 - Gen Loss: 8.7114, GAN: 1.2003, L1: 0.0751, Disc: 0.6113\n  [Train] Batch 070 - Gen Loss: 10.0237, GAN: 2.7126, L1: 0.0731, Disc: 0.3493\n  [Train] Batch 080 - Gen Loss: 6.8269, GAN: 1.1283, L1: 0.0570, Disc: 0.7469\n  [Train] Batch 090 - Gen Loss: 6.7957, GAN: 1.2784, L1: 0.0552, Disc: 1.1839\n  [Train] Batch 100 - Gen Loss: 8.1343, GAN: 2.1707, L1: 0.0596, Disc: 0.5731\n  [Train] Batch 110 - Gen Loss: 7.9738, GAN: 2.0536, L1: 0.0592, Disc: 0.5193\n  [Train] Batch 120 - Gen Loss: 9.6720, GAN: 2.6019, L1: 0.0707, Disc: 0.3308\n  [Train] Batch 130 - Gen Loss: 8.5183, GAN: 2.0362, L1: 0.0648, Disc: 0.4550\n  [Train] Batch 140 - Gen Loss: 7.8406, GAN: 1.0627, L1: 0.0678, Disc: 0.6571\n  [Train] Batch 150 - Gen Loss: 9.4320, GAN: 2.0075, L1: 0.0742, Disc: 0.5633\n  [Train] Batch 160 - Gen Loss: 8.9255, GAN: 2.6174, L1: 0.0631, Disc: 1.0362\n  [Train] Batch 170 - Gen Loss: 9.2392, GAN: 1.8593, L1: 0.0738, Disc: 0.3680\n  [Train] Batch 180 - Gen Loss: 9.0229, GAN: 2.1152, L1: 0.0691, Disc: 0.4097\n  [Train] Batch 190 - Gen Loss: 8.0306, GAN: 1.2648, L1: 0.0677, Disc: 0.5136\n  [Train] Batch 200 - Gen Loss: 8.0659, GAN: 1.6149, L1: 0.0645, Disc: 0.6083\n  [Train] Batch 210 - Gen Loss: 10.4016, GAN: 2.9711, L1: 0.0743, Disc: 0.4500\n  [Train] Batch 220 - Gen Loss: 8.2676, GAN: 1.4247, L1: 0.0684, Disc: 0.8740\n  [Train] Batch 230 - Gen Loss: 8.7720, GAN: 1.4640, L1: 0.0731, Disc: 0.5503\n  [Train] Batch 240 - Gen Loss: 9.2923, GAN: 2.9031, L1: 0.0639, Disc: 0.5026\n  [Train] Batch 250 - Gen Loss: 9.0901, GAN: 1.3231, L1: 0.0777, Disc: 0.6177\n  [Train] Batch 260 - Gen Loss: 9.0138, GAN: 1.8149, L1: 0.0720, Disc: 0.4466\n  [Train] Batch 270 - Gen Loss: 10.6395, GAN: 3.5577, L1: 0.0708, Disc: 0.8070\n  [Train] Batch 280 - Gen Loss: 8.4572, GAN: 0.7396, L1: 0.0772, Disc: 0.9002\n  [Train] Batch 290 - Gen Loss: 9.5181, GAN: 2.2730, L1: 0.0725, Disc: 0.5639\n  [Train] Batch 300 - Gen Loss: 7.7806, GAN: 1.2993, L1: 0.0648, Disc: 0.5257\n  [Train] Batch 310 - Gen Loss: 9.2451, GAN: 2.0603, L1: 0.0718, Disc: 0.6970\n  [Train] Batch 320 - Gen Loss: 8.4114, GAN: 1.9951, L1: 0.0642, Disc: 0.7395\n  [Train] Batch 330 - Gen Loss: 9.8238, GAN: 2.3663, L1: 0.0746, Disc: 0.3634\n  [Train] Batch 340 - Gen Loss: 9.6270, GAN: 2.6043, L1: 0.0702, Disc: 0.3240\n  [Train] Batch 350 - Gen Loss: 8.1133, GAN: 1.0992, L1: 0.0701, Disc: 0.5814\n  [Train] Batch 360 - Gen Loss: 8.2404, GAN: 1.5783, L1: 0.0666, Disc: 0.4522\n  [Train] Batch 370 - Gen Loss: 6.9498, GAN: 0.4247, L1: 0.0653, Disc: 1.5036\n  [Train] Batch 380 - Gen Loss: 8.1268, GAN: 1.7121, L1: 0.0641, Disc: 4.2951\n  [Train] Batch 390 - Gen Loss: 5.9424, GAN: 1.0340, L1: 0.0491, Disc: 1.7440\n  [Train] Batch 400 - Gen Loss: 6.0675, GAN: 0.5670, L1: 0.0550, Disc: 1.3953\n  [Train] Batch 410 - Gen Loss: 6.3346, GAN: 1.2584, L1: 0.0508, Disc: 0.9676\n  [Val] MAE: 0.1874, RMSE: 0.3253, SSIM: 0.6171\nEpoch 46 completed in 130.95 seconds.\n\nEpoch 47/150\n  [Train] Batch 000 - Gen Loss: 6.7826, GAN: 0.9230, L1: 0.0586, Disc: 0.9730\n  [Train] Batch 010 - Gen Loss: 8.1763, GAN: 1.1376, L1: 0.0704, Disc: 0.7861\n  [Train] Batch 020 - Gen Loss: 8.8879, GAN: 2.2730, L1: 0.0661, Disc: 0.6894\n  [Train] Batch 030 - Gen Loss: 8.2616, GAN: 0.7650, L1: 0.0750, Disc: 0.8765\n  [Train] Batch 040 - Gen Loss: 10.0634, GAN: 2.4808, L1: 0.0758, Disc: 0.7636\n  [Train] Batch 050 - Gen Loss: 7.1198, GAN: 0.4609, L1: 0.0666, Disc: 1.3561\n  [Train] Batch 060 - Gen Loss: 6.7584, GAN: 0.6371, L1: 0.0612, Disc: 1.1230\n  [Train] Batch 070 - Gen Loss: 8.5796, GAN: 1.2210, L1: 0.0736, Disc: 0.7168\n  [Train] Batch 080 - Gen Loss: 11.1705, GAN: 3.7955, L1: 0.0737, Disc: 0.7267\n  [Train] Batch 090 - Gen Loss: 7.2063, GAN: 1.4539, L1: 0.0575, Disc: 1.0061\n  [Train] Batch 100 - Gen Loss: 6.9358, GAN: 1.6252, L1: 0.0531, Disc: 0.7789\n  [Train] Batch 110 - Gen Loss: 6.6030, GAN: 1.2644, L1: 0.0534, Disc: 0.7536\n  [Train] Batch 120 - Gen Loss: 7.3756, GAN: 0.9101, L1: 0.0647, Disc: 0.9101\n  [Train] Batch 130 - Gen Loss: 7.0901, GAN: 0.7877, L1: 0.0630, Disc: 1.0098\n  [Train] Batch 140 - Gen Loss: 9.0817, GAN: 2.3541, L1: 0.0673, Disc: 0.8122\n  [Train] Batch 150 - Gen Loss: 8.1689, GAN: 1.8904, L1: 0.0628, Disc: 0.7010\n  [Train] Batch 160 - Gen Loss: 9.1147, GAN: 1.6931, L1: 0.0742, Disc: 0.5776\n  [Train] Batch 170 - Gen Loss: 8.3313, GAN: 1.6624, L1: 0.0667, Disc: 0.4715\n  [Train] Batch 180 - Gen Loss: 8.8949, GAN: 1.4281, L1: 0.0747, Disc: 0.4778\n  [Train] Batch 190 - Gen Loss: 7.5237, GAN: 1.5686, L1: 0.0596, Disc: 0.4837\n  [Train] Batch 200 - Gen Loss: 8.4591, GAN: 1.8234, L1: 0.0664, Disc: 0.5204\n  [Train] Batch 210 - Gen Loss: 7.8326, GAN: 1.1536, L1: 0.0668, Disc: 0.6667\n  [Train] Batch 220 - Gen Loss: 8.6858, GAN: 1.4787, L1: 0.0721, Disc: 0.5628\n  [Train] Batch 230 - Gen Loss: 8.8621, GAN: 1.3971, L1: 0.0746, Disc: 0.5528\n  [Train] Batch 240 - Gen Loss: 9.1571, GAN: 1.7259, L1: 0.0743, Disc: 0.6257\n  [Train] Batch 250 - Gen Loss: 8.6526, GAN: 1.9239, L1: 0.0673, Disc: 0.3395\n  [Train] Batch 260 - Gen Loss: 8.7532, GAN: 2.1249, L1: 0.0663, Disc: 0.4150\n  [Train] Batch 270 - Gen Loss: 9.1112, GAN: 1.6423, L1: 0.0747, Disc: 0.5520\n  [Train] Batch 280 - Gen Loss: 11.3257, GAN: 4.1784, L1: 0.0715, Disc: 1.7247\n  [Train] Batch 290 - Gen Loss: 9.3252, GAN: 2.6113, L1: 0.0671, Disc: 0.8135\n  [Train] Batch 300 - Gen Loss: 9.9920, GAN: 2.4558, L1: 0.0754, Disc: 0.9072\n  [Train] Batch 310 - Gen Loss: 9.6414, GAN: 2.4539, L1: 0.0719, Disc: 0.9533\n  [Train] Batch 320 - Gen Loss: 7.5892, GAN: 1.6112, L1: 0.0598, Disc: 0.8815\n  [Train] Batch 330 - Gen Loss: 9.0080, GAN: 2.3016, L1: 0.0671, Disc: 0.6825\n  [Train] Batch 340 - Gen Loss: 8.4516, GAN: 1.6981, L1: 0.0675, Disc: 0.4105\n  [Train] Batch 350 - Gen Loss: 9.2642, GAN: 2.7334, L1: 0.0653, Disc: 0.8997\n  [Train] Batch 360 - Gen Loss: 8.2597, GAN: 1.7306, L1: 0.0653, Disc: 0.5019\n  [Train] Batch 370 - Gen Loss: 9.5194, GAN: 2.6077, L1: 0.0691, Disc: 0.6145\n  [Train] Batch 380 - Gen Loss: 7.0474, GAN: 0.8703, L1: 0.0618, Disc: 0.8768\n  [Train] Batch 390 - Gen Loss: 5.6198, GAN: 0.1926, L1: 0.0543, Disc: 2.1437\n  [Train] Batch 400 - Gen Loss: 5.4896, GAN: 0.5894, L1: 0.0490, Disc: 1.2786\n  [Train] Batch 410 - Gen Loss: 7.8951, GAN: 1.7048, L1: 0.0619, Disc: 0.5930\n  [Val] MAE: 0.1860, RMSE: 0.3228, SSIM: 0.6157\nEpoch 47 completed in 130.86 seconds.\n\nEpoch 48/150\n  [Train] Batch 000 - Gen Loss: 8.3810, GAN: 2.5057, L1: 0.0588, Disc: 0.5545\n  [Train] Batch 010 - Gen Loss: 8.6173, GAN: 0.5390, L1: 0.0808, Disc: 1.1707\n  [Train] Batch 020 - Gen Loss: 7.8365, GAN: 1.5934, L1: 0.0624, Disc: 0.7350\n  [Train] Batch 030 - Gen Loss: 9.0667, GAN: 1.2373, L1: 0.0783, Disc: 0.5577\n  [Train] Batch 040 - Gen Loss: 8.4918, GAN: 1.4491, L1: 0.0704, Disc: 0.5772\n  [Train] Batch 050 - Gen Loss: 7.6436, GAN: 1.2100, L1: 0.0643, Disc: 0.7304\n  [Train] Batch 060 - Gen Loss: 8.8665, GAN: 1.6536, L1: 0.0721, Disc: 0.5034\n  [Train] Batch 070 - Gen Loss: 8.4813, GAN: 2.0245, L1: 0.0646, Disc: 0.3860\n  [Train] Batch 080 - Gen Loss: 7.5738, GAN: 0.9564, L1: 0.0662, Disc: 0.8406\n  [Train] Batch 090 - Gen Loss: 9.0877, GAN: 2.8201, L1: 0.0627, Disc: 0.9831\n  [Train] Batch 100 - Gen Loss: 6.8981, GAN: 1.2688, L1: 0.0563, Disc: 0.7453\n  [Train] Batch 110 - Gen Loss: 7.0983, GAN: 1.6997, L1: 0.0540, Disc: 1.1737\n  [Train] Batch 120 - Gen Loss: 8.6061, GAN: 2.5735, L1: 0.0603, Disc: 0.3015\n  [Train] Batch 130 - Gen Loss: 6.5097, GAN: 0.4472, L1: 0.0606, Disc: 1.2916\n  [Train] Batch 140 - Gen Loss: 7.4703, GAN: 1.2941, L1: 0.0618, Disc: 0.6991\n  [Train] Batch 150 - Gen Loss: 9.6331, GAN: 2.4365, L1: 0.0720, Disc: 0.4436\n  [Train] Batch 160 - Gen Loss: 9.2473, GAN: 2.5522, L1: 0.0670, Disc: 0.5565\n  [Train] Batch 170 - Gen Loss: 9.0755, GAN: 2.7191, L1: 0.0636, Disc: 0.4264\n  [Train] Batch 180 - Gen Loss: 7.7779, GAN: 0.9956, L1: 0.0678, Disc: 0.6734\n  [Train] Batch 190 - Gen Loss: 8.8732, GAN: 2.4635, L1: 0.0641, Disc: 0.9221\n  [Train] Batch 200 - Gen Loss: 8.0342, GAN: 2.0764, L1: 0.0596, Disc: 0.3702\n  [Train] Batch 210 - Gen Loss: 9.1187, GAN: 2.2835, L1: 0.0684, Disc: 0.3568\n  [Train] Batch 220 - Gen Loss: 9.4527, GAN: 2.8401, L1: 0.0661, Disc: 0.5825\n  [Train] Batch 230 - Gen Loss: 8.8118, GAN: 1.8662, L1: 0.0695, Disc: 0.4382\n  [Train] Batch 240 - Gen Loss: 9.5905, GAN: 2.7152, L1: 0.0688, Disc: 0.4194\n  [Train] Batch 250 - Gen Loss: 9.9639, GAN: 3.5706, L1: 0.0639, Disc: 0.6219\n  [Train] Batch 260 - Gen Loss: 8.6547, GAN: 2.2891, L1: 0.0637, Disc: 0.8018\n  [Train] Batch 270 - Gen Loss: 8.5326, GAN: 0.8649, L1: 0.0767, Disc: 0.8249\n  [Train] Batch 280 - Gen Loss: 9.2105, GAN: 1.6243, L1: 0.0759, Disc: 0.5015\n  [Train] Batch 290 - Gen Loss: 7.6535, GAN: 1.0688, L1: 0.0658, Disc: 0.8191\n  [Train] Batch 300 - Gen Loss: 10.0784, GAN: 3.0300, L1: 0.0705, Disc: 0.2377\n  [Train] Batch 310 - Gen Loss: 8.8454, GAN: 2.2243, L1: 0.0662, Disc: 0.8495\n  [Train] Batch 320 - Gen Loss: 8.2897, GAN: 1.4610, L1: 0.0683, Disc: 0.6622\n  [Train] Batch 330 - Gen Loss: 8.1490, GAN: 1.1055, L1: 0.0704, Disc: 1.6258\n  [Train] Batch 340 - Gen Loss: 8.0994, GAN: 1.7527, L1: 0.0635, Disc: 5.5118\n  [Train] Batch 350 - Gen Loss: 7.2930, GAN: 0.7018, L1: 0.0659, Disc: 1.3250\n  [Train] Batch 360 - Gen Loss: 8.5585, GAN: 1.2500, L1: 0.0731, Disc: 1.0019\n  [Train] Batch 370 - Gen Loss: 8.1171, GAN: 1.4394, L1: 0.0668, Disc: 1.1130\n  [Train] Batch 380 - Gen Loss: 8.0342, GAN: 1.0611, L1: 0.0697, Disc: 1.1186\n  [Train] Batch 390 - Gen Loss: 6.5244, GAN: 1.0602, L1: 0.0546, Disc: 0.9026\n  [Train] Batch 400 - Gen Loss: 6.8292, GAN: 1.0647, L1: 0.0576, Disc: 1.3882\n  [Train] Batch 410 - Gen Loss: 7.2357, GAN: 0.9862, L1: 0.0625, Disc: 0.9752\n  [Val] MAE: 0.1837, RMSE: 0.3191, SSIM: 0.6230\nEpoch 48 completed in 130.97 seconds.\n\nEpoch 49/150\n  [Train] Batch 000 - Gen Loss: 6.4655, GAN: 0.9490, L1: 0.0552, Disc: 0.9263\n  [Train] Batch 010 - Gen Loss: 7.9662, GAN: 1.3758, L1: 0.0659, Disc: 0.6632\n  [Train] Batch 020 - Gen Loss: 7.0931, GAN: 1.0207, L1: 0.0607, Disc: 0.8033\n  [Train] Batch 030 - Gen Loss: 8.0995, GAN: 1.3038, L1: 0.0680, Disc: 0.8815\n  [Train] Batch 040 - Gen Loss: 7.7198, GAN: 0.8893, L1: 0.0683, Disc: 0.8175\n  [Train] Batch 050 - Gen Loss: 6.5788, GAN: 0.4586, L1: 0.0612, Disc: 1.4818\n  [Train] Batch 060 - Gen Loss: 7.2844, GAN: 1.2112, L1: 0.0607, Disc: 0.5264\n  [Train] Batch 070 - Gen Loss: 7.9814, GAN: 0.9084, L1: 0.0707, Disc: 0.8151\n  [Train] Batch 080 - Gen Loss: 6.7919, GAN: 1.6677, L1: 0.0512, Disc: 0.8598\n  [Train] Batch 090 - Gen Loss: 7.0758, GAN: 0.9632, L1: 0.0611, Disc: 0.9166\n  [Train] Batch 100 - Gen Loss: 7.4806, GAN: 1.1318, L1: 0.0635, Disc: 0.7943\n  [Train] Batch 110 - Gen Loss: 9.1695, GAN: 3.3107, L1: 0.0586, Disc: 0.3612\n  [Train] Batch 120 - Gen Loss: 8.3680, GAN: 1.8059, L1: 0.0656, Disc: 0.4972\n  [Train] Batch 130 - Gen Loss: 8.0957, GAN: 1.3333, L1: 0.0676, Disc: 1.0745\n  [Train] Batch 140 - Gen Loss: 9.0075, GAN: 2.3281, L1: 0.0668, Disc: 0.4615\n  [Train] Batch 150 - Gen Loss: 9.7922, GAN: 2.6707, L1: 0.0712, Disc: 0.2392\n  [Train] Batch 160 - Gen Loss: 7.8318, GAN: 1.4467, L1: 0.0639, Disc: 0.5918\n  [Train] Batch 170 - Gen Loss: 8.5948, GAN: 1.5831, L1: 0.0701, Disc: 0.5321\n  [Train] Batch 180 - Gen Loss: 6.7482, GAN: 1.1001, L1: 0.0565, Disc: 0.6802\n  [Train] Batch 190 - Gen Loss: 8.1333, GAN: 1.6568, L1: 0.0648, Disc: 0.6032\n  [Train] Batch 200 - Gen Loss: 8.8003, GAN: 2.0924, L1: 0.0671, Disc: 0.3913\n  [Train] Batch 210 - Gen Loss: 8.3547, GAN: 1.7768, L1: 0.0658, Disc: 0.4211\n  [Train] Batch 220 - Gen Loss: 9.4830, GAN: 3.0357, L1: 0.0645, Disc: 0.4846\n  [Train] Batch 230 - Gen Loss: 9.4757, GAN: 3.5220, L1: 0.0595, Disc: 0.9065\n  [Train] Batch 240 - Gen Loss: 7.2676, GAN: 0.9264, L1: 0.0634, Disc: 0.7688\n  [Train] Batch 250 - Gen Loss: 8.7234, GAN: 1.5361, L1: 0.0719, Disc: 0.5381\n  [Train] Batch 260 - Gen Loss: 8.2689, GAN: 1.1385, L1: 0.0713, Disc: 0.7114\n  [Train] Batch 270 - Gen Loss: 8.6559, GAN: 1.3559, L1: 0.0730, Disc: 0.5533\n  [Train] Batch 280 - Gen Loss: 7.7332, GAN: 0.8673, L1: 0.0687, Disc: 0.8714\n  [Train] Batch 290 - Gen Loss: 9.9931, GAN: 3.1241, L1: 0.0687, Disc: 0.7665\n  [Train] Batch 300 - Gen Loss: 9.1122, GAN: 1.8505, L1: 0.0726, Disc: 0.4472\n  [Train] Batch 310 - Gen Loss: 8.8632, GAN: 2.6152, L1: 0.0625, Disc: 0.6808\n  [Train] Batch 320 - Gen Loss: 9.3113, GAN: 2.2657, L1: 0.0705, Disc: 0.8224\n  [Train] Batch 330 - Gen Loss: 7.5394, GAN: 0.8911, L1: 0.0665, Disc: 0.8137\n  [Train] Batch 340 - Gen Loss: 8.7223, GAN: 2.0504, L1: 0.0667, Disc: 0.4008\n  [Train] Batch 350 - Gen Loss: 6.7795, GAN: 0.8604, L1: 0.0592, Disc: 0.7579\n  [Train] Batch 360 - Gen Loss: 8.1902, GAN: 0.7096, L1: 0.0748, Disc: 0.8965\n  [Train] Batch 370 - Gen Loss: 7.2913, GAN: 1.1770, L1: 0.0611, Disc: 3.2690\n  [Train] Batch 380 - Gen Loss: 8.2504, GAN: 1.9442, L1: 0.0631, Disc: 0.6008\n  [Train] Batch 390 - Gen Loss: 8.8071, GAN: 3.5373, L1: 0.0527, Disc: 1.6279\n  [Train] Batch 400 - Gen Loss: 6.4703, GAN: 1.0250, L1: 0.0545, Disc: 1.0955\n  [Train] Batch 410 - Gen Loss: 6.9689, GAN: 1.8035, L1: 0.0517, Disc: 0.6913\n  [Val] MAE: 0.1834, RMSE: 0.3177, SSIM: 0.6213\nEpoch 49 completed in 131.43 seconds.\n\nEpoch 50/150\n  [Train] Batch 000 - Gen Loss: 8.0285, GAN: 1.3246, L1: 0.0670, Disc: 0.6885\n  [Train] Batch 010 - Gen Loss: 7.9919, GAN: 1.5557, L1: 0.0644, Disc: 0.7272\n  [Train] Batch 020 - Gen Loss: 7.6607, GAN: 1.3871, L1: 0.0627, Disc: 0.5420\n  [Train] Batch 030 - Gen Loss: 7.5761, GAN: 1.1948, L1: 0.0638, Disc: 0.6548\n  [Train] Batch 040 - Gen Loss: 6.3153, GAN: 0.3398, L1: 0.0598, Disc: 1.5490\n  [Train] Batch 050 - Gen Loss: 7.9469, GAN: 1.2510, L1: 0.0670, Disc: 0.7573\n  [Train] Batch 060 - Gen Loss: 9.2459, GAN: 2.0234, L1: 0.0722, Disc: 0.5892\n  [Train] Batch 070 - Gen Loss: 8.2476, GAN: 2.2354, L1: 0.0601, Disc: 0.6416\n  [Train] Batch 080 - Gen Loss: 6.8436, GAN: 0.9796, L1: 0.0586, Disc: 0.7966\n  [Train] Batch 090 - Gen Loss: 6.3994, GAN: 0.9618, L1: 0.0544, Disc: 0.9716\n  [Train] Batch 100 - Gen Loss: 6.7929, GAN: 1.1502, L1: 0.0564, Disc: 0.9964\n  [Train] Batch 110 - Gen Loss: 7.1572, GAN: 1.6967, L1: 0.0546, Disc: 1.0139\n  [Train] Batch 120 - Gen Loss: 7.4171, GAN: 0.6863, L1: 0.0673, Disc: 1.0792\n  [Train] Batch 130 - Gen Loss: 8.1487, GAN: 2.1715, L1: 0.0598, Disc: 0.8904\n  [Train] Batch 140 - Gen Loss: 6.6957, GAN: 0.5944, L1: 0.0610, Disc: 1.0630\n  [Train] Batch 150 - Gen Loss: 7.8158, GAN: 1.3734, L1: 0.0644, Disc: 0.6799\n  [Train] Batch 160 - Gen Loss: 9.3987, GAN: 2.8316, L1: 0.0657, Disc: 0.4739\n  [Train] Batch 170 - Gen Loss: 6.7391, GAN: 0.6369, L1: 0.0610, Disc: 1.0058\n  [Train] Batch 180 - Gen Loss: 8.9051, GAN: 2.5971, L1: 0.0631, Disc: 0.5310\n  [Train] Batch 190 - Gen Loss: 7.9098, GAN: 1.7675, L1: 0.0614, Disc: 0.3812\n  [Train] Batch 200 - Gen Loss: 7.9106, GAN: 1.6561, L1: 0.0625, Disc: 0.4967\n  [Train] Batch 210 - Gen Loss: 9.3006, GAN: 2.3616, L1: 0.0694, Disc: 0.2731\n  [Train] Batch 220 - Gen Loss: 9.6808, GAN: 3.0137, L1: 0.0667, Disc: 0.3388\n  [Train] Batch 230 - Gen Loss: 8.2313, GAN: 1.3160, L1: 0.0692, Disc: 0.6522\n  [Train] Batch 240 - Gen Loss: 7.7163, GAN: 1.2841, L1: 0.0643, Disc: 0.6215\n  [Train] Batch 250 - Gen Loss: 7.5600, GAN: 1.0293, L1: 0.0653, Disc: 0.8786\n  [Train] Batch 260 - Gen Loss: 9.6877, GAN: 3.2171, L1: 0.0647, Disc: 0.8930\n  [Train] Batch 270 - Gen Loss: 8.4476, GAN: 1.9578, L1: 0.0649, Disc: 0.8596\n  [Train] Batch 280 - Gen Loss: 8.1881, GAN: 1.3745, L1: 0.0681, Disc: 0.6077\n  [Train] Batch 290 - Gen Loss: 7.4451, GAN: 0.7031, L1: 0.0674, Disc: 0.9309\n  [Train] Batch 300 - Gen Loss: 8.5136, GAN: 1.8629, L1: 0.0665, Disc: 0.5151\n  [Train] Batch 310 - Gen Loss: 7.9078, GAN: 2.0020, L1: 0.0591, Disc: 0.5347\n  [Train] Batch 320 - Gen Loss: 11.3412, GAN: 4.8124, L1: 0.0653, Disc: 1.1953\n  [Train] Batch 330 - Gen Loss: 7.0457, GAN: 0.9278, L1: 0.0612, Disc: 1.2749\n  [Train] Batch 340 - Gen Loss: 8.4181, GAN: 2.1236, L1: 0.0629, Disc: 0.7491\n  [Train] Batch 350 - Gen Loss: 9.4564, GAN: 2.7912, L1: 0.0667, Disc: 0.6655\n  [Train] Batch 360 - Gen Loss: 10.3111, GAN: 3.8017, L1: 0.0651, Disc: 0.9707\n  [Train] Batch 370 - Gen Loss: 8.1152, GAN: 2.0387, L1: 0.0608, Disc: 0.6678\n  [Train] Batch 380 - Gen Loss: 9.7848, GAN: 2.4161, L1: 0.0737, Disc: 0.5221\n  [Train] Batch 390 - Gen Loss: 9.3041, GAN: 4.0715, L1: 0.0523, Disc: 2.1753\n  [Train] Batch 400 - Gen Loss: 7.8372, GAN: 2.6501, L1: 0.0519, Disc: 2.0130\n  [Train] Batch 410 - Gen Loss: 8.4876, GAN: 2.3642, L1: 0.0612, Disc: 0.5843\n  [Val] MAE: 0.1844, RMSE: 0.3193, SSIM: 0.6136\nCheckpoint saved: /kaggle/working/outputs/checkpoints/ckpt_epoch_50\nEpoch 50 completed in 134.20 seconds.\n\nEpoch 51/150\n  [Train] Batch 000 - Gen Loss: 7.9933, GAN: 0.9297, L1: 0.0706, Disc: 0.7997\n  [Train] Batch 010 - Gen Loss: 8.5533, GAN: 1.4204, L1: 0.0713, Disc: 0.7633\n  [Train] Batch 020 - Gen Loss: 7.6000, GAN: 0.9585, L1: 0.0664, Disc: 0.7300\n  [Train] Batch 030 - Gen Loss: 9.1991, GAN: 1.8883, L1: 0.0731, Disc: 0.6763\n  [Train] Batch 040 - Gen Loss: 9.9202, GAN: 3.9416, L1: 0.0598, Disc: 0.9240\n  [Train] Batch 050 - Gen Loss: 9.9575, GAN: 3.0425, L1: 0.0691, Disc: 0.7798\n  [Train] Batch 060 - Gen Loss: 8.7071, GAN: 2.5417, L1: 0.0617, Disc: 0.4216\n  [Train] Batch 070 - Gen Loss: 9.0166, GAN: 2.9183, L1: 0.0610, Disc: 0.9589\n  [Train] Batch 080 - Gen Loss: 7.8687, GAN: 1.5991, L1: 0.0627, Disc: 0.6837\n  [Train] Batch 090 - Gen Loss: 8.0296, GAN: 2.0522, L1: 0.0598, Disc: 0.4601\n  [Train] Batch 100 - Gen Loss: 7.6289, GAN: 1.8326, L1: 0.0580, Disc: 1.0544\n  [Train] Batch 110 - Gen Loss: 8.7925, GAN: 1.7182, L1: 0.0707, Disc: 0.6359\n  [Train] Batch 120 - Gen Loss: 8.5069, GAN: 2.8375, L1: 0.0567, Disc: 0.9396\n  [Train] Batch 130 - Gen Loss: 6.5589, GAN: 1.4062, L1: 0.0515, Disc: 0.9157\n  [Train] Batch 140 - Gen Loss: 9.0095, GAN: 2.0759, L1: 0.0693, Disc: 0.6625\n  [Train] Batch 150 - Gen Loss: 7.7450, GAN: 1.6695, L1: 0.0608, Disc: 0.5270\n  [Train] Batch 160 - Gen Loss: 8.0880, GAN: 1.7028, L1: 0.0639, Disc: 0.6444\n  [Train] Batch 170 - Gen Loss: 9.6244, GAN: 2.7478, L1: 0.0688, Disc: 0.9263\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def test(generator, test_dataset, test_filenames, output_dir=\".\"):\n    predictions_dict = {}  # Dictionary to store predictions\n    \n    for i, spad_batch in enumerate(test_dataset):\n        # Generate predictions\n        predicted_depths = generator(spad_batch, training=False)\n        \n        \n        # Denormalize predictions\n        predicted_depths = (predicted_depths + 1) / 2\n        \n        # Save predictions for each image in the batch\n        for j in range(spad_batch.shape[0]):\n            # Get the filename\n            print(predicted_depths[j].numpy().shape)\n            idx = i * BATCH_SIZE + j\n            if idx >= len(test_filenames):\n                break\n            \n            filename = os.path.basename(test_filenames[idx])\n            image_id = filename.replace(\".png\", \"\")\n            \n            predictions_dict[image_id] = predicted_depths[j].numpy()  # Store prediction\n            \n            # Save the predicted depth map\n            output_path = os.path.join(output_dir, f\"{image_id}_depth.png\")\n            # Clip values and ensure correct type\n            depth_map_to_save = predicted_depths[j].numpy()\n            depth_map_to_save = np.clip(depth_map_to_save, 0, 1)\n\n            # Convert to grayscale and save\n            depth_map_to_save = tf.image.convert_image_dtype(depth_map_to_save, dtype=tf.uint8)\n            plt.imsave(output_path, predicted_depths[j].numpy(), cmap='viridis')\n    \n    return predictions_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:31:05.429458Z","iopub.execute_input":"2025-04-26T19:31:05.429734Z","iopub.status.idle":"2025-04-26T19:31:05.436328Z","shell.execute_reply.started":"2025-04-26T19:31:05.429713Z","shell.execute_reply":"2025-04-26T19:31:05.435536Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"print(predicted_depths[j].numpy().shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:21:06.498543Z","iopub.execute_input":"2025-04-26T19:21:06.498796Z","iopub.status.idle":"2025-04-26T19:21:06.513630Z","shell.execute_reply.started":"2025-04-26T19:21:06.498777Z","shell.execute_reply":"2025-04-26T19:21:06.512641Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1922474933.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_depths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'predicted_depths' is not defined"],"ename":"NameError","evalue":"name 'predicted_depths' is not defined","output_type":"error"}],"execution_count":51},{"cell_type":"code","source":"# Get test images\ntest_spad_paths = sorted([os.path.join(SPAD_TEST_PATH, filename) \n                         for filename in os.listdir(SPAD_TEST_PATH) \n                         if filename.endswith('.png')])\n\nprint(f\"Running inference on {len(test_spad_paths)} test images...\")\n\n# Create test dataset\ntest_dataset = create_test_dataset(test_spad_paths, batch_size=BATCH_SIZE)\n\n# Generate depth maps for test data\npredictions_dict = test(generator, test_dataset, test_spad_paths, output_dir=TEST_OUTPUT_DIR)\n\n# Convert predictions to submission CSV\nsubmission_file = convert_to_submission(predictions_dict, \n                                       output_dir=TEST_OUTPUT_DIR, \n                                       output_file=os.path.join(OUTPUT_DIR, \"submission.csv\"))\n\nprint(f\"Submission file created: {submission_file}\")\n\n# Visualize a few test predictions\nprint(\"Creating visualization of test predictions...\")\nvisualize_test_predictions(generator, test_dataset, test_spad_paths, \n                          num_examples=5, output_dir=TEST_OUTPUT_DIR)\n\nprint(\"Pipeline completed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T19:31:11.842388Z","iopub.execute_input":"2025-04-26T19:31:11.843020Z","iopub.status.idle":"2025-04-26T19:31:11.982385Z","shell.execute_reply.started":"2025-04-26T19:31:11.842994Z","shell.execute_reply":"2025-04-26T19:31:11.981295Z"}},"outputs":[{"name":"stdout","text":"Running inference on 836 test images...\n(256, 256, 1)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2037069216.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Generate depth maps for test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpredictions_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_spad_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTEST_OUTPUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Convert predictions to submission CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/3111738840.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(generator, test_dataset, test_filenames, output_dir)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# Convert to grayscale and save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mdepth_map_to_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_image_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth_map_to_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_depths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'viridis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, **kwargs)\u001b[0m\n\u001b[1;32m   2198\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2199\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2200\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m             \u001b[0msm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScalarMappable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m             \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpil_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m             \u001b[0mpil_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/cm.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(self, x, alpha, bytes, norm)\u001b[0m\n\u001b[1;32m    470\u001b[0m                     \u001b[0mxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Third dimension must be 3 or 4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'f'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Third dimension must be 3 or 4"],"ename":"ValueError","evalue":"Third dimension must be 3 or 4","output_type":"error"}],"execution_count":63},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}